{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import glob\n",
    "\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TerminateOnNaN, CSVLogger, TensorBoard\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from models.keras_ssd300_Siamese import ssd_300\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
    "from keras_layers.keras_layer_L2Normalization import L2Normalization\n",
    "\n",
    "from ssd_encoder_decoder.ssd_input_encoder import SSDInputEncoder\n",
    "from ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n",
    "\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from data_generator.object_detection_2d_geometric_ops import Resize\n",
    "from data_generator.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
    "from data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation, SSDDataAugmentation_Siamese\n",
    "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
    "\n",
    "\n",
    "img_height = 300 # Height of the model input images\n",
    "img_width = 300 # Width of the model input images\n",
    "img_channels = 3 # Number of color channels of the model input images\n",
    "mean_color = [123, 117, 104] # Per-channel mean of images. Do not change if use any of the pre-trained weights.\n",
    "# The color channel order in the original SSD is BGR,\n",
    "# so we'll have the model reverse the color channel order of the input images.\n",
    "swap_channels = [2, 1, 0]\n",
    "classes = ['background',\n",
    "           'person', 'rider', 'car', 'truck',\n",
    "           'bus', 'train', 'motorcycle', 'bicycle']\n",
    "n_classes = len(classes) - 1  # Number of positive classes, 8 for domain Cityscapes, 20 for Pascal VOC, 80 for MS COCO\n",
    "# The anchor box scaling factors used in the original SSD300 for the Pascal VOC datasets\n",
    "# scales_pascal = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05]\n",
    "# The anchor box scaling factors used in the original SSD300 for the MS COCO datasets\n",
    "scales_coco = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05]\n",
    "scales = scales_coco\n",
    "aspect_ratios = [[1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5]] # The anchor box aspect ratios used in the original SSD300; the order matters\n",
    "two_boxes_for_ar1 = True\n",
    "steps = [8, 16, 32, 64, 100, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
    "# The offsets of the first anchor box center points from the top and left borders of the image\n",
    "# as a fraction of the step size for each predictor layer.\n",
    "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
    "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
    "# The variances by which the encoded target coordinates are divided as in the original implementation\n",
    "variances = [0.1, 0.1, 0.2, 0.2]\n",
    "normalize_coords = True\n",
    "Model_Build = 'New_Model'  # 'Load_Model'\n",
    "Optimizer_Type = 'SGD'  # 'Adam' #  \n",
    "batch_size = 16  # Change the batch size if you like, or if you run into GPU memory issues.\n",
    "# alpha_distance =  0.001  # Coefficient for the distance between the source and target feature maps.\n",
    "loss_weights = [0.000001, 0.000001, 0.000001] + [0.0, 0.0, 0.0, 0.0, 0.0, 0.0] + [1.0]\n",
    "Source_Only = False\n",
    "\n",
    "if len(glob.glob('*.h5')):\n",
    "    Dataset_Build = 'Load_Dataset'\n",
    "else:\n",
    "    Dataset_Build = 'New_Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Model_Build == 'New_Model':\n",
    "    # 1: Build the Keras model.\n",
    "\n",
    "    K.clear_session()  # Clear previous models from memory.\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from keras.backend.tensorflow_backend import set_session\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "    config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "    # (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "    sess = tf.Session(config=config)\n",
    "    set_session(sess)  # set this TensorFlow session as the default session for Keras\n",
    "\n",
    "    model = ssd_300(image_size=(img_height, img_width, img_channels),\n",
    "                    n_classes=n_classes,\n",
    "                    mode='training',\n",
    "                    l2_regularization=0.0005,\n",
    "                    scales=scales,\n",
    "                    aspect_ratios_per_layer=aspect_ratios,\n",
    "                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                    steps=steps,\n",
    "                    offsets=offsets,\n",
    "                    clip_boxes=clip_boxes,\n",
    "                    variances=variances,\n",
    "                    normalize_coords=normalize_coords,\n",
    "                    subtract_mean=mean_color,\n",
    "                    swap_channels=swap_channels)\n",
    "\n",
    "    # 2: Load some weights into the model.\n",
    "\n",
    "    # TODO: Set the path to the weights you want to load.\n",
    "    weights_path = '../trained_weights/VGG_ILSVRC_16_layers_fc_reduced.h5'\n",
    "\n",
    "    model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "    # 3: Instantiate an optimizer and the SSD loss function and compile the model.\n",
    "    #    If you want to follow the original Caffe implementation, use the preset SGD\n",
    "    #    optimizer, otherwise I'd recommend the commented-out Adam optimizer.\n",
    "\n",
    "    if Optimizer_Type == 'SGD':\n",
    "        Optimizer = SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=False)\n",
    "    elif Optimizer_Type == 'Adam':\n",
    "        Optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    else:\n",
    "        raise ValueError('Undefined Optimizer_Type.')\n",
    "\n",
    "    ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "    if Source_Only:\n",
    "        model.compile(optimizer=Optimizer, loss={'pool1_GAP_substract': ssd_loss.compute_distance_loss_source_only,\n",
    "                                                 'pool2_GAP_substract': ssd_loss.compute_distance_loss_source_only,\n",
    "                                                 'pool3_GAP_substract': ssd_loss.compute_distance_loss_source_only,\n",
    "                                                 'conv4_3_norm_GAP_substract': ssd_loss.compute_distance_loss_source_only,\n",
    "                                                 'fc7_GAP_substract': ssd_loss.compute_distance_loss_source_only,\n",
    "                                                 'conv6_2_GAP_substract': ssd_loss.compute_distance_loss_source_only,\n",
    "                                                 'conv7_2_GAP_substract': ssd_loss.compute_distance_loss_source_only,\n",
    "                                                 'conv8_2_GAP_substract': ssd_loss.compute_distance_loss_source_only,\n",
    "                                                 'conv9_2_GAP_substract': ssd_loss.compute_distance_loss_source_only,\n",
    "                                                 'predictions': ssd_loss.compute_loss},\n",
    "                      loss_weights={'pool1_GAP_substract': loss_weights[0],\n",
    "                                    'pool2_GAP_substract': loss_weights[1],\n",
    "                                    'pool3_GAP_substract': loss_weights[2],\n",
    "                                    'conv4_3_norm_GAP_substract': loss_weights[3],\n",
    "                                    'fc7_GAP_substract': loss_weights[4],\n",
    "                                    'conv6_2_GAP_substract': loss_weights[5],\n",
    "                                    'conv7_2_GAP_substract': loss_weights[6],\n",
    "                                    'conv8_2_GAP_substract': loss_weights[7],\n",
    "                                    'conv9_2_GAP_substract': loss_weights[8],\n",
    "                                    'predictions': loss_weights[9]})\n",
    "    else:\n",
    "        model.compile(optimizer=Optimizer, loss={'pool1_GAP_substract': ssd_loss.compute_distance_loss,\n",
    "                                                 'pool2_GAP_substract': ssd_loss.compute_distance_loss,\n",
    "                                                 'pool3_GAP_substract': ssd_loss.compute_distance_loss,\n",
    "                                                 'conv4_3_norm_GAP_substract': ssd_loss.compute_distance_loss,\n",
    "                                                 'fc7_GAP_substract': ssd_loss.compute_distance_loss,\n",
    "                                                 'conv6_2_GAP_substract': ssd_loss.compute_distance_loss,\n",
    "                                                 'conv7_2_GAP_substract': ssd_loss.compute_distance_loss,\n",
    "                                                 'conv8_2_GAP_substract': ssd_loss.compute_distance_loss,\n",
    "                                                 'conv9_2_GAP_substract': ssd_loss.compute_distance_loss,\n",
    "                                                 'predictions': ssd_loss.compute_loss},\n",
    "                      loss_weights={'pool1_GAP_substract': loss_weights[0],\n",
    "                                    'pool2_GAP_substract': loss_weights[1],\n",
    "                                    'pool3_GAP_substract': loss_weights[2],\n",
    "                                    'conv4_3_norm_GAP_substract': loss_weights[3],\n",
    "                                    'fc7_GAP_substract': loss_weights[4],\n",
    "                                    'conv6_2_GAP_substract': loss_weights[5],\n",
    "                                    'conv7_2_GAP_substract': loss_weights[6],\n",
    "                                    'conv8_2_GAP_substract': loss_weights[7],\n",
    "                                    'conv9_2_GAP_substract': loss_weights[8],\n",
    "                                    'predictions': loss_weights[9]})\n",
    "        \n",
    "        \n",
    "elif Model_Build == 'Load_Model':\n",
    "    # TODO: Set the path to the `.h5` file of the model to be loaded.\n",
    "    model_path = '../trained_weights/VGG_ssd300_Siamese_Cityscapes/epoch-23_loss-5.2110_val_loss-6.7452.h5'\n",
    "\n",
    "    # We need to create an SSDLoss object in order to pass that to the model loader.\n",
    "    ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "    K.clear_session()  # Clear previous models from memory.\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from keras.backend.tensorflow_backend import set_session\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "    config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "    # (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "    sess = tf.Session(config=config)\n",
    "    set_session(sess)  # set this TensorFlow session as the default session for Keras\n",
    "\n",
    "    model = load_model(model_path, custom_objects={'AnchorBoxes': AnchorBoxes,\n",
    "                                                   'L2Normalization': L2Normalization,\n",
    "                                                   'compute_loss': ssd_loss.compute_loss,\n",
    "                                                   'compute_distance_loss': ssd_loss.compute_distance_loss})\n",
    "else:\n",
    "    raise ValueError('Undefined Model_Build. Model_Build should be New_Model  or Load_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labels: 100%|██████████| 2966/2966 [00:00<00:00, 3193.98it/s]\n",
      "Loading source image IDs: 100%|██████████| 2966/2966 [00:00<00:00, 11148.94it/s]\n",
      "Loading target image IDs: 100%|██████████| 2966/2966 [00:00<00:00, 10788.24it/s]\n",
      "Loading evaluation-neutrality annotations: 100%|██████████| 2966/2966 [00:00<00:00, 8076.01it/s]\n",
      "Loading labels: 100%|██████████| 493/493 [00:00<00:00, 3278.45it/s]\n",
      "Loading source image IDs: 100%|██████████| 493/493 [00:00<00:00, 11070.50it/s]\n",
      "Loading target image IDs: 0it [00:00, ?it/s]\n",
      "Loading evaluation-neutrality annotations: 100%|██████████| 493/493 [00:00<00:00, 7912.18it/s]\n"
     ]
    }
   ],
   "source": [
    "if Dataset_Build == 'New_Dataset':\n",
    "    # 1: Instantiate two `DataGenerator` objects: One for training, one for validation.\n",
    "\n",
    "    # Optional: If you have enough memory, consider loading the images into memory for the reasons explained above.\n",
    "\n",
    "    train_dataset = DataGenerator(dataset='train', load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "    val_dataset = DataGenerator(dataset='val', load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "\n",
    "    # 2: Parse the image and label lists for the training and validation datasets. This can take a while.\n",
    "\n",
    "    # TODO: Set the paths to the datasets here.\n",
    "\n",
    "    # Introduction of PascalVOC: https://arleyzhang.github.io/articles/1dc20586/\n",
    "    # The directories that contain the images.\n",
    "    Cityscapes_images_dir = '../../datasets/Cityscapes/JPEGImages'\n",
    "    Cityscapes_target_images_dir = '../../datasets/CITYSCAPES_beta_0_01/JPEGImages'\n",
    "\n",
    "    # The directories that contain the annotations.\n",
    "    Cityscapes_annotation_dir = '../../datasets/Cityscapes/Annotations'\n",
    "\n",
    "    # The paths to the image sets.\n",
    "    Cityscapes_train_source_image_set_filename = '../../datasets/Cityscapes/ImageSets/Main/train_source.txt'\n",
    "    Cityscapes_train_target_image_set_filename = '../../datasets/Cityscapes/ImageSets/Main/train_target.txt'\n",
    "    Cityscapes_test_target_image_set_filename = '../../datasets/Cityscapes/ImageSets/Main/test.txt'\n",
    "\n",
    "    # images_dirs, image_set_filenames, and annotations_dirs should have the same length\n",
    "    train_dataset.parse_xml(images_dirs=[Cityscapes_images_dir],\n",
    "                            target_images_dirs=[Cityscapes_target_images_dir],\n",
    "                            image_set_filenames=[Cityscapes_train_source_image_set_filename],\n",
    "                            target_image_set_filenames=[Cityscapes_train_target_image_set_filename],\n",
    "                            annotations_dirs=[Cityscapes_annotation_dir],\n",
    "                            classes=classes,\n",
    "                            include_classes='all',\n",
    "                            exclude_truncated=False,\n",
    "                            exclude_difficult=False,\n",
    "                            ret=False)\n",
    "\n",
    "    val_dataset.parse_xml(images_dirs=[Cityscapes_target_images_dir],\n",
    "                          image_set_filenames=[Cityscapes_test_target_image_set_filename],\n",
    "                          annotations_dirs=[Cityscapes_annotation_dir],\n",
    "                          classes=classes,\n",
    "                          include_classes='all',\n",
    "                          exclude_truncated=False,\n",
    "                          exclude_difficult=True,\n",
    "                          ret=False)\n",
    "\n",
    "    # Optional: Convert the dataset into an HDF5 dataset. This will require more disk space, but will\n",
    "    # speed up the training. Doing this is not relevant in case you activated the `load_images_into_memory`\n",
    "    # option in the constructor, because in that cas the images are in memory already anyway. If you don't\n",
    "    # want to create HDF5 datasets, comment out the subsequent two function calls.\n",
    "\n",
    "    # After create these h5 files, if you have resized the input image, you need to reload these files. Otherwise,\n",
    "    # the images and the labels will not change.\n",
    "\n",
    "    resize_image_to = (300, 600)\n",
    "    train_dataset.create_hdf5_dataset(file_path='dataset_cityscapes_train.h5',\n",
    "                                      resize=resize_image_to,\n",
    "                                      variable_image_size=True,\n",
    "                                      verbose=True)\n",
    "\n",
    "    val_dataset.create_hdf5_dataset(file_path='dataset_cityscapes_test.h5',\n",
    "                                    resize=resize_image_to,\n",
    "                                    variable_image_size=True,\n",
    "                                    verbose=True)\n",
    "\n",
    "    train_dataset = DataGenerator(dataset='train',\n",
    "                                  load_images_into_memory=False,\n",
    "                                  hdf5_dataset_path='dataset_cityscapes_train.h5',\n",
    "                                  filenames=Cityscapes_train_source_image_set_filename,\n",
    "                                  target_filenames=Cityscapes_train_target_image_set_filename,\n",
    "                                  filenames_type='text',\n",
    "                                  images_dir=Cityscapes_images_dir,\n",
    "                                  target_images_dir=Cityscapes_target_images_dir)\n",
    "\n",
    "    val_dataset = DataGenerator(dataset='val',\n",
    "                                load_images_into_memory=False,\n",
    "                                hdf5_dataset_path='dataset_cityscapes_test.h5',\n",
    "                                filenames=Cityscapes_test_target_image_set_filename,\n",
    "                                filenames_type='text',\n",
    "                                images_dir=Cityscapes_target_images_dir)\n",
    "\n",
    "elif Dataset_Build == 'Load_Dataset':\n",
    "    # 1: Instantiate two `DataGenerator` objects: One for training, one for validation.\n",
    "\n",
    "    # Load dataset from the created h5 file.\n",
    "\n",
    "    # The directories that contain the images.\n",
    "    Cityscapes_images_dir = '../../datasets/Cityscapes/JPEGImages'\n",
    "    Cityscapes_target_images_dir = '../../datasets/CITYSCAPES_beta_0_01/JPEGImages'\n",
    "\n",
    "    # The paths to the image sets.\n",
    "    Cityscapes_train_source_image_set_filename = '../../datasets/Cityscapes/ImageSets/Main/train_source.txt'\n",
    "    Cityscapes_train_target_image_set_filename = '../../datasets/Cityscapes/ImageSets/Main/train_target.txt'\n",
    "    Cityscapes_test_target_image_set_filename = '../../datasets/Cityscapes/ImageSets/Main/test.txt'\n",
    "\n",
    "    train_dataset = DataGenerator(dataset='train',\n",
    "                                  load_images_into_memory=False,\n",
    "                                  hdf5_dataset_path='dataset_cityscapes_train.h5',\n",
    "                                  filenames=Cityscapes_train_source_image_set_filename,\n",
    "                                  target_filenames=Cityscapes_train_target_image_set_filename,\n",
    "                                  filenames_type='text',\n",
    "                                  images_dir=Cityscapes_images_dir,\n",
    "                                  target_images_dir=Cityscapes_target_images_dir)\n",
    "\n",
    "    val_dataset = DataGenerator(dataset='val',\n",
    "                                load_images_into_memory=False,\n",
    "                                hdf5_dataset_path='dataset_cityscapes_test.h5',\n",
    "                                filenames=Cityscapes_test_target_image_set_filename,\n",
    "                                filenames_type='text',\n",
    "                                images_dir=Cityscapes_target_images_dir)\n",
    "\n",
    "else:\n",
    "    raise ValueError('Undefined Dataset_Build. Dataset_Build should be New_Dataset or Load_Dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the training dataset:\t  5932\n",
      "Number of images in the validation dataset:\t   493\n"
     ]
    }
   ],
   "source": [
    "# 4: Set the image transformations for pre-processing and data augmentation options.\n",
    "\n",
    "# For the training generator:\n",
    "ssd_data_augmentation = SSDDataAugmentation_Siamese(img_height=img_height,\n",
    "                                                    img_width=img_width)\n",
    "\n",
    "# For the validation generator:\n",
    "convert_to_3_channels = ConvertTo3Channels()\n",
    "resize = Resize(height=img_height, width=img_width)\n",
    "\n",
    "# 5: Instantiate an encoder that can encode ground truth labels into the format needed by the SSD loss function.\n",
    "\n",
    "# The encoder constructor needs the spatial dimensions of the model's predictor layers to create the anchor boxes.\n",
    "predictor_sizes = [model.get_layer('conv4_3_norm_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('fc7_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv6_2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv7_2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv8_2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv9_2_mbox_conf').output_shape[1:3]]\n",
    "\n",
    "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
    "                                    img_width=img_width,\n",
    "                                    n_classes=n_classes,\n",
    "                                    predictor_sizes=predictor_sizes,\n",
    "                                    scales=scales,\n",
    "                                    aspect_ratios_per_layer=aspect_ratios,\n",
    "                                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                                    steps=steps,\n",
    "                                    offsets=offsets,\n",
    "                                    clip_boxes=clip_boxes,\n",
    "                                    variances=variances,\n",
    "                                    matching_type='multi',\n",
    "                                    pos_iou_threshold=0.5,\n",
    "                                    neg_iou_limit=0.5,\n",
    "                                    normalize_coords=normalize_coords)\n",
    "\n",
    "# 6: Create the generator handles that will be passed to Keras' `fit_generator()` function.\n",
    "# The input image and label are first processed by transformations. Then, the label will be further encoded by\n",
    "# ssd_input_encoder. The encoded labels are classId and offset to each anchor box.\n",
    "train_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[ssd_data_augmentation],\n",
    "                                         label_encoder=ssd_input_encoder,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'encoded_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n",
    "val_generator = val_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[convert_to_3_channels,\n",
    "                                                      resize],\n",
    "                                     label_encoder=ssd_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'encoded_labels'},\n",
    "                                     keep_images_without_gt=False)\n",
    "\n",
    "# Get the number of samples in the training and validations datasets.\n",
    "train_dataset_size = train_dataset.get_dataset_size()\n",
    "val_dataset_size = val_dataset.get_dataset_size()\n",
    "\n",
    "print(\"Number of images in the training dataset:\\t{:>6}\".format(train_dataset_size))\n",
    "print(\"Number of images in the validation dataset:\\t{:>6}\".format(val_dataset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    if epoch < 2:\n",
    "        return 0.0005\n",
    "    elif epoch < 45:\n",
    "        return 0.001\n",
    "    elif epoch < 100:\n",
    "        return 0.0001\n",
    "    else:\n",
    "        return 0.00001\n",
    "\n",
    "# def lr_schedule(epoch):\n",
    "#     if epoch < 30:\n",
    "#         return 0.001\n",
    "#     elif epoch < 60:\n",
    "#         return 0.0001\n",
    "#     else:\n",
    "#         return 0.00001\n",
    "\n",
    "# Define model callbacks.\n",
    "checkpoint_path = '../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD'\n",
    "# checkpoint_path = '../trained_weights/current/ssd_augm_beta_0_01_source_only'\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "\n",
    "# TODO: Set the filepath under which you want to save the model.\n",
    "model_checkpoint = ModelCheckpoint(filepath=os.path.join(checkpoint_path, 'epoch-{epoch:02d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.h5'),\n",
    "                                   monitor='val_loss',\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=False,\n",
    "                                   save_weights_only=True,\n",
    "                                   mode='auto',\n",
    "                                   period=1)\n",
    "\n",
    "# model_checkpoint.best to the best validation loss from the previous training\n",
    "# model_checkpoint.best = 4.83704\n",
    "\n",
    "csv_logger = CSVLogger(filename=os.path.join(checkpoint_path, 'ssd_augm_beta_0_01_pool1_global_pool_SGD_training_log.csv'),\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "\n",
    "learning_rate_scheduler = LearningRateScheduler(schedule=lr_schedule,\n",
    "                                                verbose=1)\n",
    "\n",
    "terminate_on_nan = TerminateOnNaN()\n",
    "\n",
    "TensorBoard_monitor = TensorBoard(log_dir=checkpoint_path)\n",
    "\n",
    "callbacks = [model_checkpoint,\n",
    "             csv_logger,\n",
    "             learning_rate_scheduler,\n",
    "             terminate_on_nan,\n",
    "             TensorBoard_monitor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.0005.\n",
      "1000/1000 [==============================] - 641s 641ms/step - loss: 11.2953 - pool1_GAP_substract_loss: 26569.5471 - pool2_GAP_substract_loss: 220108.8808 - pool3_GAP_substract_loss: 346073.6674 - conv4_3_norm_GAP_substract_loss: 6.5219 - fc7_GAP_substract_loss: 2.7016 - conv6_2_GAP_substract_loss: 0.8000 - conv7_2_GAP_substract_loss: 0.3945 - conv8_2_GAP_substract_loss: 0.9671 - conv9_2_GAP_substract_loss: 3.1696 - predictions_loss: 7.9509 - val_loss: 10.1190 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 7.0563\n",
      "\n",
      "Epoch 00001: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-01_loss-11.2953_val_loss-10.1190.h5\n",
      "Epoch 2/120\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.0005.\n",
      "1000/1000 [==============================] - 630s 630ms/step - loss: 9.2461 - pool1_GAP_substract_loss: 13806.1199 - pool2_GAP_substract_loss: 127235.2420 - pool3_GAP_substract_loss: 140177.0714 - conv4_3_norm_GAP_substract_loss: 5.9571 - fc7_GAP_substract_loss: 4.2091 - conv6_2_GAP_substract_loss: 2.2038 - conv7_2_GAP_substract_loss: 1.1463 - conv8_2_GAP_substract_loss: 3.0665 - conv9_2_GAP_substract_loss: 11.4819 - predictions_loss: 6.0607 - val_loss: 9.6561 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.6239\n",
      "\n",
      "Epoch 00002: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-02_loss-9.2461_val_loss-9.6561.h5\n",
      "Epoch 3/120\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 629s 629ms/step - loss: 8.9009 - pool1_GAP_substract_loss: 8496.7210 - pool2_GAP_substract_loss: 73273.2197 - pool3_GAP_substract_loss: 70314.2552 - conv4_3_norm_GAP_substract_loss: 6.9695 - fc7_GAP_substract_loss: 4.4795 - conv6_2_GAP_substract_loss: 2.7483 - conv7_2_GAP_substract_loss: 1.7799 - conv8_2_GAP_substract_loss: 4.5840 - conv9_2_GAP_substract_loss: 16.1418 - predictions_loss: 5.8120 - val_loss: 9.2377 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.2620\n",
      "\n",
      "Epoch 00003: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-03_loss-8.9009_val_loss-9.2377.h5\n",
      "Epoch 4/120\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 629s 629ms/step - loss: 8.4442 - pool1_GAP_substract_loss: 5578.5268 - pool2_GAP_substract_loss: 44028.4598 - pool3_GAP_substract_loss: 37528.6398 - conv4_3_norm_GAP_substract_loss: 7.0167 - fc7_GAP_substract_loss: 4.5078 - conv6_2_GAP_substract_loss: 2.8582 - conv7_2_GAP_substract_loss: 2.0721 - conv8_2_GAP_substract_loss: 5.3005 - conv9_2_GAP_substract_loss: 17.9867 - predictions_loss: 5.4402 - val_loss: 8.9021 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9811\n",
      "\n",
      "Epoch 00004: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-04_loss-8.4442_val_loss-8.9021.h5\n",
      "Epoch 5/120\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 628s 628ms/step - loss: 8.1890 - pool1_GAP_substract_loss: 4220.5812 - pool2_GAP_substract_loss: 31385.3266 - pool3_GAP_substract_loss: 25022.0366 - conv4_3_norm_GAP_substract_loss: 7.4390 - fc7_GAP_substract_loss: 4.6224 - conv6_2_GAP_substract_loss: 3.1353 - conv7_2_GAP_substract_loss: 2.4530 - conv8_2_GAP_substract_loss: 6.4979 - conv9_2_GAP_substract_loss: 20.8579 - predictions_loss: 5.2524 - val_loss: 8.9062 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.0381\n",
      "\n",
      "Epoch 00005: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-05_loss-8.1890_val_loss-8.9062.h5\n",
      "Epoch 6/120\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 628s 628ms/step - loss: 8.0088 - pool1_GAP_substract_loss: 3392.6246 - pool2_GAP_substract_loss: 25158.0654 - pool3_GAP_substract_loss: 17284.1370 - conv4_3_norm_GAP_substract_loss: 7.4084 - fc7_GAP_substract_loss: 4.5866 - conv6_2_GAP_substract_loss: 3.2965 - conv7_2_GAP_substract_loss: 2.6057 - conv8_2_GAP_substract_loss: 6.9592 - conv9_2_GAP_substract_loss: 21.5858 - predictions_loss: 5.1328 - val_loss: 8.7166 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9002\n",
      "\n",
      "Epoch 00006: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-06_loss-8.0088_val_loss-8.7166.h5\n",
      "Epoch 7/120\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 628s 628ms/step - loss: 7.7883 - pool1_GAP_substract_loss: 2759.3665 - pool2_GAP_substract_loss: 20411.7268 - pool3_GAP_substract_loss: 13088.1564 - conv4_3_norm_GAP_substract_loss: 7.3483 - fc7_GAP_substract_loss: 4.9032 - conv6_2_GAP_substract_loss: 3.4758 - conv7_2_GAP_substract_loss: 2.7721 - conv8_2_GAP_substract_loss: 7.3180 - conv9_2_GAP_substract_loss: 22.7329 - predictions_loss: 4.9695 - val_loss: 8.6086 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8424\n",
      "\n",
      "Epoch 00007: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-07_loss-7.7883_val_loss-8.6086.h5\n",
      "Epoch 8/120\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 627s 627ms/step - loss: 7.6937 - pool1_GAP_substract_loss: 2392.6400 - pool2_GAP_substract_loss: 17361.9313 - pool3_GAP_substract_loss: 10109.8786 - conv4_3_norm_GAP_substract_loss: 7.3629 - fc7_GAP_substract_loss: 4.7542 - conv6_2_GAP_substract_loss: 3.6563 - conv7_2_GAP_substract_loss: 2.9619 - conv8_2_GAP_substract_loss: 7.7769 - conv9_2_GAP_substract_loss: 23.0962 - predictions_loss: 4.9282 - val_loss: 8.4110 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6939\n",
      "\n",
      "Epoch 00008: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-08_loss-7.6937_val_loss-8.4110.h5\n",
      "Epoch 9/120\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 626s 626ms/step - loss: 7.5228 - pool1_GAP_substract_loss: 2170.0316 - pool2_GAP_substract_loss: 15263.3375 - pool3_GAP_substract_loss: 8154.0612 - conv4_3_norm_GAP_substract_loss: 7.6249 - fc7_GAP_substract_loss: 4.8103 - conv6_2_GAP_substract_loss: 3.7205 - conv7_2_GAP_substract_loss: 3.1345 - conv8_2_GAP_substract_loss: 8.3582 - conv9_2_GAP_substract_loss: 24.9968 - predictions_loss: 4.8080 - val_loss: 8.5639 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8946\n",
      "\n",
      "Epoch 00009: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-09_loss-7.5228_val_loss-8.5639.h5\n",
      "Epoch 10/120\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 627s 627ms/step - loss: 7.3965 - pool1_GAP_substract_loss: 1977.8860 - pool2_GAP_substract_loss: 13585.1015 - pool3_GAP_substract_loss: 6918.3072 - conv4_3_norm_GAP_substract_loss: 7.2959 - fc7_GAP_substract_loss: 4.9427 - conv6_2_GAP_substract_loss: 3.8091 - conv7_2_GAP_substract_loss: 3.3401 - conv8_2_GAP_substract_loss: 9.0832 - conv9_2_GAP_substract_loss: 27.1071 - predictions_loss: 4.7308 - val_loss: 8.4475 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8249\n",
      "\n",
      "Epoch 00010: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-10_loss-7.3965_val_loss-8.4475.h5\n",
      "Epoch 11/120\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 627s 627ms/step - loss: 7.3438 - pool1_GAP_substract_loss: 1871.5460 - pool2_GAP_substract_loss: 12636.9264 - pool3_GAP_substract_loss: 5932.4030 - conv4_3_norm_GAP_substract_loss: 7.4999 - fc7_GAP_substract_loss: 4.7591 - conv6_2_GAP_substract_loss: 3.7808 - conv7_2_GAP_substract_loss: 3.4165 - conv8_2_GAP_substract_loss: 9.3143 - conv9_2_GAP_substract_loss: 27.8688 - predictions_loss: 4.7253 - val_loss: 8.2186 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6416\n",
      "\n",
      "Epoch 00011: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-11_loss-7.3438_val_loss-8.2186.h5\n",
      "Epoch 12/120\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 626s 626ms/step - loss: 7.2123 - pool1_GAP_substract_loss: 1722.2346 - pool2_GAP_substract_loss: 11326.7088 - pool3_GAP_substract_loss: 4872.1485 - conv4_3_norm_GAP_substract_loss: 7.1880 - fc7_GAP_substract_loss: 4.5876 - conv6_2_GAP_substract_loss: 3.7819 - conv7_2_GAP_substract_loss: 3.4681 - conv8_2_GAP_substract_loss: 9.5027 - conv9_2_GAP_substract_loss: 27.8674 - predictions_loss: 4.6404 - val_loss: 8.3885 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8560\n",
      "\n",
      "Epoch 00012: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-12_loss-7.2123_val_loss-8.3885.h5\n",
      "Epoch 13/120\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 628s 628ms/step - loss: 7.1135 - pool1_GAP_substract_loss: 1666.3610 - pool2_GAP_substract_loss: 10591.9182 - pool3_GAP_substract_loss: 4258.4170 - conv4_3_norm_GAP_substract_loss: 7.3742 - fc7_GAP_substract_loss: 4.7205 - conv6_2_GAP_substract_loss: 3.8325 - conv7_2_GAP_substract_loss: 3.5398 - conv8_2_GAP_substract_loss: 9.7806 - conv9_2_GAP_substract_loss: 29.0120 - predictions_loss: 4.5862 - val_loss: 8.1715 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6825\n",
      "\n",
      "Epoch 00013: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-13_loss-7.1135_val_loss-8.1715.h5\n",
      "Epoch 14/120\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 628s 628ms/step - loss: 7.0429 - pool1_GAP_substract_loss: 1549.8805 - pool2_GAP_substract_loss: 9571.7569 - pool3_GAP_substract_loss: 3646.4409 - conv4_3_norm_GAP_substract_loss: 7.6976 - fc7_GAP_substract_loss: 4.5322 - conv6_2_GAP_substract_loss: 3.7200 - conv7_2_GAP_substract_loss: 3.4912 - conv8_2_GAP_substract_loss: 9.3199 - conv9_2_GAP_substract_loss: 27.9728 - predictions_loss: 4.5598 - val_loss: 8.0726 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6262\n",
      "\n",
      "Epoch 00014: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-14_loss-7.0429_val_loss-8.0726.h5\n",
      "Epoch 15/120\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 627s 627ms/step - loss: 6.9243 - pool1_GAP_substract_loss: 1507.2665 - pool2_GAP_substract_loss: 9020.2648 - pool3_GAP_substract_loss: 3184.7537 - conv4_3_norm_GAP_substract_loss: 7.8177 - fc7_GAP_substract_loss: 4.6791 - conv6_2_GAP_substract_loss: 3.8665 - conv7_2_GAP_substract_loss: 3.7330 - conv8_2_GAP_substract_loss: 10.0469 - conv9_2_GAP_substract_loss: 29.4593 - predictions_loss: 4.4836 - val_loss: 8.1802 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7753\n",
      "\n",
      "Epoch 00015: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-15_loss-6.9243_val_loss-8.1802.h5\n",
      "Epoch 16/120\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 627s 627ms/step - loss: 6.8732 - pool1_GAP_substract_loss: 1410.2759 - pool2_GAP_substract_loss: 8271.0214 - pool3_GAP_substract_loss: 2834.3368 - conv4_3_norm_GAP_substract_loss: 7.5750 - fc7_GAP_substract_loss: 4.4613 - conv6_2_GAP_substract_loss: 3.7426 - conv7_2_GAP_substract_loss: 3.6930 - conv8_2_GAP_substract_loss: 10.1231 - conv9_2_GAP_substract_loss: 29.1496 - predictions_loss: 4.4746 - val_loss: 7.9240 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.5597\n",
      "\n",
      "Epoch 00016: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-16_loss-6.8732_val_loss-7.9240.h5\n",
      "Epoch 17/120\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 627s 627ms/step - loss: 6.7855 - pool1_GAP_substract_loss: 1397.0766 - pool2_GAP_substract_loss: 7849.0308 - pool3_GAP_substract_loss: 2540.7550 - conv4_3_norm_GAP_substract_loss: 7.6333 - fc7_GAP_substract_loss: 4.4255 - conv6_2_GAP_substract_loss: 3.7870 - conv7_2_GAP_substract_loss: 3.8582 - conv8_2_GAP_substract_loss: 10.5676 - conv9_2_GAP_substract_loss: 29.8151 - predictions_loss: 4.4272 - val_loss: 7.8942 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.5697\n",
      "\n",
      "Epoch 00017: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-17_loss-6.7855_val_loss-7.8942.h5\n",
      "Epoch 18/120\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 626s 626ms/step - loss: 6.6889 - pool1_GAP_substract_loss: 1349.1360 - pool2_GAP_substract_loss: 7619.6247 - pool3_GAP_substract_loss: 2257.1155 - conv4_3_norm_GAP_substract_loss: 7.9444 - fc7_GAP_substract_loss: 4.4016 - conv6_2_GAP_substract_loss: 3.8230 - conv7_2_GAP_substract_loss: 3.8909 - conv8_2_GAP_substract_loss: 10.4605 - conv9_2_GAP_substract_loss: 28.9063 - predictions_loss: 4.3703 - val_loss: 7.7743 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.4885\n",
      "\n",
      "Epoch 00018: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-18_loss-6.6889_val_loss-7.7743.h5\n",
      "Epoch 19/120\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 626s 626ms/step - loss: 6.6155 - pool1_GAP_substract_loss: 1294.1835 - pool2_GAP_substract_loss: 7219.2614 - pool3_GAP_substract_loss: 2095.3137 - conv4_3_norm_GAP_substract_loss: 8.0587 - fc7_GAP_substract_loss: 4.3602 - conv6_2_GAP_substract_loss: 3.8691 - conv7_2_GAP_substract_loss: 3.9658 - conv8_2_GAP_substract_loss: 10.7662 - conv9_2_GAP_substract_loss: 29.4512 - predictions_loss: 4.3358 - val_loss: 7.6944 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.4466\n",
      "\n",
      "Epoch 00019: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-19_loss-6.6155_val_loss-7.6944.h5\n",
      "Epoch 20/120\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 6.5360 - pool1_GAP_substract_loss: 1281.1781 - pool2_GAP_substract_loss: 6977.1983 - pool3_GAP_substract_loss: 1914.3542 - conv4_3_norm_GAP_substract_loss: 8.1854 - fc7_GAP_substract_loss: 4.3398 - conv6_2_GAP_substract_loss: 3.8678 - conv7_2_GAP_substract_loss: 4.0257 - conv8_2_GAP_substract_loss: 10.7449 - conv9_2_GAP_substract_loss: 28.6628 - predictions_loss: 4.2939 - val_loss: 7.8148 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6041\n",
      "\n",
      "Epoch 00020: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-20_loss-6.5360_val_loss-7.8148.h5\n",
      "Epoch 21/120\n",
      "\n",
      "Epoch 00021: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 633s 633ms/step - loss: 6.4997 - pool1_GAP_substract_loss: 1223.5357 - pool2_GAP_substract_loss: 6489.9568 - pool3_GAP_substract_loss: 1694.3961 - conv4_3_norm_GAP_substract_loss: 8.3432 - fc7_GAP_substract_loss: 4.2977 - conv6_2_GAP_substract_loss: 3.8815 - conv7_2_GAP_substract_loss: 4.1344 - conv8_2_GAP_substract_loss: 10.9742 - conv9_2_GAP_substract_loss: 28.5693 - predictions_loss: 4.2949 - val_loss: 7.7973 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6227\n",
      "\n",
      "Epoch 00021: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-21_loss-6.4997_val_loss-7.7973.h5\n",
      "Epoch 22/120\n",
      "\n",
      "Epoch 00022: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 627s 627ms/step - loss: 6.4348 - pool1_GAP_substract_loss: 1211.6535 - pool2_GAP_substract_loss: 6330.1795 - pool3_GAP_substract_loss: 1575.0565 - conv4_3_norm_GAP_substract_loss: 8.1333 - fc7_GAP_substract_loss: 4.2539 - conv6_2_GAP_substract_loss: 3.9756 - conv7_2_GAP_substract_loss: 4.2358 - conv8_2_GAP_substract_loss: 11.4998 - conv9_2_GAP_substract_loss: 30.0188 - predictions_loss: 4.2657 - val_loss: 7.7074 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.5680\n",
      "\n",
      "Epoch 00022: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-22_loss-6.4348_val_loss-7.7074.h5\n",
      "Epoch 23/120\n",
      "\n",
      "Epoch 00023: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 633s 633ms/step - loss: 6.3883 - pool1_GAP_substract_loss: 1172.7858 - pool2_GAP_substract_loss: 6023.7963 - pool3_GAP_substract_loss: 1443.3133 - conv4_3_norm_GAP_substract_loss: 8.1287 - fc7_GAP_substract_loss: 4.1479 - conv6_2_GAP_substract_loss: 3.9545 - conv7_2_GAP_substract_loss: 4.2889 - conv8_2_GAP_substract_loss: 11.6359 - conv9_2_GAP_substract_loss: 30.0444 - predictions_loss: 4.2545 - val_loss: 7.8196 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7147\n",
      "\n",
      "Epoch 00023: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-23_loss-6.3883_val_loss-7.8196.h5\n",
      "Epoch 24/120\n",
      "\n",
      "Epoch 00024: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 626s 626ms/step - loss: 6.2927 - pool1_GAP_substract_loss: 1174.8985 - pool2_GAP_substract_loss: 5876.4887 - pool3_GAP_substract_loss: 1355.9845 - conv4_3_norm_GAP_substract_loss: 8.6688 - fc7_GAP_substract_loss: 4.2919 - conv6_2_GAP_substract_loss: 3.9992 - conv7_2_GAP_substract_loss: 4.3620 - conv8_2_GAP_substract_loss: 12.0244 - conv9_2_GAP_substract_loss: 30.5595 - predictions_loss: 4.1930 - val_loss: 7.7008 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6296\n",
      "\n",
      "Epoch 00024: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-24_loss-6.2927_val_loss-7.7008.h5\n",
      "Epoch 25/120\n",
      "\n",
      "Epoch 00025: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 627s 627ms/step - loss: 6.2557 - pool1_GAP_substract_loss: 1130.5884 - pool2_GAP_substract_loss: 5670.3403 - pool3_GAP_substract_loss: 1189.2545 - conv4_3_norm_GAP_substract_loss: 8.2148 - fc7_GAP_substract_loss: 4.1761 - conv6_2_GAP_substract_loss: 4.0061 - conv7_2_GAP_substract_loss: 4.3584 - conv8_2_GAP_substract_loss: 11.7270 - conv9_2_GAP_substract_loss: 29.2252 - predictions_loss: 4.1897 - val_loss: 7.5984 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.5603\n",
      "\n",
      "Epoch 00025: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-25_loss-6.2557_val_loss-7.5984.h5\n",
      "Epoch 26/120\n",
      "\n",
      "Epoch 00026: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 626s 626ms/step - loss: 6.2102 - pool1_GAP_substract_loss: 1113.2075 - pool2_GAP_substract_loss: 5511.4981 - pool3_GAP_substract_loss: 1122.7526 - conv4_3_norm_GAP_substract_loss: 8.6217 - fc7_GAP_substract_loss: 4.2168 - conv6_2_GAP_substract_loss: 4.1115 - conv7_2_GAP_substract_loss: 4.4826 - conv8_2_GAP_substract_loss: 12.1721 - conv9_2_GAP_substract_loss: 30.2401 - predictions_loss: 4.1769 - val_loss: 7.6894 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6834\n",
      "\n",
      "Epoch 00026: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-26_loss-6.2102_val_loss-7.6894.h5\n",
      "Epoch 27/120\n",
      "\n",
      "Epoch 00027: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 638s 638ms/step - loss: 6.1334 - pool1_GAP_substract_loss: 1112.2334 - pool2_GAP_substract_loss: 5313.9304 - pool3_GAP_substract_loss: 1058.5965 - conv4_3_norm_GAP_substract_loss: 8.8789 - fc7_GAP_substract_loss: 4.0717 - conv6_2_GAP_substract_loss: 4.0175 - conv7_2_GAP_substract_loss: 4.5111 - conv8_2_GAP_substract_loss: 12.3294 - conv9_2_GAP_substract_loss: 30.3660 - predictions_loss: 4.1320 - val_loss: 7.7803 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8056\n",
      "\n",
      "Epoch 00027: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-27_loss-6.1334_val_loss-7.7803.h5\n",
      "Epoch 28/120\n",
      "\n",
      "Epoch 00028: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 645s 645ms/step - loss: 6.0884 - pool1_GAP_substract_loss: 1096.0055 - pool2_GAP_substract_loss: 5151.6087 - pool3_GAP_substract_loss: 980.5828 - conv4_3_norm_GAP_substract_loss: 8.8241 - fc7_GAP_substract_loss: 4.0708 - conv6_2_GAP_substract_loss: 4.0087 - conv7_2_GAP_substract_loss: 4.4576 - conv8_2_GAP_substract_loss: 12.3277 - conv9_2_GAP_substract_loss: 29.9851 - predictions_loss: 4.1182 - val_loss: 7.5129 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.5690\n",
      "\n",
      "Epoch 00028: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-28_loss-6.0884_val_loss-7.5129.h5\n",
      "Epoch 29/120\n",
      "\n",
      "Epoch 00029: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 627s 627ms/step - loss: 6.0478 - pool1_GAP_substract_loss: 1066.3163 - pool2_GAP_substract_loss: 5008.1139 - pool3_GAP_substract_loss: 936.4987 - conv4_3_norm_GAP_substract_loss: 8.8748 - fc7_GAP_substract_loss: 4.1109 - conv6_2_GAP_substract_loss: 4.0500 - conv7_2_GAP_substract_loss: 4.4896 - conv8_2_GAP_substract_loss: 12.2501 - conv9_2_GAP_substract_loss: 28.9071 - predictions_loss: 4.1082 - val_loss: 7.5252 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6112\n",
      "\n",
      "Epoch 00029: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-29_loss-6.0478_val_loss-7.5252.h5\n",
      "Epoch 30/120\n",
      "\n",
      "Epoch 00030: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 626s 626ms/step - loss: 5.9817 - pool1_GAP_substract_loss: 1051.0494 - pool2_GAP_substract_loss: 4918.4134 - pool3_GAP_substract_loss: 914.4301 - conv4_3_norm_GAP_substract_loss: 8.8403 - fc7_GAP_substract_loss: 4.0189 - conv6_2_GAP_substract_loss: 4.0274 - conv7_2_GAP_substract_loss: 4.5589 - conv8_2_GAP_substract_loss: 12.4485 - conv9_2_GAP_substract_loss: 28.8143 - predictions_loss: 4.0720 - val_loss: 7.4814 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.5968\n",
      "\n",
      "Epoch 00030: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-30_loss-5.9817_val_loss-7.4814.h5\n",
      "Epoch 31/120\n",
      "\n",
      "Epoch 00031: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 634s 634ms/step - loss: 5.9579 - pool1_GAP_substract_loss: 1025.1848 - pool2_GAP_substract_loss: 4713.7040 - pool3_GAP_substract_loss: 823.2541 - conv4_3_norm_GAP_substract_loss: 8.7420 - fc7_GAP_substract_loss: 3.9633 - conv6_2_GAP_substract_loss: 4.0439 - conv7_2_GAP_substract_loss: 4.5946 - conv8_2_GAP_substract_loss: 12.2454 - conv9_2_GAP_substract_loss: 27.0024 - predictions_loss: 4.0773 - val_loss: 7.3984 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.5423\n",
      "\n",
      "Epoch 00031: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-31_loss-5.9579_val_loss-7.3984.h5\n",
      "Epoch 32/120\n",
      "\n",
      "Epoch 00032: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 627s 627ms/step - loss: 5.8927 - pool1_GAP_substract_loss: 1032.8246 - pool2_GAP_substract_loss: 4595.5918 - pool3_GAP_substract_loss: 776.1586 - conv4_3_norm_GAP_substract_loss: 8.7389 - fc7_GAP_substract_loss: 3.9701 - conv6_2_GAP_substract_loss: 4.0077 - conv7_2_GAP_substract_loss: 4.5346 - conv8_2_GAP_substract_loss: 12.4183 - conv9_2_GAP_substract_loss: 27.9685 - predictions_loss: 4.0404 - val_loss: 7.6229 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7949\n",
      "\n",
      "Epoch 00032: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-32_loss-5.8927_val_loss-7.6229.h5\n",
      "Epoch 33/120\n",
      "\n",
      "Epoch 00033: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 650s 650ms/step - loss: 5.8882 - pool1_GAP_substract_loss: 1021.5429 - pool2_GAP_substract_loss: 4463.5277 - pool3_GAP_substract_loss: 730.5579 - conv4_3_norm_GAP_substract_loss: 9.2139 - fc7_GAP_substract_loss: 3.9537 - conv6_2_GAP_substract_loss: 4.0594 - conv7_2_GAP_substract_loss: 4.6123 - conv8_2_GAP_substract_loss: 12.1924 - conv9_2_GAP_substract_loss: 26.3878 - predictions_loss: 4.0637 - val_loss: 7.6957 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8949\n",
      "\n",
      "Epoch 00033: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-33_loss-5.8882_val_loss-7.6957.h5\n",
      "Epoch 34/120\n",
      "\n",
      "Epoch 00034: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 648s 648ms/step - loss: 5.7859 - pool1_GAP_substract_loss: 1017.2485 - pool2_GAP_substract_loss: 4384.0316 - pool3_GAP_substract_loss: 691.2226 - conv4_3_norm_GAP_substract_loss: 9.1016 - fc7_GAP_substract_loss: 3.9698 - conv6_2_GAP_substract_loss: 4.1204 - conv7_2_GAP_substract_loss: 4.6375 - conv8_2_GAP_substract_loss: 12.3608 - conv9_2_GAP_substract_loss: 27.1288 - predictions_loss: 3.9884 - val_loss: 7.7178 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9439\n",
      "\n",
      "Epoch 00034: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-34_loss-5.7859_val_loss-7.7178.h5\n",
      "Epoch 35/120\n",
      "\n",
      "Epoch 00035: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 627s 627ms/step - loss: 5.7522 - pool1_GAP_substract_loss: 974.6422 - pool2_GAP_substract_loss: 4176.8122 - pool3_GAP_substract_loss: 638.7396 - conv4_3_norm_GAP_substract_loss: 9.0396 - fc7_GAP_substract_loss: 3.9473 - conv6_2_GAP_substract_loss: 4.1020 - conv7_2_GAP_substract_loss: 4.6551 - conv8_2_GAP_substract_loss: 12.3056 - conv9_2_GAP_substract_loss: 26.6174 - predictions_loss: 3.9816 - val_loss: 7.3866 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6387\n",
      "\n",
      "Epoch 00035: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-35_loss-5.7522_val_loss-7.3866.h5\n",
      "Epoch 36/120\n",
      "\n",
      "Epoch 00036: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 626s 626ms/step - loss: 5.6938 - pool1_GAP_substract_loss: 977.3401 - pool2_GAP_substract_loss: 4115.8861 - pool3_GAP_substract_loss: 623.6613 - conv4_3_norm_GAP_substract_loss: 9.2179 - fc7_GAP_substract_loss: 3.9183 - conv6_2_GAP_substract_loss: 4.1645 - conv7_2_GAP_substract_loss: 4.7207 - conv8_2_GAP_substract_loss: 12.3109 - conv9_2_GAP_substract_loss: 25.9437 - predictions_loss: 3.9489 - val_loss: 7.3734 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6510\n",
      "\n",
      "Epoch 00036: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-36_loss-5.6938_val_loss-7.3734.h5\n",
      "Epoch 37/120\n",
      "\n",
      "Epoch 00037: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 627s 627ms/step - loss: 5.6885 - pool1_GAP_substract_loss: 1001.4853 - pool2_GAP_substract_loss: 4075.2933 - pool3_GAP_substract_loss: 606.6378 - conv4_3_norm_GAP_substract_loss: 9.2239 - fc7_GAP_substract_loss: 3.9722 - conv6_2_GAP_substract_loss: 4.2073 - conv7_2_GAP_substract_loss: 4.8692 - conv8_2_GAP_substract_loss: 12.6799 - conv9_2_GAP_substract_loss: 26.3206 - predictions_loss: 3.9684 - val_loss: 7.4616 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7639\n",
      "\n",
      "Epoch 00037: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-37_loss-5.6885_val_loss-7.4616.h5\n",
      "Epoch 38/120\n",
      "\n",
      "Epoch 00038: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 646s 646ms/step - loss: 5.6215 - pool1_GAP_substract_loss: 990.0946 - pool2_GAP_substract_loss: 4041.4544 - pool3_GAP_substract_loss: 584.3982 - conv4_3_norm_GAP_substract_loss: 9.5961 - fc7_GAP_substract_loss: 4.0678 - conv6_2_GAP_substract_loss: 4.2907 - conv7_2_GAP_substract_loss: 4.9597 - conv8_2_GAP_substract_loss: 12.8231 - conv9_2_GAP_substract_loss: 26.3004 - predictions_loss: 3.9261 - val_loss: 7.5662 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8929\n",
      "\n",
      "Epoch 00038: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-38_loss-5.6215_val_loss-7.5662.h5\n",
      "Epoch 39/120\n",
      "\n",
      "Epoch 00039: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 626s 626ms/step - loss: 5.6209 - pool1_GAP_substract_loss: 958.6639 - pool2_GAP_substract_loss: 3787.5977 - pool3_GAP_substract_loss: 543.9769 - conv4_3_norm_GAP_substract_loss: 9.6448 - fc7_GAP_substract_loss: 3.9499 - conv6_2_GAP_substract_loss: 4.1493 - conv7_2_GAP_substract_loss: 4.8658 - conv8_2_GAP_substract_loss: 12.6645 - conv9_2_GAP_substract_loss: 25.4679 - predictions_loss: 3.9499 - val_loss: 7.3986 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7490\n",
      "\n",
      "Epoch 00039: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-39_loss-5.6209_val_loss-7.3986.h5\n",
      "Epoch 40/120\n",
      "\n",
      "Epoch 00040: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 626s 626ms/step - loss: 5.5538 - pool1_GAP_substract_loss: 950.8431 - pool2_GAP_substract_loss: 3706.7697 - pool3_GAP_substract_loss: 518.9265 - conv4_3_norm_GAP_substract_loss: 9.9502 - fc7_GAP_substract_loss: 3.9656 - conv6_2_GAP_substract_loss: 4.2032 - conv7_2_GAP_substract_loss: 4.9750 - conv8_2_GAP_substract_loss: 13.0538 - conv9_2_GAP_substract_loss: 25.5849 - predictions_loss: 3.9063 - val_loss: 7.6298 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.0033\n",
      "\n",
      "Epoch 00040: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-40_loss-5.5538_val_loss-7.6298.h5\n",
      "Epoch 41/120\n",
      "\n",
      "Epoch 00041: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 627s 627ms/step - loss: 5.5176 - pool1_GAP_substract_loss: 952.4541 - pool2_GAP_substract_loss: 3685.1780 - pool3_GAP_substract_loss: 490.7750 - conv4_3_norm_GAP_substract_loss: 9.6909 - fc7_GAP_substract_loss: 3.9661 - conv6_2_GAP_substract_loss: 4.2220 - conv7_2_GAP_substract_loss: 4.9304 - conv8_2_GAP_substract_loss: 12.8626 - conv9_2_GAP_substract_loss: 24.8411 - predictions_loss: 3.8930 - val_loss: 7.6576 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.0537\n",
      "\n",
      "Epoch 00041: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-41_loss-5.5176_val_loss-7.6576.h5\n",
      "Epoch 42/120\n",
      "\n",
      "Epoch 00042: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 631s 631ms/step - loss: 5.4899 - pool1_GAP_substract_loss: 931.7863 - pool2_GAP_substract_loss: 3501.9277 - pool3_GAP_substract_loss: 486.0879 - conv4_3_norm_GAP_substract_loss: 9.5858 - fc7_GAP_substract_loss: 4.0082 - conv6_2_GAP_substract_loss: 4.3106 - conv7_2_GAP_substract_loss: 5.0803 - conv8_2_GAP_substract_loss: 13.0606 - conv9_2_GAP_substract_loss: 24.9945 - predictions_loss: 3.8877 - val_loss: 7.5847 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.0028\n",
      "\n",
      "Epoch 00042: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-42_loss-5.4899_val_loss-7.5847.h5\n",
      "Epoch 43/120\n",
      "\n",
      "Epoch 00043: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 628s 628ms/step - loss: 5.4505 - pool1_GAP_substract_loss: 923.0706 - pool2_GAP_substract_loss: 3465.9016 - pool3_GAP_substract_loss: 464.2128 - conv4_3_norm_GAP_substract_loss: 9.4786 - fc7_GAP_substract_loss: 3.9874 - conv6_2_GAP_substract_loss: 4.3318 - conv7_2_GAP_substract_loss: 5.1977 - conv8_2_GAP_substract_loss: 13.3995 - conv9_2_GAP_substract_loss: 25.3545 - predictions_loss: 3.8702 - val_loss: 7.4087 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8485\n",
      "\n",
      "Epoch 00043: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-43_loss-5.4505_val_loss-7.4087.h5\n",
      "Epoch 44/120\n",
      "\n",
      "Epoch 00044: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 632s 632ms/step - loss: 5.4070 - pool1_GAP_substract_loss: 892.8301 - pool2_GAP_substract_loss: 3496.4721 - pool3_GAP_substract_loss: 458.3598 - conv4_3_norm_GAP_substract_loss: 9.6069 - fc7_GAP_substract_loss: 3.9624 - conv6_2_GAP_substract_loss: 4.2896 - conv7_2_GAP_substract_loss: 5.2359 - conv8_2_GAP_substract_loss: 13.5574 - conv9_2_GAP_substract_loss: 25.2998 - predictions_loss: 3.8485 - val_loss: 7.3875 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8485\n",
      "\n",
      "Epoch 00044: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-44_loss-5.4070_val_loss-7.3875.h5\n",
      "Epoch 45/120\n",
      "\n",
      "Epoch 00045: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 654s 654ms/step - loss: 5.3623 - pool1_GAP_substract_loss: 909.9343 - pool2_GAP_substract_loss: 3349.7558 - pool3_GAP_substract_loss: 425.3329 - conv4_3_norm_GAP_substract_loss: 9.8403 - fc7_GAP_substract_loss: 4.0676 - conv6_2_GAP_substract_loss: 4.4453 - conv7_2_GAP_substract_loss: 5.4013 - conv8_2_GAP_substract_loss: 14.0439 - conv9_2_GAP_substract_loss: 26.2080 - predictions_loss: 3.8246 - val_loss: 7.2763 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7580\n",
      "\n",
      "Epoch 00045: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-45_loss-5.3623_val_loss-7.2763.h5\n",
      "Epoch 46/120\n",
      "\n",
      "Epoch 00046: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 628s 628ms/step - loss: 5.1730 - pool1_GAP_substract_loss: 881.8712 - pool2_GAP_substract_loss: 3414.7764 - pool3_GAP_substract_loss: 437.0953 - conv4_3_norm_GAP_substract_loss: 9.8152 - fc7_GAP_substract_loss: 4.2694 - conv6_2_GAP_substract_loss: 4.7062 - conv7_2_GAP_substract_loss: 5.7336 - conv8_2_GAP_substract_loss: 14.9055 - conv9_2_GAP_substract_loss: 27.2447 - predictions_loss: 3.6473 - val_loss: 7.2471 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7314\n",
      "\n",
      "Epoch 00046: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-46_loss-5.1730_val_loss-7.2471.h5\n",
      "Epoch 47/120\n",
      "\n",
      "Epoch 00047: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 667s 667ms/step - loss: 5.1355 - pool1_GAP_substract_loss: 870.2568 - pool2_GAP_substract_loss: 3475.8178 - pool3_GAP_substract_loss: 451.2287 - conv4_3_norm_GAP_substract_loss: 9.6626 - fc7_GAP_substract_loss: 4.3541 - conv6_2_GAP_substract_loss: 4.8443 - conv7_2_GAP_substract_loss: 5.9231 - conv8_2_GAP_substract_loss: 15.0240 - conv9_2_GAP_substract_loss: 26.5569 - predictions_loss: 3.6124 - val_loss: 7.2285 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7153\n",
      "\n",
      "Epoch 00047: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-47_loss-5.1355_val_loss-7.2285.h5\n",
      "Epoch 48/120\n",
      "\n",
      "Epoch 00048: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 5.1291 - pool1_GAP_substract_loss: 868.9299 - pool2_GAP_substract_loss: 3596.6349 - pool3_GAP_substract_loss: 466.7921 - conv4_3_norm_GAP_substract_loss: 9.6806 - fc7_GAP_substract_loss: 4.4431 - conv6_2_GAP_substract_loss: 5.0268 - conv7_2_GAP_substract_loss: 6.1843 - conv8_2_GAP_substract_loss: 15.6996 - conv9_2_GAP_substract_loss: 27.6685 - predictions_loss: 3.6085 - val_loss: 7.2092 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6986\n",
      "\n",
      "Epoch 00048: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-48_loss-5.1291_val_loss-7.2092.h5\n",
      "Epoch 49/120\n",
      "\n",
      "Epoch 00049: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 626s 626ms/step - loss: 5.1189 - pool1_GAP_substract_loss: 867.5352 - pool2_GAP_substract_loss: 3648.8784 - pool3_GAP_substract_loss: 487.1984 - conv4_3_norm_GAP_substract_loss: 9.7167 - fc7_GAP_substract_loss: 4.5357 - conv6_2_GAP_substract_loss: 5.1634 - conv7_2_GAP_substract_loss: 6.3612 - conv8_2_GAP_substract_loss: 15.9947 - conv9_2_GAP_substract_loss: 27.5863 - predictions_loss: 3.6009 - val_loss: 7.1398 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6317\n",
      "\n",
      "Epoch 00049: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-49_loss-5.1189_val_loss-7.1398.h5\n",
      "Epoch 50/120\n",
      "\n",
      "Epoch 00050: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 626s 626ms/step - loss: 5.1035 - pool1_GAP_substract_loss: 866.4804 - pool2_GAP_substract_loss: 3694.3396 - pool3_GAP_substract_loss: 496.4269 - conv4_3_norm_GAP_substract_loss: 9.6559 - fc7_GAP_substract_loss: 4.5885 - conv6_2_GAP_substract_loss: 5.2903 - conv7_2_GAP_substract_loss: 6.5945 - conv8_2_GAP_substract_loss: 16.7328 - conv9_2_GAP_substract_loss: 29.1598 - predictions_loss: 3.5880 - val_loss: 7.2904 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7848\n",
      "\n",
      "Epoch 00050: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-50_loss-5.1035_val_loss-7.2904.h5\n",
      "Epoch 51/120\n",
      "\n",
      "Epoch 00051: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 5.0918 - pool1_GAP_substract_loss: 873.2675 - pool2_GAP_substract_loss: 3802.3394 - pool3_GAP_substract_loss: 503.7524 - conv4_3_norm_GAP_substract_loss: 9.7102 - fc7_GAP_substract_loss: 4.6780 - conv6_2_GAP_substract_loss: 5.4203 - conv7_2_GAP_substract_loss: 6.6828 - conv8_2_GAP_substract_loss: 16.5848 - conv9_2_GAP_substract_loss: 28.4562 - predictions_loss: 3.5788 - val_loss: 7.2571 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7541\n",
      "\n",
      "Epoch 00051: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-51_loss-5.0918_val_loss-7.2571.h5\n",
      "Epoch 52/120\n",
      "\n",
      "Epoch 00052: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 626s 626ms/step - loss: 5.0668 - pool1_GAP_substract_loss: 861.9535 - pool2_GAP_substract_loss: 3800.7777 - pool3_GAP_substract_loss: 512.8465 - conv4_3_norm_GAP_substract_loss: 9.6166 - fc7_GAP_substract_loss: 4.6275 - conv6_2_GAP_substract_loss: 5.3614 - conv7_2_GAP_substract_loss: 6.6050 - conv8_2_GAP_substract_loss: 16.4035 - conv9_2_GAP_substract_loss: 28.0981 - predictions_loss: 3.5564 - val_loss: 7.2610 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7605\n",
      "\n",
      "Epoch 00052: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-52_loss-5.0668_val_loss-7.2610.h5\n",
      "Epoch 53/120\n",
      "\n",
      "Epoch 00053: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 627s 627ms/step - loss: 5.0704 - pool1_GAP_substract_loss: 848.4997 - pool2_GAP_substract_loss: 3868.5586 - pool3_GAP_substract_loss: 523.2356 - conv4_3_norm_GAP_substract_loss: 9.7441 - fc7_GAP_substract_loss: 4.7075 - conv6_2_GAP_substract_loss: 5.4441 - conv7_2_GAP_substract_loss: 6.6277 - conv8_2_GAP_substract_loss: 16.1422 - conv9_2_GAP_substract_loss: 26.9624 - predictions_loss: 3.5626 - val_loss: 7.2524 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7544\n",
      "\n",
      "Epoch 00053: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-53_loss-5.0704_val_loss-7.2524.h5\n",
      "Epoch 54/120\n",
      "\n",
      "Epoch 00054: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 642s 642ms/step - loss: 5.0640 - pool1_GAP_substract_loss: 848.1400 - pool2_GAP_substract_loss: 3857.4502 - pool3_GAP_substract_loss: 521.7655 - conv4_3_norm_GAP_substract_loss: 9.5963 - fc7_GAP_substract_loss: 4.6802 - conv6_2_GAP_substract_loss: 5.4599 - conv7_2_GAP_substract_loss: 6.6827 - conv8_2_GAP_substract_loss: 16.4935 - conv9_2_GAP_substract_loss: 27.9578 - predictions_loss: 3.5587 - val_loss: 7.2973 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8018\n",
      "\n",
      "Epoch 00054: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-54_loss-5.0640_val_loss-7.2973.h5\n",
      "Epoch 55/120\n",
      "\n",
      "Epoch 00055: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 638s 638ms/step - loss: 5.0328 - pool1_GAP_substract_loss: 841.0041 - pool2_GAP_substract_loss: 3846.9542 - pool3_GAP_substract_loss: 526.0482 - conv4_3_norm_GAP_substract_loss: 9.6239 - fc7_GAP_substract_loss: 4.6901 - conv6_2_GAP_substract_loss: 5.4846 - conv7_2_GAP_substract_loss: 6.7513 - conv8_2_GAP_substract_loss: 16.8361 - conv9_2_GAP_substract_loss: 28.7652 - predictions_loss: 3.5301 - val_loss: 7.3076 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8146\n",
      "\n",
      "Epoch 00055: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-55_loss-5.0328_val_loss-7.3076.h5\n",
      "Epoch 56/120\n",
      "\n",
      "Epoch 00056: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 644s 644ms/step - loss: 5.0660 - pool1_GAP_substract_loss: 827.3326 - pool2_GAP_substract_loss: 3826.6513 - pool3_GAP_substract_loss: 533.8941 - conv4_3_norm_GAP_substract_loss: 9.6083 - fc7_GAP_substract_loss: 4.7762 - conv6_2_GAP_substract_loss: 5.6610 - conv7_2_GAP_substract_loss: 6.9888 - conv8_2_GAP_substract_loss: 17.4112 - conv9_2_GAP_substract_loss: 29.6020 - predictions_loss: 3.5659 - val_loss: 7.2729 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7823\n",
      "\n",
      "Epoch 00056: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-56_loss-5.0660_val_loss-7.2729.h5\n",
      "Epoch 57/120\n",
      "\n",
      "Epoch 00057: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 641s 641ms/step - loss: 5.0121 - pool1_GAP_substract_loss: 837.3646 - pool2_GAP_substract_loss: 3934.3177 - pool3_GAP_substract_loss: 547.1212 - conv4_3_norm_GAP_substract_loss: 9.8509 - fc7_GAP_substract_loss: 4.8495 - conv6_2_GAP_substract_loss: 5.6807 - conv7_2_GAP_substract_loss: 7.0238 - conv8_2_GAP_substract_loss: 17.4178 - conv9_2_GAP_substract_loss: 29.1746 - predictions_loss: 3.5144 - val_loss: 7.3324 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8443\n",
      "\n",
      "Epoch 00057: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-57_loss-5.0121_val_loss-7.3324.h5\n",
      "Epoch 58/120\n",
      "\n",
      "Epoch 00058: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 645s 645ms/step - loss: 5.0311 - pool1_GAP_substract_loss: 824.3736 - pool2_GAP_substract_loss: 3890.4525 - pool3_GAP_substract_loss: 548.1161 - conv4_3_norm_GAP_substract_loss: 9.7635 - fc7_GAP_substract_loss: 4.8663 - conv6_2_GAP_substract_loss: 5.7258 - conv7_2_GAP_substract_loss: 7.0513 - conv8_2_GAP_substract_loss: 17.3219 - conv9_2_GAP_substract_loss: 29.0672 - predictions_loss: 3.5359 - val_loss: 7.3008 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8151\n",
      "\n",
      "Epoch 00058: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-58_loss-5.0311_val_loss-7.3008.h5\n",
      "Epoch 59/120\n",
      "\n",
      "Epoch 00059: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 635s 635ms/step - loss: 5.0213 - pool1_GAP_substract_loss: 824.6381 - pool2_GAP_substract_loss: 3932.3056 - pool3_GAP_substract_loss: 551.3026 - conv4_3_norm_GAP_substract_loss: 9.8273 - fc7_GAP_substract_loss: 4.9538 - conv6_2_GAP_substract_loss: 5.9152 - conv7_2_GAP_substract_loss: 7.3666 - conv8_2_GAP_substract_loss: 18.3931 - conv9_2_GAP_substract_loss: 30.8190 - predictions_loss: 3.5286 - val_loss: 7.3415 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8582\n",
      "\n",
      "Epoch 00059: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-59_loss-5.0213_val_loss-7.3415.h5\n",
      "Epoch 60/120\n",
      "\n",
      "Epoch 00060: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 636s 636ms/step - loss: 5.0016 - pool1_GAP_substract_loss: 841.3551 - pool2_GAP_substract_loss: 3994.9510 - pool3_GAP_substract_loss: 555.5286 - conv4_3_norm_GAP_substract_loss: 9.8730 - fc7_GAP_substract_loss: 4.9535 - conv6_2_GAP_substract_loss: 5.8877 - conv7_2_GAP_substract_loss: 7.3040 - conv8_2_GAP_substract_loss: 18.1059 - conv9_2_GAP_substract_loss: 30.0905 - predictions_loss: 3.5111 - val_loss: 7.3643 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8835\n",
      "\n",
      "Epoch 00060: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-60_loss-5.0016_val_loss-7.3643.h5\n",
      "Epoch 61/120\n",
      "\n",
      "Epoch 00061: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 640s 640ms/step - loss: 5.0284 - pool1_GAP_substract_loss: 814.8102 - pool2_GAP_substract_loss: 3947.8043 - pool3_GAP_substract_loss: 553.2467 - conv4_3_norm_GAP_substract_loss: 9.6619 - fc7_GAP_substract_loss: 4.8957 - conv6_2_GAP_substract_loss: 5.8079 - conv7_2_GAP_substract_loss: 7.1666 - conv8_2_GAP_substract_loss: 17.6397 - conv9_2_GAP_substract_loss: 29.4013 - predictions_loss: 3.5407 - val_loss: 7.3264 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8481\n",
      "\n",
      "Epoch 00061: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-61_loss-5.0284_val_loss-7.3264.h5\n",
      "Epoch 62/120\n",
      "\n",
      "Epoch 00062: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 655s 655ms/step - loss: 5.0167 - pool1_GAP_substract_loss: 833.6699 - pool2_GAP_substract_loss: 4005.0643 - pool3_GAP_substract_loss: 560.7937 - conv4_3_norm_GAP_substract_loss: 9.6523 - fc7_GAP_substract_loss: 4.9982 - conv6_2_GAP_substract_loss: 5.9321 - conv7_2_GAP_substract_loss: 7.2265 - conv8_2_GAP_substract_loss: 17.3918 - conv9_2_GAP_substract_loss: 28.6043 - predictions_loss: 3.5312 - val_loss: 7.3657 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8898\n",
      "\n",
      "Epoch 00062: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-62_loss-5.0167_val_loss-7.3657.h5\n",
      "Epoch 63/120\n",
      "\n",
      "Epoch 00063: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 4.9962 - pool1_GAP_substract_loss: 827.5583 - pool2_GAP_substract_loss: 4039.9706 - pool3_GAP_substract_loss: 569.7125 - conv4_3_norm_GAP_substract_loss: 9.6899 - fc7_GAP_substract_loss: 5.0517 - conv6_2_GAP_substract_loss: 6.0204 - conv7_2_GAP_substract_loss: 7.3520 - conv8_2_GAP_substract_loss: 17.9395 - conv9_2_GAP_substract_loss: 29.5690 - predictions_loss: 3.5132 - val_loss: 7.3550 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8815\n",
      "\n",
      "Epoch 00063: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-63_loss-4.9962_val_loss-7.3550.h5\n",
      "Epoch 64/120\n",
      "\n",
      "Epoch 00064: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 629s 629ms/step - loss: 4.9914 - pool1_GAP_substract_loss: 823.0858 - pool2_GAP_substract_loss: 4015.7341 - pool3_GAP_substract_loss: 565.2323 - conv4_3_norm_GAP_substract_loss: 9.6221 - fc7_GAP_substract_loss: 4.9705 - conv6_2_GAP_substract_loss: 5.9644 - conv7_2_GAP_substract_loss: 7.3373 - conv8_2_GAP_substract_loss: 18.1212 - conv9_2_GAP_substract_loss: 29.8623 - predictions_loss: 3.5108 - val_loss: 7.3864 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9153\n",
      "\n",
      "Epoch 00064: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-64_loss-4.9914_val_loss-7.3864.h5\n",
      "Epoch 65/120\n",
      "\n",
      "Epoch 00065: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 4.9970 - pool1_GAP_substract_loss: 827.7652 - pool2_GAP_substract_loss: 4065.1418 - pool3_GAP_substract_loss: 568.9909 - conv4_3_norm_GAP_substract_loss: 9.6496 - fc7_GAP_substract_loss: 5.1082 - conv6_2_GAP_substract_loss: 6.1638 - conv7_2_GAP_substract_loss: 7.5232 - conv8_2_GAP_substract_loss: 18.1295 - conv9_2_GAP_substract_loss: 29.5990 - predictions_loss: 3.5188 - val_loss: 7.3398 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8710\n",
      "\n",
      "Epoch 00065: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-65_loss-4.9970_val_loss-7.3398.h5\n",
      "Epoch 66/120\n",
      "\n",
      "Epoch 00066: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 623s 623ms/step - loss: 4.9787 - pool1_GAP_substract_loss: 807.9829 - pool2_GAP_substract_loss: 4008.0534 - pool3_GAP_substract_loss: 564.8362 - conv4_3_norm_GAP_substract_loss: 9.5060 - fc7_GAP_substract_loss: 5.0410 - conv6_2_GAP_substract_loss: 6.0241 - conv7_2_GAP_substract_loss: 7.3836 - conv8_2_GAP_substract_loss: 18.0804 - conv9_2_GAP_substract_loss: 29.4405 - predictions_loss: 3.5031 - val_loss: 7.4186 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9522\n",
      "\n",
      "Epoch 00066: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-66_loss-4.9787_val_loss-7.4186.h5\n",
      "Epoch 67/120\n",
      "\n",
      "Epoch 00067: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 628s 628ms/step - loss: 4.9941 - pool1_GAP_substract_loss: 815.9387 - pool2_GAP_substract_loss: 4058.9603 - pool3_GAP_substract_loss: 566.2185 - conv4_3_norm_GAP_substract_loss: 9.5223 - fc7_GAP_substract_loss: 4.9634 - conv6_2_GAP_substract_loss: 5.9976 - conv7_2_GAP_substract_loss: 7.4268 - conv8_2_GAP_substract_loss: 18.3230 - conv9_2_GAP_substract_loss: 29.9363 - predictions_loss: 3.5208 - val_loss: 7.4884 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.0245\n",
      "\n",
      "Epoch 00067: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-67_loss-4.9941_val_loss-7.4884.h5\n",
      "Epoch 68/120\n",
      "\n",
      "Epoch 00068: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 631s 631ms/step - loss: 4.9690 - pool1_GAP_substract_loss: 822.2092 - pool2_GAP_substract_loss: 4137.5910 - pool3_GAP_substract_loss: 577.0761 - conv4_3_norm_GAP_substract_loss: 9.6820 - fc7_GAP_substract_loss: 5.0694 - conv6_2_GAP_substract_loss: 6.1851 - conv7_2_GAP_substract_loss: 7.6902 - conv8_2_GAP_substract_loss: 18.9518 - conv9_2_GAP_substract_loss: 30.7184 - predictions_loss: 3.4980 - val_loss: 7.3258 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8642\n",
      "\n",
      "Epoch 00068: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-68_loss-4.9690_val_loss-7.3258.h5\n",
      "Epoch 69/120\n",
      "\n",
      "Epoch 00069: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 629s 629ms/step - loss: 4.9668 - pool1_GAP_substract_loss: 816.0744 - pool2_GAP_substract_loss: 4084.6643 - pool3_GAP_substract_loss: 573.2295 - conv4_3_norm_GAP_substract_loss: 9.6569 - fc7_GAP_substract_loss: 5.1197 - conv6_2_GAP_substract_loss: 6.2900 - conv7_2_GAP_substract_loss: 7.8167 - conv8_2_GAP_substract_loss: 19.1087 - conv9_2_GAP_substract_loss: 31.0025 - predictions_loss: 3.4983 - val_loss: 7.3963 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9371\n",
      "\n",
      "Epoch 00069: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-69_loss-4.9668_val_loss-7.3963.h5\n",
      "Epoch 70/120\n",
      "\n",
      "Epoch 00070: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 633s 633ms/step - loss: 4.9468 - pool1_GAP_substract_loss: 803.8922 - pool2_GAP_substract_loss: 4156.9662 - pool3_GAP_substract_loss: 572.7507 - conv4_3_norm_GAP_substract_loss: 9.6656 - fc7_GAP_substract_loss: 5.1387 - conv6_2_GAP_substract_loss: 6.2400 - conv7_2_GAP_substract_loss: 7.6729 - conv8_2_GAP_substract_loss: 18.6108 - conv9_2_GAP_substract_loss: 30.1642 - predictions_loss: 3.4808 - val_loss: 7.3392 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8823\n",
      "\n",
      "Epoch 00070: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-70_loss-4.9468_val_loss-7.3392.h5\n",
      "Epoch 71/120\n",
      "\n",
      "Epoch 00071: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 631s 631ms/step - loss: 4.9349 - pool1_GAP_substract_loss: 816.5174 - pool2_GAP_substract_loss: 4163.0912 - pool3_GAP_substract_loss: 581.9730 - conv4_3_norm_GAP_substract_loss: 9.7965 - fc7_GAP_substract_loss: 5.0907 - conv6_2_GAP_substract_loss: 6.2414 - conv7_2_GAP_substract_loss: 7.7480 - conv8_2_GAP_substract_loss: 19.2960 - conv9_2_GAP_substract_loss: 31.6568 - predictions_loss: 3.4711 - val_loss: 7.3252 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8707\n",
      "\n",
      "Epoch 00071: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-71_loss-4.9349_val_loss-7.3252.h5\n",
      "Epoch 72/120\n",
      "\n",
      "Epoch 00072: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 629s 629ms/step - loss: 4.9401 - pool1_GAP_substract_loss: 809.1243 - pool2_GAP_substract_loss: 4145.4280 - pool3_GAP_substract_loss: 578.4496 - conv4_3_norm_GAP_substract_loss: 9.7380 - fc7_GAP_substract_loss: 5.2184 - conv6_2_GAP_substract_loss: 6.4354 - conv7_2_GAP_substract_loss: 7.9393 - conv8_2_GAP_substract_loss: 19.2935 - conv9_2_GAP_substract_loss: 31.2120 - predictions_loss: 3.4787 - val_loss: 7.4508 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9987\n",
      "\n",
      "Epoch 00072: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-72_loss-4.9401_val_loss-7.4508.h5\n",
      "Epoch 73/120\n",
      "\n",
      "Epoch 00073: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 634s 634ms/step - loss: 4.9341 - pool1_GAP_substract_loss: 822.6607 - pool2_GAP_substract_loss: 4255.7282 - pool3_GAP_substract_loss: 590.7410 - conv4_3_norm_GAP_substract_loss: 9.8633 - fc7_GAP_substract_loss: 5.2654 - conv6_2_GAP_substract_loss: 6.4553 - conv7_2_GAP_substract_loss: 7.9150 - conv8_2_GAP_substract_loss: 19.1524 - conv9_2_GAP_substract_loss: 30.8035 - predictions_loss: 3.4750 - val_loss: 7.3970 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9472\n",
      "\n",
      "Epoch 00073: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-73_loss-4.9341_val_loss-7.3970.h5\n",
      "Epoch 74/120\n",
      "\n",
      "Epoch 00074: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 626s 626ms/step - loss: 4.9138 - pool1_GAP_substract_loss: 817.6881 - pool2_GAP_substract_loss: 4259.7569 - pool3_GAP_substract_loss: 595.9863 - conv4_3_norm_GAP_substract_loss: 9.9957 - fc7_GAP_substract_loss: 5.2710 - conv6_2_GAP_substract_loss: 6.4303 - conv7_2_GAP_substract_loss: 7.8723 - conv8_2_GAP_substract_loss: 19.1132 - conv9_2_GAP_substract_loss: 30.8151 - predictions_loss: 3.4570 - val_loss: 7.4243 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9769\n",
      "\n",
      "Epoch 00074: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-74_loss-4.9138_val_loss-7.4243.h5\n",
      "Epoch 75/120\n",
      "\n",
      "Epoch 00075: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 633s 633ms/step - loss: 4.9310 - pool1_GAP_substract_loss: 815.0562 - pool2_GAP_substract_loss: 4208.7102 - pool3_GAP_substract_loss: 584.2114 - conv4_3_norm_GAP_substract_loss: 9.8570 - fc7_GAP_substract_loss: 5.1822 - conv6_2_GAP_substract_loss: 6.3357 - conv7_2_GAP_substract_loss: 7.7965 - conv8_2_GAP_substract_loss: 19.0515 - conv9_2_GAP_substract_loss: 30.6215 - predictions_loss: 3.4765 - val_loss: 7.4236 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9784\n",
      "\n",
      "Epoch 00075: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-75_loss-4.9310_val_loss-7.4236.h5\n",
      "Epoch 76/120\n",
      "\n",
      "Epoch 00076: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 626s 626ms/step - loss: 4.9191 - pool1_GAP_substract_loss: 809.7188 - pool2_GAP_substract_loss: 4213.3100 - pool3_GAP_substract_loss: 586.5046 - conv4_3_norm_GAP_substract_loss: 9.8423 - fc7_GAP_substract_loss: 5.1940 - conv6_2_GAP_substract_loss: 6.3642 - conv7_2_GAP_substract_loss: 7.8333 - conv8_2_GAP_substract_loss: 19.0212 - conv9_2_GAP_substract_loss: 30.1781 - predictions_loss: 3.4670 - val_loss: 7.4241 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9813\n",
      "\n",
      "Epoch 00076: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-76_loss-4.9191_val_loss-7.4241.h5\n",
      "Epoch 77/120\n",
      "\n",
      "Epoch 00077: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 4.9106 - pool1_GAP_substract_loss: 820.7269 - pool2_GAP_substract_loss: 4262.9711 - pool3_GAP_substract_loss: 593.6453 - conv4_3_norm_GAP_substract_loss: 9.8515 - fc7_GAP_substract_loss: 5.2555 - conv6_2_GAP_substract_loss: 6.4934 - conv7_2_GAP_substract_loss: 8.0097 - conv8_2_GAP_substract_loss: 19.4307 - conv9_2_GAP_substract_loss: 30.9498 - predictions_loss: 3.4607 - val_loss: 7.4075 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9670\n",
      "\n",
      "Epoch 00077: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-77_loss-4.9106_val_loss-7.4075.h5\n",
      "Epoch 78/120\n",
      "\n",
      "Epoch 00078: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 630s 630ms/step - loss: 4.9085 - pool1_GAP_substract_loss: 820.4071 - pool2_GAP_substract_loss: 4261.6646 - pool3_GAP_substract_loss: 592.2129 - conv4_3_norm_GAP_substract_loss: 9.7331 - fc7_GAP_substract_loss: 5.2690 - conv6_2_GAP_substract_loss: 6.4570 - conv7_2_GAP_substract_loss: 7.9088 - conv8_2_GAP_substract_loss: 19.1993 - conv9_2_GAP_substract_loss: 30.5951 - predictions_loss: 3.4609 - val_loss: 7.3672 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9290\n",
      "\n",
      "Epoch 00078: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool1_global_pool_SGD/epoch-78_loss-4.9085_val_loss-7.3672.h5\n",
      "Epoch 79/120\n",
      "\n",
      "Epoch 00079: LearningRateScheduler setting learning rate to 0.0001.\n",
      " 140/1000 [===>..........................] - ETA: 9:01 - loss: 4.8893 - pool1_GAP_substract_loss: 825.4182 - pool2_GAP_substract_loss: 4219.5848 - pool3_GAP_substract_loss: 589.1644 - conv4_3_norm_GAP_substract_loss: 10.2282 - fc7_GAP_substract_loss: 5.5668 - conv6_2_GAP_substract_loss: 7.0946 - conv7_2_GAP_substract_loss: 8.9258 - conv8_2_GAP_substract_loss: 22.0959 - conv9_2_GAP_substract_loss: 35.0594 - predictions_loss: 3.4430"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ea67b764e796>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                               \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset_size\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2719\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2720\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2721\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2693\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2694\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1095\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initial_epoch = 0\n",
    "final_epoch = 120\n",
    "steps_per_epoch = 1000\n",
    "\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              epochs=final_epoch,\n",
    "                              callbacks=callbacks,\n",
    "                              validation_data=val_generator,\n",
    "                              validation_steps=ceil(val_dataset_size/batch_size),\n",
    "                              initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Set the generator for the val_dataset or train_dataset predictions.\n",
    "\n",
    "predict_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[ssd_data_augmentation],\n",
    "                                         label_encoder=None,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'filenames',\n",
    "                                                  'inverse_transform',\n",
    "                                                  'original_images',\n",
    "                                                  'original_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n",
    "# 2: Generate samples.\n",
    "\n",
    "batch_images, batch_filenames, batch_inverse_transforms, batch_original_images, batch_original_labels = next(predict_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_images, batch_filenames, batch_inverse_transforms, batch_original_images, batch_original_labels = next(predict_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "print(\"Image:\", batch_filenames[i])\n",
    "colors = plt.cm.hsv(np.linspace(0, 1, n_classes+1)).tolist()\n",
    "classes = ['background',\n",
    "           'person', 'rider', 'car', 'truck',\n",
    "           'bus', 'train', 'motorcycle', 'bicycle']\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.imshow(batch_images[0][i])\n",
    "plt.show()\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.imshow(batch_images[1][i])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3  # Which batch item to look at\n",
    "\n",
    "print(\"Image:\", batch_filenames[i])\n",
    "print()\n",
    "print(\"Ground truth boxes:\\n\")\n",
    "print(np.array(batch_original_labels[i]))\n",
    "\n",
    "# 3: Make predictions.\n",
    "\n",
    "y_pred = model.predict(batch_images)[-1]\n",
    "\n",
    "# Now let's decode the raw predictions in `y_pred`.\n",
    "\n",
    "# Had we created the model in 'inference' or 'inference_fast' mode,\n",
    "# then the model's final layer would be a `DecodeDetections` layer and\n",
    "# `y_pred` would already contain the decoded predictions,\n",
    "# but since we created the model in 'training' mode,\n",
    "# the model outputs raw predictions that still need to be decoded and filtered.\n",
    "# This is what the `decode_detections()` function is for.\n",
    "# It does exactly what the `DecodeDetections` layer would do,\n",
    "# but using Numpy instead of TensorFlow (i.e. on the CPU instead of the GPU).\n",
    "\n",
    "# `decode_detections()` with default argument values follows the procedure of the original SSD implementation:\n",
    "# First, a very low confidence threshold of 0.01 is applied to filter out the majority of the predicted boxes,\n",
    "# then greedy non-maximum suppression is performed per class with an intersection-over-union threshold of 0.45,\n",
    "# and out of what is left after that, the top 200 highest confidence boxes are returned.\n",
    "# Those settings are for precision-recall scoring purposes though.\n",
    "# In order to get some usable final predictions, we'll set the confidence threshold much higher, e.g. to 0.5,\n",
    "# since we're only interested in the very confident predictions.\n",
    "\n",
    "# 4: Decode the raw predictions in `y_pred`.\n",
    "\n",
    "y_pred_decoded = decode_detections(y_pred,\n",
    "                                   confidence_thresh=0.35,\n",
    "                                   iou_threshold=0.4,\n",
    "                                   top_k=200,\n",
    "                                   normalize_coords=normalize_coords,\n",
    "                                   img_height=img_height,\n",
    "                                   img_width=img_width)\n",
    "\n",
    "# We made the predictions on the resized images,\n",
    "# but we'd like to visualize the outcome on the original input images,\n",
    "# so we'll convert the coordinates accordingly.\n",
    "# Don't worry about that opaque `apply_inverse_transforms()` function below,\n",
    "# in this simple case it just applies `(* original_image_size / resized_image_size)` to the box coordinates.\n",
    "\n",
    "# 5: Convert the predictions for the original image.\n",
    "\n",
    "y_pred_decoded_inv = apply_inverse_transforms(y_pred_decoded, batch_inverse_transforms)\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=90)\n",
    "print(\"Predicted boxes:\\n\")\n",
    "print('   class   conf xmin   ymin   xmax   ymax')\n",
    "print(y_pred_decoded_inv[i])\n",
    "\n",
    "# Finally, let's draw the predicted boxes onto the image.\n",
    "# Each predicted box says its confidence next to the category name.\n",
    "# The ground truth boxes are also drawn onto the image in green for comparison.\n",
    "\n",
    "# 5: Draw the predicted boxes onto the image\n",
    "\n",
    "# Set the colors for the bounding boxes\n",
    "colors = plt.cm.hsv(np.linspace(0, 1, n_classes+1)).tolist()\n",
    "classes = ['background',\n",
    "           'person', 'rider', 'car', 'truck',\n",
    "           'bus', 'train', 'motorcycle', 'bicycle']\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.imshow(batch_original_images[i])\n",
    "\n",
    "current_axis = plt.gca()\n",
    "\n",
    "for box in batch_original_labels[i]:\n",
    "    xmin = box[1]\n",
    "    ymin = box[2]\n",
    "    xmax = box[3]\n",
    "    ymax = box[4]\n",
    "    label = '{}'.format(classes[int(box[0])])\n",
    "    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color='green', fill=False, linewidth=2))\n",
    "    current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor': 'green', 'alpha': 1.0})\n",
    "\n",
    "for box in y_pred_decoded_inv[i]:\n",
    "    xmin = box[2]\n",
    "    ymin = box[3]\n",
    "    xmax = box[4]\n",
    "    ymax = box[5]\n",
    "    color = colors[int(box[0])]\n",
    "    label = '{}: {:.2f}'.format(classes[int(box[0])], box[1])\n",
    "    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color=color, fill=False, linewidth=2))\n",
    "    current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor': color, 'alpha': 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
