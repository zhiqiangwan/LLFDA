{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import glob\n",
    "\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TerminateOnNaN, CSVLogger, TensorBoard\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from models.keras_ssd300_Siamese import ssd_300\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
    "from keras_layers.keras_layer_L2Normalization import L2Normalization\n",
    "\n",
    "from ssd_encoder_decoder.ssd_input_encoder import SSDInputEncoder\n",
    "from ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n",
    "\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from data_generator.object_detection_2d_geometric_ops import Resize\n",
    "from data_generator.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
    "from data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation, SSDDataAugmentation_Siamese\n",
    "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
    "\n",
    "\n",
    "img_height = 300 # Height of the model input images\n",
    "img_width = 300 # Width of the model input images\n",
    "img_channels = 3 # Number of color channels of the model input images\n",
    "mean_color = [123, 117, 104] # Per-channel mean of images. Do not change if use any of the pre-trained weights.\n",
    "# The color channel order in the original SSD is BGR,\n",
    "# so we'll have the model reverse the color channel order of the input images.\n",
    "swap_channels = [2, 1, 0]\n",
    "classes = ['background',\n",
    "           'person', 'rider', 'car', 'truck',\n",
    "           'bus', 'train', 'motorcycle', 'bicycle']\n",
    "n_classes = len(classes) - 1  # Number of positive classes, 8 for domain Cityscapes, 20 for Pascal VOC, 80 for MS COCO\n",
    "# The anchor box scaling factors used in the original SSD300 for the Pascal VOC datasets\n",
    "# scales_pascal = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05]\n",
    "# The anchor box scaling factors used in the original SSD300 for the MS COCO datasets\n",
    "scales_coco = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05]\n",
    "scales = scales_coco\n",
    "aspect_ratios = [[1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5]] # The anchor box aspect ratios used in the original SSD300; the order matters\n",
    "two_boxes_for_ar1 = True\n",
    "steps = [8, 16, 32, 64, 100, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
    "# The offsets of the first anchor box center points from the top and left borders of the image\n",
    "# as a fraction of the step size for each predictor layer.\n",
    "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
    "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
    "# The variances by which the encoded target coordinates are divided as in the original implementation\n",
    "variances = [0.1, 0.1, 0.2, 0.2]\n",
    "normalize_coords = True\n",
    "Model_Build = 'New_Model'  # 'Load_Model'\n",
    "Optimizer_Type = 'SGD'  # 'Adam' #  \n",
    "batch_size = 16  # Change the batch size if you like, or if you run into GPU memory issues.\n",
    "# alpha_distance =  0.001  # Coefficient for the distance between the source and target feature maps.\n",
    "loss_weights = [0.0000001, 0.0000001, 0.0000001] + [0.0, 0.0, 0.0, 0.0, 0.0, 0.0] + [1.0]\n",
    "Source_Only = False\n",
    "\n",
    "if len(glob.glob('*.h5')):\n",
    "    Dataset_Build = 'Load_Dataset'\n",
    "else:\n",
    "    Dataset_Build = 'New_Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Model_Build == 'New_Model':\n",
    "    # 1: Build the Keras model.\n",
    "\n",
    "    K.clear_session()  # Clear previous models from memory.\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from keras.backend.tensorflow_backend import set_session\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "    config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "    # (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "    sess = tf.Session(config=config)\n",
    "    set_session(sess)  # set this TensorFlow session as the default session for Keras\n",
    "\n",
    "    model = ssd_300(image_size=(img_height, img_width, img_channels),\n",
    "                    n_classes=n_classes,\n",
    "                    mode='training',\n",
    "                    l2_regularization=0.0005,\n",
    "                    scales=scales,\n",
    "                    aspect_ratios_per_layer=aspect_ratios,\n",
    "                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                    steps=steps,\n",
    "                    offsets=offsets,\n",
    "                    clip_boxes=clip_boxes,\n",
    "                    variances=variances,\n",
    "                    normalize_coords=normalize_coords,\n",
    "                    subtract_mean=mean_color,\n",
    "                    swap_channels=swap_channels)\n",
    "\n",
    "    # 2: Load some weights into the model.\n",
    "\n",
    "    # TODO: Set the path to the weights you want to load.\n",
    "    weights_path = '../trained_weights/VGG_ILSVRC_16_layers_fc_reduced.h5'\n",
    "\n",
    "    model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "    # 3: Instantiate an optimizer and the SSD loss function and compile the model.\n",
    "    #    If you want to follow the original Caffe implementation, use the preset SGD\n",
    "    #    optimizer, otherwise I'd recommend the commented-out Adam optimizer.\n",
    "\n",
    "    if Optimizer_Type == 'SGD':\n",
    "        Optimizer = SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=False)\n",
    "    elif Optimizer_Type == 'Adam':\n",
    "        Optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    else:\n",
    "        raise ValueError('Undefined Optimizer_Type.')\n",
    "\n",
    "    ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "    if Source_Only:\n",
    "        model.compile(optimizer=Optimizer, loss={'pool1_GAP_substract': ssd_loss.compute_distance_loss_source_only,\n",
    "                                                 'pool2_GAP_substract': ssd_loss.compute_distance_loss_source_only,\n",
    "                                                 'pool3_GAP_substract': ssd_loss.compute_distance_loss_source_only,\n",
    "                                                 'conv4_3_norm_GAP_substract': ssd_loss.compute_distance_loss_source_only,\n",
    "                                                 'fc7_GAP_substract': ssd_loss.compute_distance_loss_source_only,\n",
    "                                                 'conv6_2_GAP_substract': ssd_loss.compute_distance_loss_source_only,\n",
    "                                                 'conv7_2_GAP_substract': ssd_loss.compute_distance_loss_source_only,\n",
    "                                                 'conv8_2_GAP_substract': ssd_loss.compute_distance_loss_source_only,\n",
    "                                                 'conv9_2_GAP_substract': ssd_loss.compute_distance_loss_source_only,\n",
    "                                                 'predictions': ssd_loss.compute_loss},\n",
    "                      loss_weights={'pool1_GAP_substract': loss_weights[0],\n",
    "                                    'pool2_GAP_substract': loss_weights[1],\n",
    "                                    'pool3_GAP_substract': loss_weights[2],\n",
    "                                    'conv4_3_norm_GAP_substract': loss_weights[3],\n",
    "                                    'fc7_GAP_substract': loss_weights[4],\n",
    "                                    'conv6_2_GAP_substract': loss_weights[5],\n",
    "                                    'conv7_2_GAP_substract': loss_weights[6],\n",
    "                                    'conv8_2_GAP_substract': loss_weights[7],\n",
    "                                    'conv9_2_GAP_substract': loss_weights[8],\n",
    "                                    'predictions': loss_weights[9]})\n",
    "    else:\n",
    "        model.compile(optimizer=Optimizer, loss={'pool1_GAP_substract': ssd_loss.compute_distance_loss,\n",
    "                                                 'pool2_GAP_substract': ssd_loss.compute_distance_loss,\n",
    "                                                 'pool3_GAP_substract': ssd_loss.compute_distance_loss,\n",
    "                                                 'conv4_3_norm_GAP_substract': ssd_loss.compute_distance_loss,\n",
    "                                                 'fc7_GAP_substract': ssd_loss.compute_distance_loss,\n",
    "                                                 'conv6_2_GAP_substract': ssd_loss.compute_distance_loss,\n",
    "                                                 'conv7_2_GAP_substract': ssd_loss.compute_distance_loss,\n",
    "                                                 'conv8_2_GAP_substract': ssd_loss.compute_distance_loss,\n",
    "                                                 'conv9_2_GAP_substract': ssd_loss.compute_distance_loss,\n",
    "                                                 'predictions': ssd_loss.compute_loss},\n",
    "                      loss_weights={'pool1_GAP_substract': loss_weights[0],\n",
    "                                    'pool2_GAP_substract': loss_weights[1],\n",
    "                                    'pool3_GAP_substract': loss_weights[2],\n",
    "                                    'conv4_3_norm_GAP_substract': loss_weights[3],\n",
    "                                    'fc7_GAP_substract': loss_weights[4],\n",
    "                                    'conv6_2_GAP_substract': loss_weights[5],\n",
    "                                    'conv7_2_GAP_substract': loss_weights[6],\n",
    "                                    'conv8_2_GAP_substract': loss_weights[7],\n",
    "                                    'conv9_2_GAP_substract': loss_weights[8],\n",
    "                                    'predictions': loss_weights[9]})\n",
    "        \n",
    "        \n",
    "elif Model_Build == 'Load_Model':\n",
    "    # TODO: Set the path to the `.h5` file of the model to be loaded.\n",
    "    model_path = '../trained_weights/VGG_ssd300_Siamese_Cityscapes/epoch-23_loss-5.2110_val_loss-6.7452.h5'\n",
    "\n",
    "    # We need to create an SSDLoss object in order to pass that to the model loader.\n",
    "    ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "    K.clear_session()  # Clear previous models from memory.\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from keras.backend.tensorflow_backend import set_session\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "    config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "    # (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "    sess = tf.Session(config=config)\n",
    "    set_session(sess)  # set this TensorFlow session as the default session for Keras\n",
    "\n",
    "    model = load_model(model_path, custom_objects={'AnchorBoxes': AnchorBoxes,\n",
    "                                                   'L2Normalization': L2Normalization,\n",
    "                                                   'compute_loss': ssd_loss.compute_loss,\n",
    "                                                   'compute_distance_loss': ssd_loss.compute_distance_loss})\n",
    "else:\n",
    "    raise ValueError('Undefined Model_Build. Model_Build should be New_Model  or Load_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labels: 100%|██████████| 2966/2966 [00:00<00:00, 3203.24it/s]\n",
      "Loading source image IDs: 100%|██████████| 2966/2966 [00:00<00:00, 11064.72it/s]\n",
      "Loading target image IDs: 100%|██████████| 2966/2966 [00:00<00:00, 11352.07it/s]\n",
      "Loading evaluation-neutrality annotations: 100%|██████████| 2966/2966 [00:00<00:00, 8094.67it/s]\n",
      "Loading labels: 100%|██████████| 493/493 [00:00<00:00, 3184.80it/s]\n",
      "Loading source image IDs: 100%|██████████| 493/493 [00:00<00:00, 11547.97it/s]\n",
      "Loading target image IDs: 0it [00:00, ?it/s]\n",
      "Loading evaluation-neutrality annotations: 100%|██████████| 493/493 [00:00<00:00, 7885.35it/s]\n"
     ]
    }
   ],
   "source": [
    "if Dataset_Build == 'New_Dataset':\n",
    "    # 1: Instantiate two `DataGenerator` objects: One for training, one for validation.\n",
    "\n",
    "    # Optional: If you have enough memory, consider loading the images into memory for the reasons explained above.\n",
    "\n",
    "    train_dataset = DataGenerator(dataset='train', load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "    val_dataset = DataGenerator(dataset='val', load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "\n",
    "    # 2: Parse the image and label lists for the training and validation datasets. This can take a while.\n",
    "\n",
    "    # TODO: Set the paths to the datasets here.\n",
    "\n",
    "    # Introduction of PascalVOC: https://arleyzhang.github.io/articles/1dc20586/\n",
    "    # The directories that contain the images.\n",
    "    Cityscapes_images_dir = '../../datasets/Cityscapes/JPEGImages'\n",
    "    Cityscapes_target_images_dir = '../../datasets/CITYSCAPES_beta_0_01/JPEGImages'\n",
    "\n",
    "    # The directories that contain the annotations.\n",
    "    Cityscapes_annotation_dir = '../../datasets/Cityscapes/Annotations'\n",
    "\n",
    "    # The paths to the image sets.\n",
    "    Cityscapes_train_source_image_set_filename = '../../datasets/Cityscapes/ImageSets/Main/train_source.txt'\n",
    "    Cityscapes_train_target_image_set_filename = '../../datasets/Cityscapes/ImageSets/Main/train_target.txt'\n",
    "    Cityscapes_test_target_image_set_filename = '../../datasets/Cityscapes/ImageSets/Main/test.txt'\n",
    "\n",
    "    # images_dirs, image_set_filenames, and annotations_dirs should have the same length\n",
    "    train_dataset.parse_xml(images_dirs=[Cityscapes_images_dir],\n",
    "                            target_images_dirs=[Cityscapes_target_images_dir],\n",
    "                            image_set_filenames=[Cityscapes_train_source_image_set_filename],\n",
    "                            target_image_set_filenames=[Cityscapes_train_target_image_set_filename],\n",
    "                            annotations_dirs=[Cityscapes_annotation_dir],\n",
    "                            classes=classes,\n",
    "                            include_classes='all',\n",
    "                            exclude_truncated=False,\n",
    "                            exclude_difficult=False,\n",
    "                            ret=False)\n",
    "\n",
    "    val_dataset.parse_xml(images_dirs=[Cityscapes_target_images_dir],\n",
    "                          image_set_filenames=[Cityscapes_test_target_image_set_filename],\n",
    "                          annotations_dirs=[Cityscapes_annotation_dir],\n",
    "                          classes=classes,\n",
    "                          include_classes='all',\n",
    "                          exclude_truncated=False,\n",
    "                          exclude_difficult=True,\n",
    "                          ret=False)\n",
    "\n",
    "    # Optional: Convert the dataset into an HDF5 dataset. This will require more disk space, but will\n",
    "    # speed up the training. Doing this is not relevant in case you activated the `load_images_into_memory`\n",
    "    # option in the constructor, because in that cas the images are in memory already anyway. If you don't\n",
    "    # want to create HDF5 datasets, comment out the subsequent two function calls.\n",
    "\n",
    "    # After create these h5 files, if you have resized the input image, you need to reload these files. Otherwise,\n",
    "    # the images and the labels will not change.\n",
    "\n",
    "    resize_image_to = (300, 600)\n",
    "    train_dataset.create_hdf5_dataset(file_path='dataset_cityscapes_train.h5',\n",
    "                                      resize=resize_image_to,\n",
    "                                      variable_image_size=True,\n",
    "                                      verbose=True)\n",
    "\n",
    "    val_dataset.create_hdf5_dataset(file_path='dataset_cityscapes_test.h5',\n",
    "                                    resize=resize_image_to,\n",
    "                                    variable_image_size=True,\n",
    "                                    verbose=True)\n",
    "\n",
    "    train_dataset = DataGenerator(dataset='train',\n",
    "                                  load_images_into_memory=False,\n",
    "                                  hdf5_dataset_path='dataset_cityscapes_train.h5',\n",
    "                                  filenames=Cityscapes_train_source_image_set_filename,\n",
    "                                  target_filenames=Cityscapes_train_target_image_set_filename,\n",
    "                                  filenames_type='text',\n",
    "                                  images_dir=Cityscapes_images_dir,\n",
    "                                  target_images_dir=Cityscapes_target_images_dir)\n",
    "\n",
    "    val_dataset = DataGenerator(dataset='val',\n",
    "                                load_images_into_memory=False,\n",
    "                                hdf5_dataset_path='dataset_cityscapes_test.h5',\n",
    "                                filenames=Cityscapes_test_target_image_set_filename,\n",
    "                                filenames_type='text',\n",
    "                                images_dir=Cityscapes_target_images_dir)\n",
    "\n",
    "elif Dataset_Build == 'Load_Dataset':\n",
    "    # 1: Instantiate two `DataGenerator` objects: One for training, one for validation.\n",
    "\n",
    "    # Load dataset from the created h5 file.\n",
    "\n",
    "    # The directories that contain the images.\n",
    "    Cityscapes_images_dir = '../../datasets/Cityscapes/JPEGImages'\n",
    "    Cityscapes_target_images_dir = '../../datasets/CITYSCAPES_beta_0_01/JPEGImages'\n",
    "\n",
    "    # The paths to the image sets.\n",
    "    Cityscapes_train_source_image_set_filename = '../../datasets/Cityscapes/ImageSets/Main/train_source.txt'\n",
    "    Cityscapes_train_target_image_set_filename = '../../datasets/Cityscapes/ImageSets/Main/train_target.txt'\n",
    "    Cityscapes_test_target_image_set_filename = '../../datasets/Cityscapes/ImageSets/Main/test.txt'\n",
    "\n",
    "    train_dataset = DataGenerator(dataset='train',\n",
    "                                  load_images_into_memory=False,\n",
    "                                  hdf5_dataset_path='dataset_cityscapes_train.h5',\n",
    "                                  filenames=Cityscapes_train_source_image_set_filename,\n",
    "                                  target_filenames=Cityscapes_train_target_image_set_filename,\n",
    "                                  filenames_type='text',\n",
    "                                  images_dir=Cityscapes_images_dir,\n",
    "                                  target_images_dir=Cityscapes_target_images_dir)\n",
    "\n",
    "    val_dataset = DataGenerator(dataset='val',\n",
    "                                load_images_into_memory=False,\n",
    "                                hdf5_dataset_path='dataset_cityscapes_test.h5',\n",
    "                                filenames=Cityscapes_test_target_image_set_filename,\n",
    "                                filenames_type='text',\n",
    "                                images_dir=Cityscapes_target_images_dir)\n",
    "\n",
    "else:\n",
    "    raise ValueError('Undefined Dataset_Build. Dataset_Build should be New_Dataset or Load_Dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the training dataset:\t  2966\n",
      "Number of images in the validation dataset:\t   493\n"
     ]
    }
   ],
   "source": [
    "# 4: Set the image transformations for pre-processing and data augmentation options.\n",
    "\n",
    "# For the training generator:\n",
    "ssd_data_augmentation = SSDDataAugmentation_Siamese(img_height=img_height,\n",
    "                                                    img_width=img_width)\n",
    "\n",
    "# For the validation generator:\n",
    "convert_to_3_channels = ConvertTo3Channels()\n",
    "resize = Resize(height=img_height, width=img_width)\n",
    "\n",
    "# 5: Instantiate an encoder that can encode ground truth labels into the format needed by the SSD loss function.\n",
    "\n",
    "# The encoder constructor needs the spatial dimensions of the model's predictor layers to create the anchor boxes.\n",
    "predictor_sizes = [model.get_layer('conv4_3_norm_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('fc7_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv6_2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv7_2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv8_2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv9_2_mbox_conf').output_shape[1:3]]\n",
    "\n",
    "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
    "                                    img_width=img_width,\n",
    "                                    n_classes=n_classes,\n",
    "                                    predictor_sizes=predictor_sizes,\n",
    "                                    scales=scales,\n",
    "                                    aspect_ratios_per_layer=aspect_ratios,\n",
    "                                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                                    steps=steps,\n",
    "                                    offsets=offsets,\n",
    "                                    clip_boxes=clip_boxes,\n",
    "                                    variances=variances,\n",
    "                                    matching_type='multi',\n",
    "                                    pos_iou_threshold=0.5,\n",
    "                                    neg_iou_limit=0.5,\n",
    "                                    normalize_coords=normalize_coords)\n",
    "\n",
    "# 6: Create the generator handles that will be passed to Keras' `fit_generator()` function.\n",
    "# The input image and label are first processed by transformations. Then, the label will be further encoded by\n",
    "# ssd_input_encoder. The encoded labels are classId and offset to each anchor box.\n",
    "train_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[ssd_data_augmentation],\n",
    "                                         label_encoder=ssd_input_encoder,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'encoded_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n",
    "val_generator = val_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[convert_to_3_channels,\n",
    "                                                      resize],\n",
    "                                     label_encoder=ssd_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'encoded_labels'},\n",
    "                                     keep_images_without_gt=False)\n",
    "\n",
    "# Get the number of samples in the training and validations datasets.\n",
    "train_dataset_size = train_dataset.get_dataset_size()\n",
    "val_dataset_size = val_dataset.get_dataset_size()\n",
    "\n",
    "print(\"Number of images in the training dataset:\\t{:>6}\".format(train_dataset_size))\n",
    "print(\"Number of images in the validation dataset:\\t{:>6}\".format(val_dataset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    if epoch < 2:\n",
    "        return 0.0005\n",
    "    elif epoch < 55:\n",
    "        return 0.001\n",
    "    elif epoch < 100:\n",
    "        return 0.0001\n",
    "    else:\n",
    "        return 0.00001\n",
    "\n",
    "# def lr_schedule(epoch):\n",
    "#     if epoch < 30:\n",
    "#         return 0.001\n",
    "#     elif epoch < 60:\n",
    "#         return 0.0001\n",
    "#     else:\n",
    "#         return 0.00001\n",
    "\n",
    "# Define model callbacks.\n",
    "checkpoint_path = '../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001'\n",
    "# checkpoint_path = '../trained_weights/current/ssd_augm_beta_0_01_source_only'\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "\n",
    "# TODO: Set the filepath under which you want to save the model.\n",
    "model_checkpoint = ModelCheckpoint(filepath=os.path.join(checkpoint_path, 'epoch-{epoch:02d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.h5'),\n",
    "                                   monitor='val_loss',\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=False,\n",
    "                                   save_weights_only=True,\n",
    "                                   mode='auto',\n",
    "                                   period=1)\n",
    "\n",
    "# model_checkpoint.best to the best validation loss from the previous training\n",
    "# model_checkpoint.best = 4.83704\n",
    "\n",
    "csv_logger = CSVLogger(filename=os.path.join(checkpoint_path, 'ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001_log.csv'),\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "\n",
    "learning_rate_scheduler = LearningRateScheduler(schedule=lr_schedule,\n",
    "                                                verbose=1)\n",
    "\n",
    "terminate_on_nan = TerminateOnNaN()\n",
    "\n",
    "TensorBoard_monitor = TensorBoard(log_dir=checkpoint_path)\n",
    "\n",
    "callbacks = [model_checkpoint,\n",
    "             csv_logger,\n",
    "             learning_rate_scheduler,\n",
    "             terminate_on_nan,\n",
    "             TensorBoard_monitor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.0005.\n",
      "1000/1000 [==============================] - 635s 635ms/step - loss: 10.1050 - pool1_GAP_substract_loss: 87499.3490 - pool2_GAP_substract_loss: 251484.0748 - pool3_GAP_substract_loss: 229036.1866 - conv4_3_norm_GAP_substract_loss: 5.1788 - fc7_GAP_substract_loss: 6.7922 - conv6_2_GAP_substract_loss: 3.5118 - conv7_2_GAP_substract_loss: 2.0017 - conv8_2_GAP_substract_loss: 4.3204 - conv9_2_GAP_substract_loss: 12.8588 - predictions_loss: 6.9728 - val_loss: 9.8827 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.8219\n",
      "\n",
      "Epoch 00001: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-01_loss-10.1050_val_loss-9.8827.h5\n",
      "Epoch 2/120\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.0005.\n",
      "1000/1000 [==============================] - 628s 628ms/step - loss: 8.8850 - pool1_GAP_substract_loss: 69930.8199 - pool2_GAP_substract_loss: 115006.5876 - pool3_GAP_substract_loss: 78609.0918 - conv4_3_norm_GAP_substract_loss: 5.2473 - fc7_GAP_substract_loss: 6.7720 - conv6_2_GAP_substract_loss: 4.2315 - conv7_2_GAP_substract_loss: 2.1869 - conv8_2_GAP_substract_loss: 4.3503 - conv9_2_GAP_substract_loss: 15.6338 - predictions_loss: 5.8121 - val_loss: 9.4428 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.4105\n",
      "\n",
      "Epoch 00002: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-02_loss-8.8850_val_loss-9.4428.h5\n",
      "Epoch 3/120\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 629s 629ms/step - loss: 8.6424 - pool1_GAP_substract_loss: 57725.0424 - pool2_GAP_substract_loss: 78697.7173 - pool3_GAP_substract_loss: 46663.4642 - conv4_3_norm_GAP_substract_loss: 5.7021 - fc7_GAP_substract_loss: 6.3565 - conv6_2_GAP_substract_loss: 3.9474 - conv7_2_GAP_substract_loss: 2.3745 - conv8_2_GAP_substract_loss: 4.7297 - conv9_2_GAP_substract_loss: 17.4971 - predictions_loss: 5.6188 - val_loss: 9.0663 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.0883\n",
      "\n",
      "Epoch 00003: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-03_loss-8.6424_val_loss-9.0663.h5\n",
      "Epoch 4/120\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 628s 628ms/step - loss: 8.2756 - pool1_GAP_substract_loss: 49296.9114 - pool2_GAP_substract_loss: 59477.1794 - pool3_GAP_substract_loss: 30095.4634 - conv4_3_norm_GAP_substract_loss: 6.2387 - fc7_GAP_substract_loss: 6.2748 - conv6_2_GAP_substract_loss: 4.0950 - conv7_2_GAP_substract_loss: 2.7949 - conv8_2_GAP_substract_loss: 5.9044 - conv9_2_GAP_substract_loss: 19.9939 - predictions_loss: 5.3104 - val_loss: 9.1203 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.1956\n",
      "\n",
      "Epoch 00004: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-04_loss-8.2756_val_loss-9.1203.h5\n",
      "Epoch 5/120\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 629s 629ms/step - loss: 7.9976 - pool1_GAP_substract_loss: 43956.5791 - pool2_GAP_substract_loss: 50301.8911 - pool3_GAP_substract_loss: 23565.3883 - conv4_3_norm_GAP_substract_loss: 6.6840 - fc7_GAP_substract_loss: 6.3712 - conv6_2_GAP_substract_loss: 4.2029 - conv7_2_GAP_substract_loss: 3.0903 - conv8_2_GAP_substract_loss: 6.8337 - conv9_2_GAP_substract_loss: 22.4501 - predictions_loss: 5.0873 - val_loss: 9.0168 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.1444\n",
      "\n",
      "Epoch 00005: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-05_loss-7.9976_val_loss-9.0168.h5\n",
      "Epoch 6/120\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 627s 627ms/step - loss: 7.8734 - pool1_GAP_substract_loss: 40076.0889 - pool2_GAP_substract_loss: 42219.6130 - pool3_GAP_substract_loss: 18675.6523 - conv4_3_norm_GAP_substract_loss: 6.9328 - fc7_GAP_substract_loss: 6.4549 - conv6_2_GAP_substract_loss: 4.1986 - conv7_2_GAP_substract_loss: 3.2248 - conv8_2_GAP_substract_loss: 7.1381 - conv9_2_GAP_substract_loss: 22.9646 - predictions_loss: 5.0166 - val_loss: 8.8791 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.0578\n",
      "\n",
      "Epoch 00006: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-06_loss-7.8734_val_loss-8.8791.h5\n",
      "Epoch 7/120\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 628s 628ms/step - loss: 7.6465 - pool1_GAP_substract_loss: 37721.1580 - pool2_GAP_substract_loss: 38536.8366 - pool3_GAP_substract_loss: 15463.2610 - conv4_3_norm_GAP_substract_loss: 7.0183 - fc7_GAP_substract_loss: 6.4689 - conv6_2_GAP_substract_loss: 4.3093 - conv7_2_GAP_substract_loss: 3.3586 - conv8_2_GAP_substract_loss: 7.3126 - conv9_2_GAP_substract_loss: 22.6970 - predictions_loss: 4.8411 - val_loss: 8.8331 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.0617\n",
      "\n",
      "Epoch 00007: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-07_loss-7.6465_val_loss-8.8331.h5\n",
      "Epoch 8/120\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 626s 626ms/step - loss: 7.5607 - pool1_GAP_substract_loss: 34692.2649 - pool2_GAP_substract_loss: 35094.1510 - pool3_GAP_substract_loss: 13269.1645 - conv4_3_norm_GAP_substract_loss: 7.3170 - fc7_GAP_substract_loss: 6.4276 - conv6_2_GAP_substract_loss: 4.4470 - conv7_2_GAP_substract_loss: 3.7516 - conv8_2_GAP_substract_loss: 8.3537 - conv9_2_GAP_substract_loss: 25.0893 - predictions_loss: 4.8056 - val_loss: 8.4169 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6944\n",
      "\n",
      "Epoch 00008: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-08_loss-7.5607_val_loss-8.4169.h5\n",
      "Epoch 9/120\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 7.4165 - pool1_GAP_substract_loss: 33600.0842 - pool2_GAP_substract_loss: 31958.1632 - pool3_GAP_substract_loss: 11376.8868 - conv4_3_norm_GAP_substract_loss: 7.4387 - fc7_GAP_substract_loss: 6.4288 - conv6_2_GAP_substract_loss: 4.3860 - conv7_2_GAP_substract_loss: 3.7657 - conv8_2_GAP_substract_loss: 8.6691 - conv9_2_GAP_substract_loss: 26.4533 - predictions_loss: 4.7102 - val_loss: 8.3887 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7138\n",
      "\n",
      "Epoch 00009: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-09_loss-7.4165_val_loss-8.3887.h5\n",
      "Epoch 10/120\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 626s 626ms/step - loss: 7.2882 - pool1_GAP_substract_loss: 31859.6866 - pool2_GAP_substract_loss: 29197.6767 - pool3_GAP_substract_loss: 9833.4457 - conv4_3_norm_GAP_substract_loss: 7.4820 - fc7_GAP_substract_loss: 6.3844 - conv6_2_GAP_substract_loss: 4.2726 - conv7_2_GAP_substract_loss: 3.8044 - conv8_2_GAP_substract_loss: 8.7682 - conv9_2_GAP_substract_loss: 26.0633 - predictions_loss: 4.6296 - val_loss: 8.6887 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.0604\n",
      "\n",
      "Epoch 00010: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-10_loss-7.2882_val_loss-8.6887.h5\n",
      "Epoch 11/120\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 7.2164 - pool1_GAP_substract_loss: 29754.3903 - pool2_GAP_substract_loss: 26053.5457 - pool3_GAP_substract_loss: 8464.1287 - conv4_3_norm_GAP_substract_loss: 7.6052 - fc7_GAP_substract_loss: 6.1183 - conv6_2_GAP_substract_loss: 4.2872 - conv7_2_GAP_substract_loss: 3.8400 - conv8_2_GAP_substract_loss: 8.9723 - conv9_2_GAP_substract_loss: 26.0688 - predictions_loss: 4.6045 - val_loss: 8.4146 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8318\n",
      "\n",
      "Epoch 00011: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-11_loss-7.2164_val_loss-8.4146.h5\n",
      "Epoch 12/120\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 626s 626ms/step - loss: 7.1325 - pool1_GAP_substract_loss: 29106.8469 - pool2_GAP_substract_loss: 25440.4313 - pool3_GAP_substract_loss: 7838.5172 - conv4_3_norm_GAP_substract_loss: 7.5542 - fc7_GAP_substract_loss: 6.1244 - conv6_2_GAP_substract_loss: 4.1683 - conv7_2_GAP_substract_loss: 3.7098 - conv8_2_GAP_substract_loss: 8.6995 - conv9_2_GAP_substract_loss: 25.3209 - predictions_loss: 4.5658 - val_loss: 8.2076 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6694\n",
      "\n",
      "Epoch 00012: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-12_loss-7.1325_val_loss-8.2076.h5\n",
      "Epoch 13/120\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 7.0076 - pool1_GAP_substract_loss: 27422.8815 - pool2_GAP_substract_loss: 23825.9410 - pool3_GAP_substract_loss: 6894.6837 - conv4_3_norm_GAP_substract_loss: 7.7186 - fc7_GAP_substract_loss: 5.9610 - conv6_2_GAP_substract_loss: 4.1022 - conv7_2_GAP_substract_loss: 3.6866 - conv8_2_GAP_substract_loss: 8.6336 - conv9_2_GAP_substract_loss: 24.4662 - predictions_loss: 4.4854 - val_loss: 8.6236 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.1289\n",
      "\n",
      "Epoch 00013: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-13_loss-7.0076_val_loss-8.6236.h5\n",
      "Epoch 14/120\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 6.9236 - pool1_GAP_substract_loss: 26698.9587 - pool2_GAP_substract_loss: 22714.1589 - pool3_GAP_substract_loss: 6408.5272 - conv4_3_norm_GAP_substract_loss: 8.0602 - fc7_GAP_substract_loss: 6.1127 - conv6_2_GAP_substract_loss: 4.3165 - conv7_2_GAP_substract_loss: 3.9974 - conv8_2_GAP_substract_loss: 9.4525 - conv9_2_GAP_substract_loss: 26.2811 - predictions_loss: 4.4445 - val_loss: 8.3098 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8575\n",
      "\n",
      "Epoch 00014: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-14_loss-6.9236_val_loss-8.3098.h5\n",
      "Epoch 15/120\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 625s 625ms/step - loss: 6.8790 - pool1_GAP_substract_loss: 25962.8249 - pool2_GAP_substract_loss: 21485.3360 - pool3_GAP_substract_loss: 5672.4928 - conv4_3_norm_GAP_substract_loss: 8.0401 - fc7_GAP_substract_loss: 6.0168 - conv6_2_GAP_substract_loss: 4.2580 - conv7_2_GAP_substract_loss: 3.9385 - conv8_2_GAP_substract_loss: 9.1074 - conv9_2_GAP_substract_loss: 24.4563 - predictions_loss: 4.4423 - val_loss: 8.0127 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6019\n",
      "\n",
      "Epoch 00015: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-15_loss-6.8790_val_loss-8.0127.h5\n",
      "Epoch 16/120\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 6.7655 - pool1_GAP_substract_loss: 24998.1122 - pool2_GAP_substract_loss: 20224.3033 - pool3_GAP_substract_loss: 5359.3906 - conv4_3_norm_GAP_substract_loss: 8.2374 - fc7_GAP_substract_loss: 5.8977 - conv6_2_GAP_substract_loss: 4.2355 - conv7_2_GAP_substract_loss: 4.0283 - conv8_2_GAP_substract_loss: 9.4581 - conv9_2_GAP_substract_loss: 24.9709 - predictions_loss: 4.3700 - val_loss: 8.3399 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9697\n",
      "\n",
      "Epoch 00016: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-16_loss-6.7655_val_loss-8.3399.h5\n",
      "Epoch 17/120\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 6.6935 - pool1_GAP_substract_loss: 24227.8020 - pool2_GAP_substract_loss: 18831.1040 - pool3_GAP_substract_loss: 4816.4510 - conv4_3_norm_GAP_substract_loss: 8.1369 - fc7_GAP_substract_loss: 5.8026 - conv6_2_GAP_substract_loss: 4.2570 - conv7_2_GAP_substract_loss: 3.9732 - conv8_2_GAP_substract_loss: 9.1517 - conv9_2_GAP_substract_loss: 24.1439 - predictions_loss: 4.3383 - val_loss: 8.0317 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7011\n",
      "\n",
      "Epoch 00017: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-17_loss-6.6935_val_loss-8.0317.h5\n",
      "Epoch 18/120\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 6.6330 - pool1_GAP_substract_loss: 23391.8252 - pool2_GAP_substract_loss: 18208.0877 - pool3_GAP_substract_loss: 4350.3030 - conv4_3_norm_GAP_substract_loss: 8.2235 - fc7_GAP_substract_loss: 5.7101 - conv6_2_GAP_substract_loss: 4.0902 - conv7_2_GAP_substract_loss: 3.7931 - conv8_2_GAP_substract_loss: 8.8843 - conv9_2_GAP_substract_loss: 23.1724 - predictions_loss: 4.3171 - val_loss: 7.8032 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.5112\n",
      "\n",
      "Epoch 00018: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-18_loss-6.6330_val_loss-7.8032.h5\n",
      "Epoch 19/120\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 6.5400 - pool1_GAP_substract_loss: 23077.6641 - pool2_GAP_substract_loss: 17805.5499 - pool3_GAP_substract_loss: 4082.7447 - conv4_3_norm_GAP_substract_loss: 8.5937 - fc7_GAP_substract_loss: 5.7920 - conv6_2_GAP_substract_loss: 4.3346 - conv7_2_GAP_substract_loss: 4.0810 - conv8_2_GAP_substract_loss: 9.5546 - conv9_2_GAP_substract_loss: 24.8295 - predictions_loss: 4.2624 - val_loss: 7.8382 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.5839\n",
      "\n",
      "Epoch 00019: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-19_loss-6.5400_val_loss-7.8382.h5\n",
      "Epoch 20/120\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 6.5066 - pool1_GAP_substract_loss: 22470.4335 - pool2_GAP_substract_loss: 17191.4832 - pool3_GAP_substract_loss: 3776.7586 - conv4_3_norm_GAP_substract_loss: 8.8595 - fc7_GAP_substract_loss: 5.7411 - conv6_2_GAP_substract_loss: 4.1958 - conv7_2_GAP_substract_loss: 3.9919 - conv8_2_GAP_substract_loss: 9.5941 - conv9_2_GAP_substract_loss: 24.6372 - predictions_loss: 4.2662 - val_loss: 7.9228 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7051\n",
      "\n",
      "Epoch 00020: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-20_loss-6.5066_val_loss-7.9228.h5\n",
      "Epoch 21/120\n",
      "\n",
      "Epoch 00021: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 6.4331 - pool1_GAP_substract_loss: 22147.8399 - pool2_GAP_substract_loss: 16790.9581 - pool3_GAP_substract_loss: 3461.2807 - conv4_3_norm_GAP_substract_loss: 8.6878 - fc7_GAP_substract_loss: 5.6637 - conv6_2_GAP_substract_loss: 4.2257 - conv7_2_GAP_substract_loss: 4.0933 - conv8_2_GAP_substract_loss: 9.6782 - conv9_2_GAP_substract_loss: 24.1783 - predictions_loss: 4.2292 - val_loss: 7.8031 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6214\n",
      "\n",
      "Epoch 00021: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-21_loss-6.4331_val_loss-7.8031.h5\n",
      "Epoch 22/120\n",
      "\n",
      "Epoch 00022: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 624s 624ms/step - loss: 6.3684 - pool1_GAP_substract_loss: 21288.8694 - pool2_GAP_substract_loss: 15912.2867 - pool3_GAP_substract_loss: 3297.9235 - conv4_3_norm_GAP_substract_loss: 8.9287 - fc7_GAP_substract_loss: 5.7068 - conv6_2_GAP_substract_loss: 4.2777 - conv7_2_GAP_substract_loss: 4.1837 - conv8_2_GAP_substract_loss: 9.9131 - conv9_2_GAP_substract_loss: 24.6705 - predictions_loss: 4.2003 - val_loss: 7.8828 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7362\n",
      "\n",
      "Epoch 00022: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-22_loss-6.3684_val_loss-7.8828.h5\n",
      "Epoch 23/120\n",
      "\n",
      "Epoch 00023: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 6.3297 - pool1_GAP_substract_loss: 20601.9356 - pool2_GAP_substract_loss: 15039.5731 - pool3_GAP_substract_loss: 2999.4934 - conv4_3_norm_GAP_substract_loss: 8.8570 - fc7_GAP_substract_loss: 5.3993 - conv6_2_GAP_substract_loss: 4.1363 - conv7_2_GAP_substract_loss: 4.0835 - conv8_2_GAP_substract_loss: 9.8043 - conv9_2_GAP_substract_loss: 24.1985 - predictions_loss: 4.1964 - val_loss: 7.7702 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6579\n",
      "\n",
      "Epoch 00023: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-23_loss-6.3297_val_loss-7.7702.h5\n",
      "Epoch 24/120\n",
      "\n",
      "Epoch 00024: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 6.2492 - pool1_GAP_substract_loss: 20936.0134 - pool2_GAP_substract_loss: 14911.4912 - pool3_GAP_substract_loss: 2822.4797 - conv4_3_norm_GAP_substract_loss: 9.0647 - fc7_GAP_substract_loss: 5.5707 - conv6_2_GAP_substract_loss: 4.4164 - conv7_2_GAP_substract_loss: 4.4084 - conv8_2_GAP_substract_loss: 10.6641 - conv9_2_GAP_substract_loss: 25.9562 - predictions_loss: 4.1498 - val_loss: 7.8767 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7980\n",
      "\n",
      "Epoch 00024: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-24_loss-6.2492_val_loss-7.8767.h5\n",
      "Epoch 25/120\n",
      "\n",
      "Epoch 00025: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 6.2147 - pool1_GAP_substract_loss: 20016.3111 - pool2_GAP_substract_loss: 14361.6222 - pool3_GAP_substract_loss: 2609.1066 - conv4_3_norm_GAP_substract_loss: 9.1836 - fc7_GAP_substract_loss: 5.4282 - conv6_2_GAP_substract_loss: 4.2453 - conv7_2_GAP_substract_loss: 4.1755 - conv8_2_GAP_substract_loss: 9.9014 - conv9_2_GAP_substract_loss: 23.4922 - predictions_loss: 4.1486 - val_loss: 7.6528 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6067\n",
      "\n",
      "Epoch 00025: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-25_loss-6.2147_val_loss-7.6528.h5\n",
      "Epoch 26/120\n",
      "\n",
      "Epoch 00026: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 6.1522 - pool1_GAP_substract_loss: 19765.4649 - pool2_GAP_substract_loss: 14040.8839 - pool3_GAP_substract_loss: 2546.3662 - conv4_3_norm_GAP_substract_loss: 9.3312 - fc7_GAP_substract_loss: 5.2997 - conv6_2_GAP_substract_loss: 4.2132 - conv7_2_GAP_substract_loss: 4.3979 - conv8_2_GAP_substract_loss: 10.6785 - conv9_2_GAP_substract_loss: 24.8293 - predictions_loss: 4.1184 - val_loss: 7.7964 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7821\n",
      "\n",
      "Epoch 00026: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-26_loss-6.1522_val_loss-7.7964.h5\n",
      "Epoch 27/120\n",
      "\n",
      "Epoch 00027: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 6.0799 - pool1_GAP_substract_loss: 19240.1712 - pool2_GAP_substract_loss: 13705.1578 - pool3_GAP_substract_loss: 2377.8676 - conv4_3_norm_GAP_substract_loss: 9.2001 - fc7_GAP_substract_loss: 5.2433 - conv6_2_GAP_substract_loss: 4.2439 - conv7_2_GAP_substract_loss: 4.4786 - conv8_2_GAP_substract_loss: 10.7105 - conv9_2_GAP_substract_loss: 24.1050 - predictions_loss: 4.0777 - val_loss: 7.6212 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6382\n",
      "\n",
      "Epoch 00027: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-27_loss-6.0799_val_loss-7.6212.h5\n",
      "Epoch 28/120\n",
      "\n",
      "Epoch 00028: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 6.0214 - pool1_GAP_substract_loss: 18803.9137 - pool2_GAP_substract_loss: 13486.9016 - pool3_GAP_substract_loss: 2304.1593 - conv4_3_norm_GAP_substract_loss: 9.5835 - fc7_GAP_substract_loss: 5.2927 - conv6_2_GAP_substract_loss: 4.2273 - conv7_2_GAP_substract_loss: 4.4128 - conv8_2_GAP_substract_loss: 10.6699 - conv9_2_GAP_substract_loss: 24.2021 - predictions_loss: 4.0502 - val_loss: 7.9043 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9518\n",
      "\n",
      "Epoch 00028: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-28_loss-6.0214_val_loss-7.9043.h5\n",
      "Epoch 29/120\n",
      "\n",
      "Epoch 00029: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 625s 625ms/step - loss: 5.9673 - pool1_GAP_substract_loss: 18477.6784 - pool2_GAP_substract_loss: 13548.5261 - pool3_GAP_substract_loss: 2239.8291 - conv4_3_norm_GAP_substract_loss: 9.8922 - fc7_GAP_substract_loss: 5.4101 - conv6_2_GAP_substract_loss: 4.4958 - conv7_2_GAP_substract_loss: 4.8365 - conv8_2_GAP_substract_loss: 11.6728 - conv9_2_GAP_substract_loss: 26.2223 - predictions_loss: 4.0263 - val_loss: 7.8015 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8789\n",
      "\n",
      "Epoch 00029: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-29_loss-5.9673_val_loss-7.8015.h5\n",
      "Epoch 30/120\n",
      "\n",
      "Epoch 00030: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 5.9204 - pool1_GAP_substract_loss: 18339.0931 - pool2_GAP_substract_loss: 12937.0064 - pool3_GAP_substract_loss: 2095.7277 - conv4_3_norm_GAP_substract_loss: 9.9679 - fc7_GAP_substract_loss: 5.2453 - conv6_2_GAP_substract_loss: 4.3503 - conv7_2_GAP_substract_loss: 4.7288 - conv8_2_GAP_substract_loss: 11.6706 - conv9_2_GAP_substract_loss: 25.9871 - predictions_loss: 4.0089 - val_loss: 7.8583 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9646\n",
      "\n",
      "Epoch 00030: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-30_loss-5.9204_val_loss-7.8583.h5\n",
      "Epoch 31/120\n",
      "\n",
      "Epoch 00031: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 5.8695 - pool1_GAP_substract_loss: 17921.8939 - pool2_GAP_substract_loss: 12086.9797 - pool3_GAP_substract_loss: 1925.8982 - conv4_3_norm_GAP_substract_loss: 9.9904 - fc7_GAP_substract_loss: 5.1827 - conv6_2_GAP_substract_loss: 4.2981 - conv7_2_GAP_substract_loss: 4.7568 - conv8_2_GAP_substract_loss: 11.7669 - conv9_2_GAP_substract_loss: 25.6078 - predictions_loss: 3.9868 - val_loss: 7.8107 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9454\n",
      "\n",
      "Epoch 00031: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-31_loss-5.8695_val_loss-7.8107.h5\n",
      "Epoch 32/120\n",
      "\n",
      "Epoch 00032: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 5.8676 - pool1_GAP_substract_loss: 17687.6848 - pool2_GAP_substract_loss: 12217.2629 - pool3_GAP_substract_loss: 1839.6967 - conv4_3_norm_GAP_substract_loss: 9.8794 - fc7_GAP_substract_loss: 5.0908 - conv6_2_GAP_substract_loss: 4.2172 - conv7_2_GAP_substract_loss: 4.6016 - conv8_2_GAP_substract_loss: 11.3004 - conv9_2_GAP_substract_loss: 24.4626 - predictions_loss: 4.0131 - val_loss: 7.9551 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.1177\n",
      "\n",
      "Epoch 00032: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-32_loss-5.8676_val_loss-7.9551.h5\n",
      "Epoch 33/120\n",
      "\n",
      "Epoch 00033: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 5.8111 - pool1_GAP_substract_loss: 17363.7594 - pool2_GAP_substract_loss: 12291.6206 - pool3_GAP_substract_loss: 1840.7466 - conv4_3_norm_GAP_substract_loss: 10.1126 - fc7_GAP_substract_loss: 5.0703 - conv6_2_GAP_substract_loss: 4.1730 - conv7_2_GAP_substract_loss: 4.4717 - conv8_2_GAP_substract_loss: 10.9162 - conv9_2_GAP_substract_loss: 23.0021 - predictions_loss: 3.9841 - val_loss: 7.7014 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8909\n",
      "\n",
      "Epoch 00033: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-33_loss-5.8111_val_loss-7.7014.h5\n",
      "Epoch 34/120\n",
      "\n",
      "Epoch 00034: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 5.7360 - pool1_GAP_substract_loss: 17181.4273 - pool2_GAP_substract_loss: 11927.7926 - pool3_GAP_substract_loss: 1733.6109 - conv4_3_norm_GAP_substract_loss: 10.1675 - fc7_GAP_substract_loss: 5.2280 - conv6_2_GAP_substract_loss: 4.3928 - conv7_2_GAP_substract_loss: 4.7467 - conv8_2_GAP_substract_loss: 11.5466 - conv9_2_GAP_substract_loss: 24.0463 - predictions_loss: 3.9357 - val_loss: 7.8299 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.0459\n",
      "\n",
      "Epoch 00034: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-34_loss-5.7360_val_loss-7.8299.h5\n",
      "Epoch 35/120\n",
      "\n",
      "Epoch 00035: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 5.7149 - pool1_GAP_substract_loss: 17042.7487 - pool2_GAP_substract_loss: 11796.2705 - pool3_GAP_substract_loss: 1638.6240 - conv4_3_norm_GAP_substract_loss: 10.0561 - fc7_GAP_substract_loss: 5.0553 - conv6_2_GAP_substract_loss: 4.2341 - conv7_2_GAP_substract_loss: 4.5933 - conv8_2_GAP_substract_loss: 11.2265 - conv9_2_GAP_substract_loss: 23.0829 - predictions_loss: 3.9407 - val_loss: 7.5777 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8194\n",
      "\n",
      "Epoch 00035: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-35_loss-5.7149_val_loss-7.5777.h5\n",
      "Epoch 36/120\n",
      "\n",
      "Epoch 00036: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 625s 625ms/step - loss: 5.6404 - pool1_GAP_substract_loss: 16846.5087 - pool2_GAP_substract_loss: 11336.9095 - pool3_GAP_substract_loss: 1534.0144 - conv4_3_norm_GAP_substract_loss: 10.1762 - fc7_GAP_substract_loss: 4.9923 - conv6_2_GAP_substract_loss: 4.2635 - conv7_2_GAP_substract_loss: 4.6409 - conv8_2_GAP_substract_loss: 11.1469 - conv9_2_GAP_substract_loss: 22.6080 - predictions_loss: 3.8920 - val_loss: 7.7004 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9676\n",
      "\n",
      "Epoch 00036: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-36_loss-5.6404_val_loss-7.7004.h5\n",
      "Epoch 37/120\n",
      "\n",
      "Epoch 00037: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 5.6123 - pool1_GAP_substract_loss: 17153.0816 - pool2_GAP_substract_loss: 11179.0901 - pool3_GAP_substract_loss: 1491.8599 - conv4_3_norm_GAP_substract_loss: 9.9414 - fc7_GAP_substract_loss: 5.0112 - conv6_2_GAP_substract_loss: 4.3735 - conv7_2_GAP_substract_loss: 4.9106 - conv8_2_GAP_substract_loss: 11.8835 - conv9_2_GAP_substract_loss: 24.3793 - predictions_loss: 3.8889 - val_loss: 7.8916 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.1834\n",
      "\n",
      "Epoch 00037: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-37_loss-5.6123_val_loss-7.8916.h5\n",
      "Epoch 38/120\n",
      "\n",
      "Epoch 00038: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 5.5860 - pool1_GAP_substract_loss: 16641.4664 - pool2_GAP_substract_loss: 10909.4753 - pool3_GAP_substract_loss: 1449.9203 - conv4_3_norm_GAP_substract_loss: 10.2730 - fc7_GAP_substract_loss: 5.0118 - conv6_2_GAP_substract_loss: 4.4232 - conv7_2_GAP_substract_loss: 4.9609 - conv8_2_GAP_substract_loss: 12.0166 - conv9_2_GAP_substract_loss: 24.0828 - predictions_loss: 3.8871 - val_loss: 7.4348 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7507\n",
      "\n",
      "Epoch 00038: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-38_loss-5.5860_val_loss-7.4348.h5\n",
      "Epoch 39/120\n",
      "\n",
      "Epoch 00039: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 5.5108 - pool1_GAP_substract_loss: 16312.9237 - pool2_GAP_substract_loss: 10727.3536 - pool3_GAP_substract_loss: 1356.0972 - conv4_3_norm_GAP_substract_loss: 10.3545 - fc7_GAP_substract_loss: 5.0808 - conv6_2_GAP_substract_loss: 4.4400 - conv7_2_GAP_substract_loss: 4.9367 - conv8_2_GAP_substract_loss: 11.8229 - conv9_2_GAP_substract_loss: 23.4551 - predictions_loss: 3.8356 - val_loss: 7.6005 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9400\n",
      "\n",
      "Epoch 00039: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-39_loss-5.5108_val_loss-7.6005.h5\n",
      "Epoch 40/120\n",
      "\n",
      "Epoch 00040: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 5.5416 - pool1_GAP_substract_loss: 15791.1382 - pool2_GAP_substract_loss: 10311.5524 - pool3_GAP_substract_loss: 1318.5924 - conv4_3_norm_GAP_substract_loss: 10.1637 - fc7_GAP_substract_loss: 5.0770 - conv6_2_GAP_substract_loss: 4.4842 - conv7_2_GAP_substract_loss: 5.0840 - conv8_2_GAP_substract_loss: 12.0114 - conv9_2_GAP_substract_loss: 23.1033 - predictions_loss: 3.8899 - val_loss: 7.6294 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9919\n",
      "\n",
      "Epoch 00040: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-40_loss-5.5416_val_loss-7.6294.h5\n",
      "Epoch 41/120\n",
      "\n",
      "Epoch 00041: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 5.4657 - pool1_GAP_substract_loss: 16000.3367 - pool2_GAP_substract_loss: 10519.4743 - pool3_GAP_substract_loss: 1301.9339 - conv4_3_norm_GAP_substract_loss: 10.3739 - fc7_GAP_substract_loss: 5.0131 - conv6_2_GAP_substract_loss: 4.4354 - conv7_2_GAP_substract_loss: 5.0730 - conv8_2_GAP_substract_loss: 12.2625 - conv9_2_GAP_substract_loss: 23.6735 - predictions_loss: 3.8366 - val_loss: 7.6458 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.0307\n",
      "\n",
      "Epoch 00041: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-41_loss-5.4657_val_loss-7.6458.h5\n",
      "Epoch 42/120\n",
      "\n",
      "Epoch 00042: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 5.4343 - pool1_GAP_substract_loss: 15531.1327 - pool2_GAP_substract_loss: 10301.9000 - pool3_GAP_substract_loss: 1239.9227 - conv4_3_norm_GAP_substract_loss: 10.3046 - fc7_GAP_substract_loss: 4.9086 - conv6_2_GAP_substract_loss: 4.3432 - conv7_2_GAP_substract_loss: 4.9391 - conv8_2_GAP_substract_loss: 11.6919 - conv9_2_GAP_substract_loss: 21.9025 - predictions_loss: 3.8276 - val_loss: 7.6465 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.0535\n",
      "\n",
      "Epoch 00042: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-42_loss-5.4343_val_loss-7.6465.h5\n",
      "Epoch 43/120\n",
      "\n",
      "Epoch 00043: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 625s 625ms/step - loss: 5.3914 - pool1_GAP_substract_loss: 15542.1203 - pool2_GAP_substract_loss: 10013.6815 - pool3_GAP_substract_loss: 1230.4262 - conv4_3_norm_GAP_substract_loss: 10.3340 - fc7_GAP_substract_loss: 4.9014 - conv6_2_GAP_substract_loss: 4.3243 - conv7_2_GAP_substract_loss: 4.9491 - conv8_2_GAP_substract_loss: 11.7811 - conv9_2_GAP_substract_loss: 21.9580 - predictions_loss: 3.8065 - val_loss: 7.5125 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9409\n",
      "\n",
      "Epoch 00043: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-43_loss-5.3914_val_loss-7.5125.h5\n",
      "Epoch 44/120\n",
      "\n",
      "Epoch 00044: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 5.3564 - pool1_GAP_substract_loss: 15394.6713 - pool2_GAP_substract_loss: 9811.8825 - pool3_GAP_substract_loss: 1187.0013 - conv4_3_norm_GAP_substract_loss: 10.7228 - fc7_GAP_substract_loss: 4.8225 - conv6_2_GAP_substract_loss: 4.2578 - conv7_2_GAP_substract_loss: 4.9989 - conv8_2_GAP_substract_loss: 12.4285 - conv9_2_GAP_substract_loss: 23.4844 - predictions_loss: 3.7925 - val_loss: 7.7274 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.1767\n",
      "\n",
      "Epoch 00044: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-44_loss-5.3564_val_loss-7.7274.h5\n",
      "Epoch 45/120\n",
      "\n",
      "Epoch 00045: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 5.3190 - pool1_GAP_substract_loss: 15181.5614 - pool2_GAP_substract_loss: 9801.2558 - pool3_GAP_substract_loss: 1138.6431 - conv4_3_norm_GAP_substract_loss: 10.3368 - fc7_GAP_substract_loss: 5.0543 - conv6_2_GAP_substract_loss: 4.5449 - conv7_2_GAP_substract_loss: 5.2876 - conv8_2_GAP_substract_loss: 12.5333 - conv9_2_GAP_substract_loss: 22.9619 - predictions_loss: 3.7758 - val_loss: 7.7831 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.2527\n",
      "\n",
      "Epoch 00045: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-45_loss-5.3190_val_loss-7.7831.h5\n",
      "Epoch 46/120\n",
      "\n",
      "Epoch 00046: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 5.2949 - pool1_GAP_substract_loss: 15003.7336 - pool2_GAP_substract_loss: 9534.5610 - pool3_GAP_substract_loss: 1109.2872 - conv4_3_norm_GAP_substract_loss: 10.3661 - fc7_GAP_substract_loss: 4.9899 - conv6_2_GAP_substract_loss: 4.5156 - conv7_2_GAP_substract_loss: 5.4093 - conv8_2_GAP_substract_loss: 13.0451 - conv9_2_GAP_substract_loss: 23.5888 - predictions_loss: 3.7719 - val_loss: 7.7349 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.2243\n",
      "\n",
      "Epoch 00046: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-46_loss-5.2949_val_loss-7.7349.h5\n",
      "Epoch 47/120\n",
      "\n",
      "Epoch 00047: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 5.2416 - pool1_GAP_substract_loss: 14828.0343 - pool2_GAP_substract_loss: 9470.0412 - pool3_GAP_substract_loss: 1054.8206 - conv4_3_norm_GAP_substract_loss: 10.5345 - fc7_GAP_substract_loss: 4.9621 - conv6_2_GAP_substract_loss: 4.4467 - conv7_2_GAP_substract_loss: 5.3027 - conv8_2_GAP_substract_loss: 12.9481 - conv9_2_GAP_substract_loss: 23.3070 - predictions_loss: 3.7383 - val_loss: 7.6111 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.1200\n",
      "\n",
      "Epoch 00047: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-47_loss-5.2416_val_loss-7.6111.h5\n",
      "Epoch 48/120\n",
      "\n",
      "Epoch 00048: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 5.2203 - pool1_GAP_substract_loss: 14834.7184 - pool2_GAP_substract_loss: 9618.0442 - pool3_GAP_substract_loss: 1060.2020 - conv4_3_norm_GAP_substract_loss: 10.6885 - fc7_GAP_substract_loss: 5.0043 - conv6_2_GAP_substract_loss: 4.6073 - conv7_2_GAP_substract_loss: 5.4715 - conv8_2_GAP_substract_loss: 13.0860 - conv9_2_GAP_substract_loss: 23.6523 - predictions_loss: 3.7363 - val_loss: 7.3997 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9278\n",
      "\n",
      "Epoch 00048: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-48_loss-5.2203_val_loss-7.3997.h5\n",
      "Epoch 49/120\n",
      "\n",
      "Epoch 00049: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 5.1724 - pool1_GAP_substract_loss: 14631.3536 - pool2_GAP_substract_loss: 9587.1169 - pool3_GAP_substract_loss: 1026.3615 - conv4_3_norm_GAP_substract_loss: 10.8706 - fc7_GAP_substract_loss: 4.9667 - conv6_2_GAP_substract_loss: 4.5436 - conv7_2_GAP_substract_loss: 5.4428 - conv8_2_GAP_substract_loss: 12.7496 - conv9_2_GAP_substract_loss: 22.3983 - predictions_loss: 3.7074 - val_loss: 7.3030 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8498\n",
      "\n",
      "Epoch 00049: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-49_loss-5.1724_val_loss-7.3030.h5\n",
      "Epoch 50/120\n",
      "\n",
      "Epoch 00050: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 624s 624ms/step - loss: 5.1715 - pool1_GAP_substract_loss: 14600.4536 - pool2_GAP_substract_loss: 9248.0358 - pool3_GAP_substract_loss: 1020.2782 - conv4_3_norm_GAP_substract_loss: 10.9725 - fc7_GAP_substract_loss: 5.0042 - conv6_2_GAP_substract_loss: 4.5399 - conv7_2_GAP_substract_loss: 5.5639 - conv8_2_GAP_substract_loss: 13.2127 - conv9_2_GAP_substract_loss: 22.9548 - predictions_loss: 3.7249 - val_loss: 7.5268 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.0918\n",
      "\n",
      "Epoch 00050: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-50_loss-5.1715_val_loss-7.5268.h5\n",
      "Epoch 51/120\n",
      "\n",
      "Epoch 00051: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 5.1672 - pool1_GAP_substract_loss: 14021.7134 - pool2_GAP_substract_loss: 9028.4511 - pool3_GAP_substract_loss: 952.6442 - conv4_3_norm_GAP_substract_loss: 10.3578 - fc7_GAP_substract_loss: 4.8604 - conv6_2_GAP_substract_loss: 4.3688 - conv7_2_GAP_substract_loss: 5.3352 - conv8_2_GAP_substract_loss: 12.4869 - conv9_2_GAP_substract_loss: 21.4072 - predictions_loss: 3.7387 - val_loss: 7.6028 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.1856\n",
      "\n",
      "Epoch 00051: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-51_loss-5.1672_val_loss-7.6028.h5\n",
      "Epoch 52/120\n",
      "\n",
      "Epoch 00052: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 626s 626ms/step - loss: 5.0863 - pool1_GAP_substract_loss: 14352.8656 - pool2_GAP_substract_loss: 8889.4321 - pool3_GAP_substract_loss: 957.0224 - conv4_3_norm_GAP_substract_loss: 10.6623 - fc7_GAP_substract_loss: 4.9919 - conv6_2_GAP_substract_loss: 4.6254 - conv7_2_GAP_substract_loss: 5.7001 - conv8_2_GAP_substract_loss: 13.4043 - conv9_2_GAP_substract_loss: 23.1493 - predictions_loss: 3.6753 - val_loss: 7.6759 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.2760\n",
      "\n",
      "Epoch 00052: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-52_loss-5.0863_val_loss-7.6759.h5\n",
      "Epoch 53/120\n",
      "\n",
      "Epoch 00053: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 5.0626 - pool1_GAP_substract_loss: 14258.6212 - pool2_GAP_substract_loss: 8925.2637 - pool3_GAP_substract_loss: 921.4542 - conv4_3_norm_GAP_substract_loss: 10.5850 - fc7_GAP_substract_loss: 4.9239 - conv6_2_GAP_substract_loss: 4.6251 - conv7_2_GAP_substract_loss: 5.7454 - conv8_2_GAP_substract_loss: 13.3836 - conv9_2_GAP_substract_loss: 22.3696 - predictions_loss: 3.6688 - val_loss: 7.4583 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.0752\n",
      "\n",
      "Epoch 00053: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-53_loss-5.0626_val_loss-7.4583.h5\n",
      "Epoch 54/120\n",
      "\n",
      "Epoch 00054: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 5.0495 - pool1_GAP_substract_loss: 14324.8870 - pool2_GAP_substract_loss: 8713.0132 - pool3_GAP_substract_loss: 900.5835 - conv4_3_norm_GAP_substract_loss: 10.7233 - fc7_GAP_substract_loss: 4.9488 - conv6_2_GAP_substract_loss: 4.6669 - conv7_2_GAP_substract_loss: 5.8762 - conv8_2_GAP_substract_loss: 13.6971 - conv9_2_GAP_substract_loss: 22.8259 - predictions_loss: 3.6724 - val_loss: 7.7247 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.3582\n",
      "\n",
      "Epoch 00054: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-54_loss-5.0495_val_loss-7.7247.h5\n",
      "Epoch 55/120\n",
      "\n",
      "Epoch 00055: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 623s 623ms/step - loss: 5.0377 - pool1_GAP_substract_loss: 13988.3996 - pool2_GAP_substract_loss: 8534.3730 - pool3_GAP_substract_loss: 888.8297 - conv4_3_norm_GAP_substract_loss: 11.1224 - fc7_GAP_substract_loss: 4.9405 - conv6_2_GAP_substract_loss: 4.6899 - conv7_2_GAP_substract_loss: 5.9894 - conv8_2_GAP_substract_loss: 13.9537 - conv9_2_GAP_substract_loss: 22.8084 - predictions_loss: 3.6769 - val_loss: 7.6420 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.2916\n",
      "\n",
      "Epoch 00055: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-55_loss-5.0377_val_loss-7.6420.h5\n",
      "Epoch 56/120\n",
      "\n",
      "Epoch 00056: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 4.8230 - pool1_GAP_substract_loss: 14125.0517 - pool2_GAP_substract_loss: 8540.7048 - pool3_GAP_substract_loss: 881.8837 - conv4_3_norm_GAP_substract_loss: 11.1033 - fc7_GAP_substract_loss: 5.2449 - conv6_2_GAP_substract_loss: 5.0985 - conv7_2_GAP_substract_loss: 6.5237 - conv8_2_GAP_substract_loss: 14.9371 - conv9_2_GAP_substract_loss: 24.0530 - predictions_loss: 3.4713 - val_loss: 7.4065 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.0582\n",
      "\n",
      "Epoch 00056: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-56_loss-4.8230_val_loss-7.4065.h5\n",
      "Epoch 57/120\n",
      "\n",
      "Epoch 00057: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 624s 624ms/step - loss: 4.8067 - pool1_GAP_substract_loss: 13876.4701 - pool2_GAP_substract_loss: 8791.1952 - pool3_GAP_substract_loss: 926.9488 - conv4_3_norm_GAP_substract_loss: 10.7511 - fc7_GAP_substract_loss: 5.2232 - conv6_2_GAP_substract_loss: 5.1395 - conv7_2_GAP_substract_loss: 6.5809 - conv8_2_GAP_substract_loss: 14.8208 - conv9_2_GAP_substract_loss: 23.2609 - predictions_loss: 3.4572 - val_loss: 7.3816 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.0355\n",
      "\n",
      "Epoch 00057: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-57_loss-4.8067_val_loss-7.3816.h5\n",
      "Epoch 58/120\n",
      "\n",
      "Epoch 00058: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 4.7496 - pool1_GAP_substract_loss: 14081.9993 - pool2_GAP_substract_loss: 9179.3726 - pool3_GAP_substract_loss: 969.0042 - conv4_3_norm_GAP_substract_loss: 11.2002 - fc7_GAP_substract_loss: 5.3949 - conv6_2_GAP_substract_loss: 5.4557 - conv7_2_GAP_substract_loss: 7.0719 - conv8_2_GAP_substract_loss: 15.9819 - conv9_2_GAP_substract_loss: 25.1220 - predictions_loss: 3.4022 - val_loss: 7.5327 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.1888\n",
      "\n",
      "Epoch 00058: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-58_loss-4.7496_val_loss-7.5327.h5\n",
      "Epoch 59/120\n",
      "\n",
      "Epoch 00059: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 631s 631ms/step - loss: 4.7742 - pool1_GAP_substract_loss: 13839.6573 - pool2_GAP_substract_loss: 9246.5851 - pool3_GAP_substract_loss: 996.7811 - conv4_3_norm_GAP_substract_loss: 11.2676 - fc7_GAP_substract_loss: 5.5666 - conv6_2_GAP_substract_loss: 5.6417 - conv7_2_GAP_substract_loss: 7.2832 - conv8_2_GAP_substract_loss: 16.4298 - conv9_2_GAP_substract_loss: 25.8135 - predictions_loss: 3.4289 - val_loss: 7.4840 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.1422\n",
      "\n",
      "Epoch 00059: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-59_loss-4.7742_val_loss-7.4840.h5\n",
      "Epoch 60/120\n",
      "\n",
      "Epoch 00060: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 641s 641ms/step - loss: 4.7457 - pool1_GAP_substract_loss: 14019.8047 - pool2_GAP_substract_loss: 9411.0242 - pool3_GAP_substract_loss: 1024.8214 - conv4_3_norm_GAP_substract_loss: 10.8956 - fc7_GAP_substract_loss: 5.5541 - conv6_2_GAP_substract_loss: 5.6380 - conv7_2_GAP_substract_loss: 7.2708 - conv8_2_GAP_substract_loss: 16.1760 - conv9_2_GAP_substract_loss: 24.9862 - predictions_loss: 3.4025 - val_loss: 7.4107 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.0711\n",
      "\n",
      "Epoch 00060: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-60_loss-4.7457_val_loss-7.4107.h5\n",
      "Epoch 61/120\n",
      "\n",
      "Epoch 00061: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 4.7213 - pool1_GAP_substract_loss: 13992.7843 - pool2_GAP_substract_loss: 9680.8874 - pool3_GAP_substract_loss: 1052.7984 - conv4_3_norm_GAP_substract_loss: 10.8945 - fc7_GAP_substract_loss: 5.5738 - conv6_2_GAP_substract_loss: 5.6657 - conv7_2_GAP_substract_loss: 7.2894 - conv8_2_GAP_substract_loss: 16.4417 - conv9_2_GAP_substract_loss: 25.6888 - predictions_loss: 3.3802 - val_loss: 7.5434 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.2059\n",
      "\n",
      "Epoch 00061: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-61_loss-4.7213_val_loss-7.5434.h5\n",
      "Epoch 62/120\n",
      "\n",
      "Epoch 00062: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 623s 623ms/step - loss: 4.7388 - pool1_GAP_substract_loss: 14076.5535 - pool2_GAP_substract_loss: 9779.3568 - pool3_GAP_substract_loss: 1059.9084 - conv4_3_norm_GAP_substract_loss: 10.9615 - fc7_GAP_substract_loss: 5.7376 - conv6_2_GAP_substract_loss: 5.8442 - conv7_2_GAP_substract_loss: 7.5089 - conv8_2_GAP_substract_loss: 16.7969 - conv9_2_GAP_substract_loss: 26.1567 - predictions_loss: 3.3998 - val_loss: 7.5753 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.2400\n",
      "\n",
      "Epoch 00062: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-62_loss-4.7388_val_loss-7.5753.h5\n",
      "Epoch 63/120\n",
      "\n",
      "Epoch 00063: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 622s 622ms/step - loss: 4.7329 - pool1_GAP_substract_loss: 13792.9787 - pool2_GAP_substract_loss: 9573.7131 - pool3_GAP_substract_loss: 1058.9243 - conv4_3_norm_GAP_substract_loss: 10.9108 - fc7_GAP_substract_loss: 5.7253 - conv6_2_GAP_substract_loss: 5.9442 - conv7_2_GAP_substract_loss: 7.6271 - conv8_2_GAP_substract_loss: 16.7783 - conv9_2_GAP_substract_loss: 26.0712 - predictions_loss: 3.3962 - val_loss: 7.4478 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.1146\n",
      "\n",
      "Epoch 00063: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-63_loss-4.7329_val_loss-7.4478.h5\n",
      "Epoch 64/120\n",
      "\n",
      "Epoch 00064: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 623s 623ms/step - loss: 4.6983 - pool1_GAP_substract_loss: 13728.2005 - pool2_GAP_substract_loss: 9591.2780 - pool3_GAP_substract_loss: 1070.8608 - conv4_3_norm_GAP_substract_loss: 11.0031 - fc7_GAP_substract_loss: 5.7157 - conv6_2_GAP_substract_loss: 5.8825 - conv7_2_GAP_substract_loss: 7.5422 - conv8_2_GAP_substract_loss: 16.7851 - conv9_2_GAP_substract_loss: 25.8967 - predictions_loss: 3.3637 - val_loss: 7.5934 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.2623\n",
      "\n",
      "Epoch 00064: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-64_loss-4.6983_val_loss-7.5934.h5\n",
      "Epoch 65/120\n",
      "\n",
      "Epoch 00065: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 622s 622ms/step - loss: 4.7064 - pool1_GAP_substract_loss: 13882.1597 - pool2_GAP_substract_loss: 9738.6439 - pool3_GAP_substract_loss: 1076.0290 - conv4_3_norm_GAP_substract_loss: 10.8949 - fc7_GAP_substract_loss: 5.7337 - conv6_2_GAP_substract_loss: 5.9137 - conv7_2_GAP_substract_loss: 7.5579 - conv8_2_GAP_substract_loss: 16.6548 - conv9_2_GAP_substract_loss: 25.6357 - predictions_loss: 3.3739 - val_loss: 7.5096 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.1806\n",
      "\n",
      "Epoch 00065: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-65_loss-4.7064_val_loss-7.5096.h5\n",
      "Epoch 66/120\n",
      "\n",
      "Epoch 00066: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 623s 623ms/step - loss: 4.7342 - pool1_GAP_substract_loss: 13689.7928 - pool2_GAP_substract_loss: 9813.6275 - pool3_GAP_substract_loss: 1090.3401 - conv4_3_norm_GAP_substract_loss: 10.6618 - fc7_GAP_substract_loss: 5.6837 - conv6_2_GAP_substract_loss: 5.8696 - conv7_2_GAP_substract_loss: 7.5740 - conv8_2_GAP_substract_loss: 16.7704 - conv9_2_GAP_substract_loss: 25.5108 - predictions_loss: 3.4038 - val_loss: 7.5623 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.2355\n",
      "\n",
      "Epoch 00066: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-66_loss-4.7342_val_loss-7.5623.h5\n",
      "Epoch 67/120\n",
      "\n",
      "Epoch 00067: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 4.7177 - pool1_GAP_substract_loss: 13677.2889 - pool2_GAP_substract_loss: 9870.8166 - pool3_GAP_substract_loss: 1099.5960 - conv4_3_norm_GAP_substract_loss: 10.7646 - fc7_GAP_substract_loss: 5.6601 - conv6_2_GAP_substract_loss: 5.8756 - conv7_2_GAP_substract_loss: 7.5282 - conv8_2_GAP_substract_loss: 16.5958 - conv9_2_GAP_substract_loss: 25.4364 - predictions_loss: 3.3894 - val_loss: 7.5102 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.1854\n",
      "\n",
      "Epoch 00067: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-67_loss-4.7177_val_loss-7.5102.h5\n",
      "Epoch 68/120\n",
      "\n",
      "Epoch 00068: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 640s 640ms/step - loss: 4.6924 - pool1_GAP_substract_loss: 13894.6129 - pool2_GAP_substract_loss: 10091.3221 - pool3_GAP_substract_loss: 1125.7289 - conv4_3_norm_GAP_substract_loss: 10.9073 - fc7_GAP_substract_loss: 5.7780 - conv6_2_GAP_substract_loss: 5.9807 - conv7_2_GAP_substract_loss: 7.6608 - conv8_2_GAP_substract_loss: 16.8102 - conv9_2_GAP_substract_loss: 25.5796 - predictions_loss: 3.3662 - val_loss: 7.4592 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.1365\n",
      "\n",
      "Epoch 00068: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-68_loss-4.6924_val_loss-7.4592.h5\n",
      "Epoch 69/120\n",
      "\n",
      "Epoch 00069: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 622s 622ms/step - loss: 4.6846 - pool1_GAP_substract_loss: 13858.1972 - pool2_GAP_substract_loss: 9930.4611 - pool3_GAP_substract_loss: 1126.5890 - conv4_3_norm_GAP_substract_loss: 10.8780 - fc7_GAP_substract_loss: 5.6941 - conv6_2_GAP_substract_loss: 6.0327 - conv7_2_GAP_substract_loss: 7.8402 - conv8_2_GAP_substract_loss: 17.1733 - conv9_2_GAP_substract_loss: 25.7608 - predictions_loss: 3.3605 - val_loss: 7.4452 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.1246\n",
      "\n",
      "Epoch 00069: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-69_loss-4.6846_val_loss-7.4452.h5\n",
      "Epoch 70/120\n",
      "\n",
      "Epoch 00070: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 622s 622ms/step - loss: 4.6717 - pool1_GAP_substract_loss: 13777.1674 - pool2_GAP_substract_loss: 9978.8969 - pool3_GAP_substract_loss: 1139.0287 - conv4_3_norm_GAP_substract_loss: 10.7956 - fc7_GAP_substract_loss: 5.7227 - conv6_2_GAP_substract_loss: 6.0282 - conv7_2_GAP_substract_loss: 7.7975 - conv8_2_GAP_substract_loss: 17.3200 - conv9_2_GAP_substract_loss: 26.2426 - predictions_loss: 3.3496 - val_loss: 7.5185 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.1999\n",
      "\n",
      "Epoch 00070: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-70_loss-4.6717_val_loss-7.5185.h5\n",
      "Epoch 71/120\n",
      "\n",
      "Epoch 00071: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 622s 622ms/step - loss: 4.6437 - pool1_GAP_substract_loss: 13744.5646 - pool2_GAP_substract_loss: 10013.9121 - pool3_GAP_substract_loss: 1144.8029 - conv4_3_norm_GAP_substract_loss: 10.7755 - fc7_GAP_substract_loss: 5.8627 - conv6_2_GAP_substract_loss: 6.1751 - conv7_2_GAP_substract_loss: 8.0317 - conv8_2_GAP_substract_loss: 18.0791 - conv9_2_GAP_substract_loss: 27.3948 - predictions_loss: 3.3237 - val_loss: 7.4135 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.0970\n",
      "\n",
      "Epoch 00071: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-71_loss-4.6437_val_loss-7.4135.h5\n",
      "Epoch 72/120\n",
      "\n",
      "Epoch 00072: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 621s 621ms/step - loss: 4.6517 - pool1_GAP_substract_loss: 13681.0717 - pool2_GAP_substract_loss: 10057.0769 - pool3_GAP_substract_loss: 1145.2180 - conv4_3_norm_GAP_substract_loss: 10.8647 - fc7_GAP_substract_loss: 5.8898 - conv6_2_GAP_substract_loss: 6.3796 - conv7_2_GAP_substract_loss: 8.3994 - conv8_2_GAP_substract_loss: 19.0717 - conv9_2_GAP_substract_loss: 29.4505 - predictions_loss: 3.3338 - val_loss: 7.4492 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.1348\n",
      "\n",
      "Epoch 00072: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-72_loss-4.6517_val_loss-7.4492.h5\n",
      "Epoch 73/120\n",
      "\n",
      "Epoch 00073: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 621s 621ms/step - loss: 4.6462 - pool1_GAP_substract_loss: 13647.8674 - pool2_GAP_substract_loss: 10062.3986 - pool3_GAP_substract_loss: 1170.9953 - conv4_3_norm_GAP_substract_loss: 10.8003 - fc7_GAP_substract_loss: 5.8953 - conv6_2_GAP_substract_loss: 6.3315 - conv7_2_GAP_substract_loss: 8.2936 - conv8_2_GAP_substract_loss: 18.6250 - conv9_2_GAP_substract_loss: 28.2966 - predictions_loss: 3.3303 - val_loss: 7.6409 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.3286\n",
      "\n",
      "Epoch 00073: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-73_loss-4.6462_val_loss-7.6409.h5\n",
      "Epoch 74/120\n",
      "\n",
      "Epoch 00074: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 621s 621ms/step - loss: 4.6457 - pool1_GAP_substract_loss: 13735.5682 - pool2_GAP_substract_loss: 10263.5678 - pool3_GAP_substract_loss: 1177.2336 - conv4_3_norm_GAP_substract_loss: 10.8133 - fc7_GAP_substract_loss: 5.9322 - conv6_2_GAP_substract_loss: 6.3856 - conv7_2_GAP_substract_loss: 8.3273 - conv8_2_GAP_substract_loss: 18.3626 - conv9_2_GAP_substract_loss: 27.7742 - predictions_loss: 3.3318 - val_loss: 7.5765 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.2662\n",
      "\n",
      "Epoch 00074: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-74_loss-4.6457_val_loss-7.5765.h5\n",
      "Epoch 75/120\n",
      "\n",
      "Epoch 00075: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 622s 622ms/step - loss: 4.6290 - pool1_GAP_substract_loss: 13686.2585 - pool2_GAP_substract_loss: 10136.3269 - pool3_GAP_substract_loss: 1172.3289 - conv4_3_norm_GAP_substract_loss: 10.7156 - fc7_GAP_substract_loss: 5.9730 - conv6_2_GAP_substract_loss: 6.4540 - conv7_2_GAP_substract_loss: 8.4321 - conv8_2_GAP_substract_loss: 18.4747 - conv9_2_GAP_substract_loss: 27.4204 - predictions_loss: 3.3172 - val_loss: 7.5381 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.2298\n",
      "\n",
      "Epoch 00075: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-75_loss-4.6290_val_loss-7.5381.h5\n",
      "Epoch 76/120\n",
      "\n",
      "Epoch 00076: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 621s 621ms/step - loss: 4.6290 - pool1_GAP_substract_loss: 13697.2832 - pool2_GAP_substract_loss: 10147.4253 - pool3_GAP_substract_loss: 1168.9044 - conv4_3_norm_GAP_substract_loss: 10.7080 - fc7_GAP_substract_loss: 5.8489 - conv6_2_GAP_substract_loss: 6.3828 - conv7_2_GAP_substract_loss: 8.3853 - conv8_2_GAP_substract_loss: 18.5641 - conv9_2_GAP_substract_loss: 27.6114 - predictions_loss: 3.3192 - val_loss: 7.5298 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.2235\n",
      "\n",
      "Epoch 00076: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-76_loss-4.6290_val_loss-7.5298.h5\n",
      "Epoch 77/120\n",
      "\n",
      "Epoch 00077: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 621s 621ms/step - loss: 4.6323 - pool1_GAP_substract_loss: 13933.3733 - pool2_GAP_substract_loss: 10306.6489 - pool3_GAP_substract_loss: 1165.5783 - conv4_3_norm_GAP_substract_loss: 10.7368 - fc7_GAP_substract_loss: 5.9895 - conv6_2_GAP_substract_loss: 6.6124 - conv7_2_GAP_substract_loss: 8.7584 - conv8_2_GAP_substract_loss: 19.4798 - conv9_2_GAP_substract_loss: 29.1950 - predictions_loss: 3.3245 - val_loss: 7.5864 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.2822\n",
      "\n",
      "Epoch 00077: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-77_loss-4.6323_val_loss-7.5864.h5\n",
      "Epoch 78/120\n",
      "\n",
      "Epoch 00078: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 622s 622ms/step - loss: 4.6348 - pool1_GAP_substract_loss: 13809.7940 - pool2_GAP_substract_loss: 10396.8900 - pool3_GAP_substract_loss: 1189.5593 - conv4_3_norm_GAP_substract_loss: 10.6539 - fc7_GAP_substract_loss: 6.0183 - conv6_2_GAP_substract_loss: 6.5946 - conv7_2_GAP_substract_loss: 8.7218 - conv8_2_GAP_substract_loss: 19.2620 - conv9_2_GAP_substract_loss: 28.8130 - predictions_loss: 3.3290 - val_loss: 7.5809 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.2787\n",
      "\n",
      "Epoch 00078: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-78_loss-4.6348_val_loss-7.5809.h5\n",
      "Epoch 79/120\n",
      "\n",
      "Epoch 00079: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 621s 621ms/step - loss: 4.6487 - pool1_GAP_substract_loss: 13580.7339 - pool2_GAP_substract_loss: 10354.9836 - pool3_GAP_substract_loss: 1178.7037 - conv4_3_norm_GAP_substract_loss: 10.7050 - fc7_GAP_substract_loss: 5.9569 - conv6_2_GAP_substract_loss: 6.4886 - conv7_2_GAP_substract_loss: 8.5142 - conv8_2_GAP_substract_loss: 18.8205 - conv9_2_GAP_substract_loss: 28.2416 - predictions_loss: 3.3450 - val_loss: 7.4426 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.1424\n",
      "\n",
      "Epoch 00079: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-79_loss-4.6487_val_loss-7.4426.h5\n",
      "Epoch 80/120\n",
      "\n",
      "Epoch 00080: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 622s 622ms/step - loss: 4.6064 - pool1_GAP_substract_loss: 13744.6790 - pool2_GAP_substract_loss: 10524.8865 - pool3_GAP_substract_loss: 1192.0749 - conv4_3_norm_GAP_substract_loss: 10.5492 - fc7_GAP_substract_loss: 6.0310 - conv6_2_GAP_substract_loss: 6.6154 - conv7_2_GAP_substract_loss: 8.7010 - conv8_2_GAP_substract_loss: 18.9349 - conv9_2_GAP_substract_loss: 27.8604 - predictions_loss: 3.3047 - val_loss: 7.5843 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.2861\n",
      "\n",
      "Epoch 00080: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-80_loss-4.6064_val_loss-7.5843.h5\n",
      "Epoch 81/120\n",
      "\n",
      "Epoch 00081: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 622s 622ms/step - loss: 4.6230 - pool1_GAP_substract_loss: 13804.8439 - pool2_GAP_substract_loss: 10486.1633 - pool3_GAP_substract_loss: 1185.5908 - conv4_3_norm_GAP_substract_loss: 10.5657 - fc7_GAP_substract_loss: 5.9929 - conv6_2_GAP_substract_loss: 6.5563 - conv7_2_GAP_substract_loss: 8.6229 - conv8_2_GAP_substract_loss: 19.2352 - conv9_2_GAP_substract_loss: 28.6455 - predictions_loss: 3.3233 - val_loss: 7.5700 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.2738\n",
      "\n",
      "Epoch 00081: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-81_loss-4.6230_val_loss-7.5700.h5\n",
      "Epoch 82/120\n",
      "\n",
      "Epoch 00082: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 621s 621ms/step - loss: 4.6167 - pool1_GAP_substract_loss: 13727.7798 - pool2_GAP_substract_loss: 10427.4949 - pool3_GAP_substract_loss: 1168.1320 - conv4_3_norm_GAP_substract_loss: 10.4973 - fc7_GAP_substract_loss: 5.9699 - conv6_2_GAP_substract_loss: 6.5796 - conv7_2_GAP_substract_loss: 8.7450 - conv8_2_GAP_substract_loss: 19.2097 - conv9_2_GAP_substract_loss: 28.4051 - predictions_loss: 3.3190 - val_loss: 7.4490 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.1548\n",
      "\n",
      "Epoch 00082: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-82_loss-4.6167_val_loss-7.4490.h5\n",
      "Epoch 83/120\n",
      "\n",
      "Epoch 00083: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 621s 621ms/step - loss: 4.5923 - pool1_GAP_substract_loss: 13630.2092 - pool2_GAP_substract_loss: 10359.8269 - pool3_GAP_substract_loss: 1174.2697 - conv4_3_norm_GAP_substract_loss: 10.6308 - fc7_GAP_substract_loss: 6.0438 - conv6_2_GAP_substract_loss: 6.6550 - conv7_2_GAP_substract_loss: 8.8051 - conv8_2_GAP_substract_loss: 19.4225 - conv9_2_GAP_substract_loss: 28.6461 - predictions_loss: 3.2965 - val_loss: 7.5103 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.2181\n",
      "\n",
      "Epoch 00083: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-83_loss-4.5923_val_loss-7.5103.h5\n",
      "Epoch 84/120\n",
      "\n",
      "Epoch 00084: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 622s 622ms/step - loss: 4.5806 - pool1_GAP_substract_loss: 13713.0590 - pool2_GAP_substract_loss: 10612.5911 - pool3_GAP_substract_loss: 1214.4397 - conv4_3_norm_GAP_substract_loss: 10.7728 - fc7_GAP_substract_loss: 6.0762 - conv6_2_GAP_substract_loss: 6.6745 - conv7_2_GAP_substract_loss: 8.8365 - conv8_2_GAP_substract_loss: 19.5642 - conv9_2_GAP_substract_loss: 28.9176 - predictions_loss: 3.2868 - val_loss: 7.6796 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.3894\n",
      "\n",
      "Epoch 00084: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-84_loss-4.5806_val_loss-7.6796.h5\n",
      "Epoch 85/120\n",
      "\n",
      "Epoch 00085: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 622s 622ms/step - loss: 4.5887 - pool1_GAP_substract_loss: 13418.1963 - pool2_GAP_substract_loss: 10423.9568 - pool3_GAP_substract_loss: 1200.4486 - conv4_3_norm_GAP_substract_loss: 10.5007 - fc7_GAP_substract_loss: 6.0443 - conv6_2_GAP_substract_loss: 6.6488 - conv7_2_GAP_substract_loss: 8.7351 - conv8_2_GAP_substract_loss: 18.9971 - conv9_2_GAP_substract_loss: 27.6545 - predictions_loss: 3.2969 - val_loss: 7.5138 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.2256\n",
      "\n",
      "Epoch 00085: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-85_loss-4.5887_val_loss-7.5138.h5\n",
      "Epoch 86/120\n",
      "\n",
      "Epoch 00086: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 622s 622ms/step - loss: 4.5843 - pool1_GAP_substract_loss: 13628.2062 - pool2_GAP_substract_loss: 10613.4100 - pool3_GAP_substract_loss: 1208.1841 - conv4_3_norm_GAP_substract_loss: 10.7885 - fc7_GAP_substract_loss: 6.1775 - conv6_2_GAP_substract_loss: 6.7962 - conv7_2_GAP_substract_loss: 8.9601 - conv8_2_GAP_substract_loss: 19.6713 - conv9_2_GAP_substract_loss: 28.9271 - predictions_loss: 3.2945 - val_loss: 7.4902 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.2039\n",
      "\n",
      "Epoch 00086: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-86_loss-4.5843_val_loss-7.4902.h5\n",
      "Epoch 87/120\n",
      "\n",
      "Epoch 00087: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 621s 621ms/step - loss: 4.5610 - pool1_GAP_substract_loss: 13782.0050 - pool2_GAP_substract_loss: 10607.6604 - pool3_GAP_substract_loss: 1209.5482 - conv4_3_norm_GAP_substract_loss: 10.6863 - fc7_GAP_substract_loss: 6.1532 - conv6_2_GAP_substract_loss: 6.7885 - conv7_2_GAP_substract_loss: 8.8949 - conv8_2_GAP_substract_loss: 19.1067 - conv9_2_GAP_substract_loss: 27.8211 - predictions_loss: 3.2731 - val_loss: 7.5824 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.2981\n",
      "\n",
      "Epoch 00087: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-87_loss-4.5610_val_loss-7.5824.h5\n",
      "Epoch 88/120\n",
      "\n",
      "Epoch 00088: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 622s 622ms/step - loss: 4.6020 - pool1_GAP_substract_loss: 13570.2365 - pool2_GAP_substract_loss: 10569.2429 - pool3_GAP_substract_loss: 1213.9384 - conv4_3_norm_GAP_substract_loss: 10.6876 - fc7_GAP_substract_loss: 6.1312 - conv6_2_GAP_substract_loss: 6.7887 - conv7_2_GAP_substract_loss: 8.9472 - conv8_2_GAP_substract_loss: 19.2188 - conv9_2_GAP_substract_loss: 27.7278 - predictions_loss: 3.3160 - val_loss: 7.6842 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.4018\n",
      "\n",
      "Epoch 00088: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-88_loss-4.6020_val_loss-7.6842.h5\n",
      "Epoch 89/120\n",
      "\n",
      "Epoch 00089: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 623s 623ms/step - loss: 4.5653 - pool1_GAP_substract_loss: 13719.6831 - pool2_GAP_substract_loss: 10815.0460 - pool3_GAP_substract_loss: 1226.4458 - conv4_3_norm_GAP_substract_loss: 10.6944 - fc7_GAP_substract_loss: 6.1498 - conv6_2_GAP_substract_loss: 6.9070 - conv7_2_GAP_substract_loss: 9.1301 - conv8_2_GAP_substract_loss: 19.8264 - conv9_2_GAP_substract_loss: 28.8457 - predictions_loss: 3.2813 - val_loss: 7.5854 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.3050\n",
      "\n",
      "Epoch 00089: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-89_loss-4.5653_val_loss-7.5854.h5\n",
      "Epoch 90/120\n",
      "\n",
      "Epoch 00090: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 621s 621ms/step - loss: 4.5637 - pool1_GAP_substract_loss: 13799.1154 - pool2_GAP_substract_loss: 10792.4773 - pool3_GAP_substract_loss: 1225.1646 - conv4_3_norm_GAP_substract_loss: 10.7347 - fc7_GAP_substract_loss: 6.2127 - conv6_2_GAP_substract_loss: 7.0279 - conv7_2_GAP_substract_loss: 9.3395 - conv8_2_GAP_substract_loss: 20.2778 - conv9_2_GAP_substract_loss: 29.3909 - predictions_loss: 3.2817 - val_loss: 7.5382 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.2597\n",
      "\n",
      "Epoch 00090: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-90_loss-4.5637_val_loss-7.5382.h5\n",
      "Epoch 91/120\n",
      "\n",
      "Epoch 00091: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 622s 622ms/step - loss: 4.5548 - pool1_GAP_substract_loss: 13984.7990 - pool2_GAP_substract_loss: 10927.7681 - pool3_GAP_substract_loss: 1228.9144 - conv4_3_norm_GAP_substract_loss: 10.8163 - fc7_GAP_substract_loss: 6.2451 - conv6_2_GAP_substract_loss: 7.0587 - conv7_2_GAP_substract_loss: 9.3098 - conv8_2_GAP_substract_loss: 20.0170 - conv9_2_GAP_substract_loss: 28.9610 - predictions_loss: 3.2747 - val_loss: 7.5395 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.2629\n",
      "\n",
      "Epoch 00091: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-91_loss-4.5548_val_loss-7.5395.h5\n",
      "Epoch 92/120\n",
      "\n",
      "Epoch 00092: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 621s 621ms/step - loss: 4.5600 - pool1_GAP_substract_loss: 13683.3790 - pool2_GAP_substract_loss: 10940.8639 - pool3_GAP_substract_loss: 1231.2583 - conv4_3_norm_GAP_substract_loss: 10.6393 - fc7_GAP_substract_loss: 6.1478 - conv6_2_GAP_substract_loss: 6.9041 - conv7_2_GAP_substract_loss: 9.1611 - conv8_2_GAP_substract_loss: 19.8363 - conv9_2_GAP_substract_loss: 28.4735 - predictions_loss: 3.2818 - val_loss: 7.6420 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.3674\n",
      "\n",
      "Epoch 00092: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-92_loss-4.5600_val_loss-7.6420.h5\n",
      "Epoch 93/120\n",
      "\n",
      "Epoch 00093: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 622s 622ms/step - loss: 4.5606 - pool1_GAP_substract_loss: 13547.2649 - pool2_GAP_substract_loss: 10710.3567 - pool3_GAP_substract_loss: 1219.7964 - conv4_3_norm_GAP_substract_loss: 10.6256 - fc7_GAP_substract_loss: 6.1292 - conv6_2_GAP_substract_loss: 6.8678 - conv7_2_GAP_substract_loss: 9.0479 - conv8_2_GAP_substract_loss: 19.6560 - conv9_2_GAP_substract_loss: 28.2728 - predictions_loss: 3.2844 - val_loss: 7.5561 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.2833\n",
      "\n",
      "Epoch 00093: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-93_loss-4.5606_val_loss-7.5561.h5\n",
      "Epoch 94/120\n",
      "\n",
      "Epoch 00094: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 622s 622ms/step - loss: 4.5251 - pool1_GAP_substract_loss: 13702.5834 - pool2_GAP_substract_loss: 10900.0576 - pool3_GAP_substract_loss: 1230.2456 - conv4_3_norm_GAP_substract_loss: 10.6766 - fc7_GAP_substract_loss: 6.1375 - conv6_2_GAP_substract_loss: 6.9765 - conv7_2_GAP_substract_loss: 9.3802 - conv8_2_GAP_substract_loss: 20.5550 - conv9_2_GAP_substract_loss: 29.7296 - predictions_loss: 3.2507 - val_loss: 7.6486 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.3778\n",
      "\n",
      "Epoch 00094: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-94_loss-4.5251_val_loss-7.6486.h5\n",
      "Epoch 95/120\n",
      "\n",
      "Epoch 00095: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 629s 629ms/step - loss: 4.5500 - pool1_GAP_substract_loss: 13710.5478 - pool2_GAP_substract_loss: 10983.8858 - pool3_GAP_substract_loss: 1240.4421 - conv4_3_norm_GAP_substract_loss: 10.8964 - fc7_GAP_substract_loss: 6.2411 - conv6_2_GAP_substract_loss: 7.0539 - conv7_2_GAP_substract_loss: 9.4340 - conv8_2_GAP_substract_loss: 20.4796 - conv9_2_GAP_substract_loss: 29.4768 - predictions_loss: 3.2775 - val_loss: 7.7179 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.4490\n",
      "\n",
      "Epoch 00095: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-95_loss-4.5500_val_loss-7.7179.h5\n",
      "Epoch 96/120\n",
      "\n",
      "Epoch 00096: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 631s 631ms/step - loss: 4.5338 - pool1_GAP_substract_loss: 13721.7519 - pool2_GAP_substract_loss: 10896.3885 - pool3_GAP_substract_loss: 1241.1215 - conv4_3_norm_GAP_substract_loss: 10.7618 - fc7_GAP_substract_loss: 6.3312 - conv6_2_GAP_substract_loss: 7.1618 - conv7_2_GAP_substract_loss: 9.6253 - conv8_2_GAP_substract_loss: 21.4546 - conv9_2_GAP_substract_loss: 31.1315 - predictions_loss: 3.2633 - val_loss: 7.6536 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.3865\n",
      "\n",
      "Epoch 00096: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_loss_weight_0_0000001/epoch-96_loss-4.5338_val_loss-7.6536.h5\n",
      "Epoch 97/120\n",
      "\n",
      "Epoch 00097: LearningRateScheduler setting learning rate to 0.0001.\n",
      " 371/1000 [==========>...................] - ETA: 6:39 - loss: 4.5817 - pool1_GAP_substract_loss: 13573.6715 - pool2_GAP_substract_loss: 10820.9177 - pool3_GAP_substract_loss: 1232.4021 - conv4_3_norm_GAP_substract_loss: 10.5262 - fc7_GAP_substract_loss: 6.2407 - conv6_2_GAP_substract_loss: 7.0597 - conv7_2_GAP_substract_loss: 9.3452 - conv8_2_GAP_substract_loss: 19.9308 - conv9_2_GAP_substract_loss: 28.1092 - predictions_loss: 3.3125"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ea67b764e796>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                               \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset_size\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2719\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2720\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2721\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2693\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2694\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initial_epoch = 0\n",
    "final_epoch = 120\n",
    "steps_per_epoch = 1000\n",
    "\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              epochs=final_epoch,\n",
    "                              callbacks=callbacks,\n",
    "                              validation_data=val_generator,\n",
    "                              validation_steps=ceil(val_dataset_size/batch_size),\n",
    "                              initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Set the generator for the val_dataset or train_dataset predictions.\n",
    "\n",
    "predict_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[ssd_data_augmentation],\n",
    "                                         label_encoder=None,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'filenames',\n",
    "                                                  'inverse_transform',\n",
    "                                                  'original_images',\n",
    "                                                  'original_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n",
    "# 2: Generate samples.\n",
    "\n",
    "batch_images, batch_filenames, batch_inverse_transforms, batch_original_images, batch_original_labels = next(predict_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_images, batch_filenames, batch_inverse_transforms, batch_original_images, batch_original_labels = next(predict_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "print(\"Image:\", batch_filenames[i])\n",
    "colors = plt.cm.hsv(np.linspace(0, 1, n_classes+1)).tolist()\n",
    "classes = ['background',\n",
    "           'person', 'rider', 'car', 'truck',\n",
    "           'bus', 'train', 'motorcycle', 'bicycle']\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.imshow(batch_images[0][i])\n",
    "plt.show()\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.imshow(batch_images[1][i])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3  # Which batch item to look at\n",
    "\n",
    "print(\"Image:\", batch_filenames[i])\n",
    "print()\n",
    "print(\"Ground truth boxes:\\n\")\n",
    "print(np.array(batch_original_labels[i]))\n",
    "\n",
    "# 3: Make predictions.\n",
    "\n",
    "y_pred = model.predict(batch_images)[-1]\n",
    "\n",
    "# Now let's decode the raw predictions in `y_pred`.\n",
    "\n",
    "# Had we created the model in 'inference' or 'inference_fast' mode,\n",
    "# then the model's final layer would be a `DecodeDetections` layer and\n",
    "# `y_pred` would already contain the decoded predictions,\n",
    "# but since we created the model in 'training' mode,\n",
    "# the model outputs raw predictions that still need to be decoded and filtered.\n",
    "# This is what the `decode_detections()` function is for.\n",
    "# It does exactly what the `DecodeDetections` layer would do,\n",
    "# but using Numpy instead of TensorFlow (i.e. on the CPU instead of the GPU).\n",
    "\n",
    "# `decode_detections()` with default argument values follows the procedure of the original SSD implementation:\n",
    "# First, a very low confidence threshold of 0.01 is applied to filter out the majority of the predicted boxes,\n",
    "# then greedy non-maximum suppression is performed per class with an intersection-over-union threshold of 0.45,\n",
    "# and out of what is left after that, the top 200 highest confidence boxes are returned.\n",
    "# Those settings are for precision-recall scoring purposes though.\n",
    "# In order to get some usable final predictions, we'll set the confidence threshold much higher, e.g. to 0.5,\n",
    "# since we're only interested in the very confident predictions.\n",
    "\n",
    "# 4: Decode the raw predictions in `y_pred`.\n",
    "\n",
    "y_pred_decoded = decode_detections(y_pred,\n",
    "                                   confidence_thresh=0.35,\n",
    "                                   iou_threshold=0.4,\n",
    "                                   top_k=200,\n",
    "                                   normalize_coords=normalize_coords,\n",
    "                                   img_height=img_height,\n",
    "                                   img_width=img_width)\n",
    "\n",
    "# We made the predictions on the resized images,\n",
    "# but we'd like to visualize the outcome on the original input images,\n",
    "# so we'll convert the coordinates accordingly.\n",
    "# Don't worry about that opaque `apply_inverse_transforms()` function below,\n",
    "# in this simple case it just applies `(* original_image_size / resized_image_size)` to the box coordinates.\n",
    "\n",
    "# 5: Convert the predictions for the original image.\n",
    "\n",
    "y_pred_decoded_inv = apply_inverse_transforms(y_pred_decoded, batch_inverse_transforms)\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=90)\n",
    "print(\"Predicted boxes:\\n\")\n",
    "print('   class   conf xmin   ymin   xmax   ymax')\n",
    "print(y_pred_decoded_inv[i])\n",
    "\n",
    "# Finally, let's draw the predicted boxes onto the image.\n",
    "# Each predicted box says its confidence next to the category name.\n",
    "# The ground truth boxes are also drawn onto the image in green for comparison.\n",
    "\n",
    "# 5: Draw the predicted boxes onto the image\n",
    "\n",
    "# Set the colors for the bounding boxes\n",
    "colors = plt.cm.hsv(np.linspace(0, 1, n_classes+1)).tolist()\n",
    "classes = ['background',\n",
    "           'person', 'rider', 'car', 'truck',\n",
    "           'bus', 'train', 'motorcycle', 'bicycle']\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.imshow(batch_original_images[i])\n",
    "\n",
    "current_axis = plt.gca()\n",
    "\n",
    "for box in batch_original_labels[i]:\n",
    "    xmin = box[1]\n",
    "    ymin = box[2]\n",
    "    xmax = box[3]\n",
    "    ymax = box[4]\n",
    "    label = '{}'.format(classes[int(box[0])])\n",
    "    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color='green', fill=False, linewidth=2))\n",
    "    current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor': 'green', 'alpha': 1.0})\n",
    "\n",
    "for box in y_pred_decoded_inv[i]:\n",
    "    xmin = box[2]\n",
    "    ymin = box[3]\n",
    "    xmax = box[4]\n",
    "    ymax = box[5]\n",
    "    color = colors[int(box[0])]\n",
    "    label = '{}: {:.2f}'.format(classes[int(box[0])], box[1])\n",
    "    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color=color, fill=False, linewidth=2))\n",
    "    current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor': color, 'alpha': 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
