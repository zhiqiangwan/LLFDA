{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import glob\n",
    "\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TerminateOnNaN, CSVLogger, TensorBoard\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from models.keras_ssd300_Siamese import ssd_300\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
    "from keras_layers.keras_layer_L2Normalization import L2Normalization\n",
    "\n",
    "from ssd_encoder_decoder.ssd_input_encoder import SSDInputEncoder\n",
    "from ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n",
    "\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from data_generator.object_detection_2d_geometric_ops import Resize\n",
    "from data_generator.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
    "from data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation, SSDDataAugmentation_Siamese\n",
    "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
    "\n",
    "\n",
    "img_height = 300 # Height of the model input images\n",
    "img_width = 300 # Width of the model input images\n",
    "img_channels = 3 # Number of color channels of the model input images\n",
    "mean_color = [123, 117, 104] # Per-channel mean of images. Do not change if use any of the pre-trained weights.\n",
    "# The color channel order in the original SSD is BGR,\n",
    "# so we'll have the model reverse the color channel order of the input images.\n",
    "swap_channels = [2, 1, 0]\n",
    "classes = ['background',\n",
    "           'person', 'rider', 'car', 'truck',\n",
    "           'bus', 'train', 'motorcycle', 'bicycle']\n",
    "n_classes = len(classes) - 1  # Number of positive classes, 8 for domain Cityscapes, 20 for Pascal VOC, 80 for MS COCO\n",
    "# The anchor box scaling factors used in the original SSD300 for the Pascal VOC datasets\n",
    "# scales_pascal = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05]\n",
    "# The anchor box scaling factors used in the original SSD300 for the MS COCO datasets\n",
    "scales_coco = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05]\n",
    "scales = scales_coco\n",
    "aspect_ratios = [[1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5]] # The anchor box aspect ratios used in the original SSD300; the order matters\n",
    "two_boxes_for_ar1 = True\n",
    "steps = [8, 16, 32, 64, 100, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
    "# The offsets of the first anchor box center points from the top and left borders of the image\n",
    "# as a fraction of the step size for each predictor layer.\n",
    "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
    "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
    "# The variances by which the encoded target coordinates are divided as in the original implementation\n",
    "variances = [0.1, 0.1, 0.2, 0.2]\n",
    "normalize_coords = True\n",
    "Model_Build = 'New_Model'  # 'Load_Model'\n",
    "Optimizer_Type = 'SGD'  # 'Adam' #  \n",
    "batch_size = 16  # Change the batch size if you like, or if you run into GPU memory issues.\n",
    "# alpha_distance =  0.001  # Coefficient for the distance between the source and target feature maps.\n",
    "loss_weights = [0.00001, 0.00001, 0.00001] + [0.0, 0.0, 0.0, 0.0, 0.0, 0.0] + [1.0]\n",
    "Source_Only = False\n",
    "\n",
    "if len(glob.glob('*.h5')):\n",
    "    Dataset_Build = 'Load_Dataset'\n",
    "else:\n",
    "    Dataset_Build = 'New_Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Model_Build == 'New_Model':\n",
    "    # 1: Build the Keras model.\n",
    "\n",
    "    K.clear_session()  # Clear previous models from memory.\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from keras.backend.tensorflow_backend import set_session\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "    config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "    # (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "    sess = tf.Session(config=config)\n",
    "    set_session(sess)  # set this TensorFlow session as the default session for Keras\n",
    "\n",
    "    model = ssd_300(image_size=(img_height, img_width, img_channels),\n",
    "                    n_classes=n_classes,\n",
    "                    mode='training',\n",
    "                    l2_regularization=0.0005,\n",
    "                    scales=scales,\n",
    "                    aspect_ratios_per_layer=aspect_ratios,\n",
    "                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                    steps=steps,\n",
    "                    offsets=offsets,\n",
    "                    clip_boxes=clip_boxes,\n",
    "                    variances=variances,\n",
    "                    normalize_coords=normalize_coords,\n",
    "                    subtract_mean=mean_color,\n",
    "                    swap_channels=swap_channels)\n",
    "\n",
    "    # 2: Load some weights into the model.\n",
    "\n",
    "    # TODO: Set the path to the weights you want to load.\n",
    "    weights_path = '../trained_weights/VGG_ILSVRC_16_layers_fc_reduced.h5'\n",
    "\n",
    "    model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "    # 3: Instantiate an optimizer and the SSD loss function and compile the model.\n",
    "    #    If you want to follow the original Caffe implementation, use the preset SGD\n",
    "    #    optimizer, otherwise I'd recommend the commented-out Adam optimizer.\n",
    "\n",
    "    if Optimizer_Type == 'SGD':\n",
    "        Optimizer = SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=False)\n",
    "    elif Optimizer_Type == 'Adam':\n",
    "        Optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    else:\n",
    "        raise ValueError('Undefined Optimizer_Type.')\n",
    "\n",
    "    ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "    if Source_Only:\n",
    "        model.compile(optimizer=Optimizer, loss={'pool1_GAP_substract': ssd_loss.compute_distance_loss_source_only,\n",
    "                                                 'pool2_GAP_substract': ssd_loss.compute_distance_loss_source_only,\n",
    "                                                 'pool3_GAP_substract': ssd_loss.compute_distance_loss_source_only,\n",
    "                                                 'conv4_3_norm_GAP_substract': ssd_loss.compute_distance_loss_source_only,\n",
    "                                                 'fc7_GAP_substract': ssd_loss.compute_distance_loss_source_only,\n",
    "                                                 'conv6_2_GAP_substract': ssd_loss.compute_distance_loss_source_only,\n",
    "                                                 'conv7_2_GAP_substract': ssd_loss.compute_distance_loss_source_only,\n",
    "                                                 'conv8_2_GAP_substract': ssd_loss.compute_distance_loss_source_only,\n",
    "                                                 'conv9_2_GAP_substract': ssd_loss.compute_distance_loss_source_only,\n",
    "                                                 'predictions': ssd_loss.compute_loss},\n",
    "                      loss_weights={'pool1_GAP_substract': loss_weights[0],\n",
    "                                    'pool2_GAP_substract': loss_weights[1],\n",
    "                                    'pool3_GAP_substract': loss_weights[2],\n",
    "                                    'conv4_3_norm_GAP_substract': loss_weights[3],\n",
    "                                    'fc7_GAP_substract': loss_weights[4],\n",
    "                                    'conv6_2_GAP_substract': loss_weights[5],\n",
    "                                    'conv7_2_GAP_substract': loss_weights[6],\n",
    "                                    'conv8_2_GAP_substract': loss_weights[7],\n",
    "                                    'conv9_2_GAP_substract': loss_weights[8],\n",
    "                                    'predictions': loss_weights[9]})\n",
    "    else:\n",
    "        model.compile(optimizer=Optimizer, loss={'pool1_GAP_substract': ssd_loss.compute_distance_loss,\n",
    "                                                 'pool2_GAP_substract': ssd_loss.compute_distance_loss,\n",
    "                                                 'pool3_GAP_substract': ssd_loss.compute_distance_loss,\n",
    "                                                 'conv4_3_norm_GAP_substract': ssd_loss.compute_distance_loss,\n",
    "                                                 'fc7_GAP_substract': ssd_loss.compute_distance_loss,\n",
    "                                                 'conv6_2_GAP_substract': ssd_loss.compute_distance_loss,\n",
    "                                                 'conv7_2_GAP_substract': ssd_loss.compute_distance_loss,\n",
    "                                                 'conv8_2_GAP_substract': ssd_loss.compute_distance_loss,\n",
    "                                                 'conv9_2_GAP_substract': ssd_loss.compute_distance_loss,\n",
    "                                                 'predictions': ssd_loss.compute_loss},\n",
    "                      loss_weights={'pool1_GAP_substract': loss_weights[0],\n",
    "                                    'pool2_GAP_substract': loss_weights[1],\n",
    "                                    'pool3_GAP_substract': loss_weights[2],\n",
    "                                    'conv4_3_norm_GAP_substract': loss_weights[3],\n",
    "                                    'fc7_GAP_substract': loss_weights[4],\n",
    "                                    'conv6_2_GAP_substract': loss_weights[5],\n",
    "                                    'conv7_2_GAP_substract': loss_weights[6],\n",
    "                                    'conv8_2_GAP_substract': loss_weights[7],\n",
    "                                    'conv9_2_GAP_substract': loss_weights[8],\n",
    "                                    'predictions': loss_weights[9]})\n",
    "        \n",
    "        \n",
    "elif Model_Build == 'Load_Model':\n",
    "    # TODO: Set the path to the `.h5` file of the model to be loaded.\n",
    "    model_path = '../trained_weights/VGG_ssd300_Siamese_Cityscapes/epoch-23_loss-5.2110_val_loss-6.7452.h5'\n",
    "\n",
    "    # We need to create an SSDLoss object in order to pass that to the model loader.\n",
    "    ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "    K.clear_session()  # Clear previous models from memory.\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from keras.backend.tensorflow_backend import set_session\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "    config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "    # (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "    sess = tf.Session(config=config)\n",
    "    set_session(sess)  # set this TensorFlow session as the default session for Keras\n",
    "\n",
    "    model = load_model(model_path, custom_objects={'AnchorBoxes': AnchorBoxes,\n",
    "                                                   'L2Normalization': L2Normalization,\n",
    "                                                   'compute_loss': ssd_loss.compute_loss,\n",
    "                                                   'compute_distance_loss': ssd_loss.compute_distance_loss})\n",
    "else:\n",
    "    raise ValueError('Undefined Model_Build. Model_Build should be New_Model  or Load_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labels: 100%|██████████| 2966/2966 [00:00<00:00, 3398.72it/s]\n",
      "Loading source image IDs: 100%|██████████| 2966/2966 [00:00<00:00, 11602.44it/s]\n",
      "Loading target image IDs: 100%|██████████| 2966/2966 [00:00<00:00, 11305.65it/s]\n",
      "Loading evaluation-neutrality annotations: 100%|██████████| 2966/2966 [00:00<00:00, 7565.84it/s]\n",
      "Loading labels: 100%|██████████| 493/493 [00:00<00:00, 3184.16it/s]\n",
      "Loading source image IDs: 100%|██████████| 493/493 [00:00<00:00, 10929.25it/s]\n",
      "Loading target image IDs: 0it [00:00, ?it/s]\n",
      "Loading evaluation-neutrality annotations: 100%|██████████| 493/493 [00:00<00:00, 8130.93it/s]\n"
     ]
    }
   ],
   "source": [
    "if Dataset_Build == 'New_Dataset':\n",
    "    # 1: Instantiate two `DataGenerator` objects: One for training, one for validation.\n",
    "\n",
    "    # Optional: If you have enough memory, consider loading the images into memory for the reasons explained above.\n",
    "\n",
    "    train_dataset = DataGenerator(dataset='train', load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "    val_dataset = DataGenerator(dataset='val', load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "\n",
    "    # 2: Parse the image and label lists for the training and validation datasets. This can take a while.\n",
    "\n",
    "    # TODO: Set the paths to the datasets here.\n",
    "\n",
    "    # Introduction of PascalVOC: https://arleyzhang.github.io/articles/1dc20586/\n",
    "    # The directories that contain the images.\n",
    "    Cityscapes_images_dir = '../../datasets/Cityscapes/JPEGImages'\n",
    "    Cityscapes_target_images_dir = '../../datasets/CITYSCAPES_beta_0_01/JPEGImages'\n",
    "\n",
    "    # The directories that contain the annotations.\n",
    "    Cityscapes_annotation_dir = '../../datasets/Cityscapes/Annotations'\n",
    "\n",
    "    # The paths to the image sets.\n",
    "    Cityscapes_train_source_image_set_filename = '../../datasets/Cityscapes/ImageSets/Main/train_source.txt'\n",
    "    Cityscapes_train_target_image_set_filename = '../../datasets/Cityscapes/ImageSets/Main/train_target.txt'\n",
    "    Cityscapes_test_target_image_set_filename = '../../datasets/Cityscapes/ImageSets/Main/test.txt'\n",
    "\n",
    "    # images_dirs, image_set_filenames, and annotations_dirs should have the same length\n",
    "    train_dataset.parse_xml(images_dirs=[Cityscapes_images_dir],\n",
    "                            target_images_dirs=[Cityscapes_target_images_dir],\n",
    "                            image_set_filenames=[Cityscapes_train_source_image_set_filename],\n",
    "                            target_image_set_filenames=[Cityscapes_train_target_image_set_filename],\n",
    "                            annotations_dirs=[Cityscapes_annotation_dir],\n",
    "                            classes=classes,\n",
    "                            include_classes='all',\n",
    "                            exclude_truncated=False,\n",
    "                            exclude_difficult=False,\n",
    "                            ret=False)\n",
    "\n",
    "    val_dataset.parse_xml(images_dirs=[Cityscapes_target_images_dir],\n",
    "                          image_set_filenames=[Cityscapes_test_target_image_set_filename],\n",
    "                          annotations_dirs=[Cityscapes_annotation_dir],\n",
    "                          classes=classes,\n",
    "                          include_classes='all',\n",
    "                          exclude_truncated=False,\n",
    "                          exclude_difficult=True,\n",
    "                          ret=False)\n",
    "\n",
    "    # Optional: Convert the dataset into an HDF5 dataset. This will require more disk space, but will\n",
    "    # speed up the training. Doing this is not relevant in case you activated the `load_images_into_memory`\n",
    "    # option in the constructor, because in that cas the images are in memory already anyway. If you don't\n",
    "    # want to create HDF5 datasets, comment out the subsequent two function calls.\n",
    "\n",
    "    # After create these h5 files, if you have resized the input image, you need to reload these files. Otherwise,\n",
    "    # the images and the labels will not change.\n",
    "\n",
    "    resize_image_to = (300, 600)\n",
    "    train_dataset.create_hdf5_dataset(file_path='dataset_cityscapes_train.h5',\n",
    "                                      resize=resize_image_to,\n",
    "                                      variable_image_size=True,\n",
    "                                      verbose=True)\n",
    "\n",
    "    val_dataset.create_hdf5_dataset(file_path='dataset_cityscapes_test.h5',\n",
    "                                    resize=resize_image_to,\n",
    "                                    variable_image_size=True,\n",
    "                                    verbose=True)\n",
    "\n",
    "    train_dataset = DataGenerator(dataset='train',\n",
    "                                  load_images_into_memory=False,\n",
    "                                  hdf5_dataset_path='dataset_cityscapes_train.h5',\n",
    "                                  filenames=Cityscapes_train_source_image_set_filename,\n",
    "                                  target_filenames=Cityscapes_train_target_image_set_filename,\n",
    "                                  filenames_type='text',\n",
    "                                  images_dir=Cityscapes_images_dir,\n",
    "                                  target_images_dir=Cityscapes_target_images_dir)\n",
    "\n",
    "    val_dataset = DataGenerator(dataset='val',\n",
    "                                load_images_into_memory=False,\n",
    "                                hdf5_dataset_path='dataset_cityscapes_test.h5',\n",
    "                                filenames=Cityscapes_test_target_image_set_filename,\n",
    "                                filenames_type='text',\n",
    "                                images_dir=Cityscapes_target_images_dir)\n",
    "\n",
    "elif Dataset_Build == 'Load_Dataset':\n",
    "    # 1: Instantiate two `DataGenerator` objects: One for training, one for validation.\n",
    "\n",
    "    # Load dataset from the created h5 file.\n",
    "\n",
    "    # The directories that contain the images.\n",
    "    Cityscapes_images_dir = '../../datasets/Cityscapes/JPEGImages'\n",
    "    Cityscapes_target_images_dir = '../../datasets/CITYSCAPES_beta_0_01/JPEGImages'\n",
    "\n",
    "    # The paths to the image sets.\n",
    "    Cityscapes_train_source_image_set_filename = '../../datasets/Cityscapes/ImageSets/Main/train_source.txt'\n",
    "    Cityscapes_train_target_image_set_filename = '../../datasets/Cityscapes/ImageSets/Main/train_target.txt'\n",
    "    Cityscapes_test_target_image_set_filename = '../../datasets/Cityscapes/ImageSets/Main/test.txt'\n",
    "\n",
    "    train_dataset = DataGenerator(dataset='train',\n",
    "                                  load_images_into_memory=False,\n",
    "                                  hdf5_dataset_path='dataset_cityscapes_train.h5',\n",
    "                                  filenames=Cityscapes_train_source_image_set_filename,\n",
    "                                  target_filenames=Cityscapes_train_target_image_set_filename,\n",
    "                                  filenames_type='text',\n",
    "                                  images_dir=Cityscapes_images_dir,\n",
    "                                  target_images_dir=Cityscapes_target_images_dir)\n",
    "\n",
    "    val_dataset = DataGenerator(dataset='val',\n",
    "                                load_images_into_memory=False,\n",
    "                                hdf5_dataset_path='dataset_cityscapes_test.h5',\n",
    "                                filenames=Cityscapes_test_target_image_set_filename,\n",
    "                                filenames_type='text',\n",
    "                                images_dir=Cityscapes_target_images_dir)\n",
    "\n",
    "else:\n",
    "    raise ValueError('Undefined Dataset_Build. Dataset_Build should be New_Dataset or Load_Dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the training dataset:\t  5932\n",
      "Number of images in the validation dataset:\t   493\n"
     ]
    }
   ],
   "source": [
    "# 4: Set the image transformations for pre-processing and data augmentation options.\n",
    "\n",
    "# For the training generator:\n",
    "ssd_data_augmentation = SSDDataAugmentation_Siamese(img_height=img_height,\n",
    "                                                    img_width=img_width)\n",
    "\n",
    "# For the validation generator:\n",
    "convert_to_3_channels = ConvertTo3Channels()\n",
    "resize = Resize(height=img_height, width=img_width)\n",
    "\n",
    "# 5: Instantiate an encoder that can encode ground truth labels into the format needed by the SSD loss function.\n",
    "\n",
    "# The encoder constructor needs the spatial dimensions of the model's predictor layers to create the anchor boxes.\n",
    "predictor_sizes = [model.get_layer('conv4_3_norm_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('fc7_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv6_2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv7_2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv8_2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv9_2_mbox_conf').output_shape[1:3]]\n",
    "\n",
    "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
    "                                    img_width=img_width,\n",
    "                                    n_classes=n_classes,\n",
    "                                    predictor_sizes=predictor_sizes,\n",
    "                                    scales=scales,\n",
    "                                    aspect_ratios_per_layer=aspect_ratios,\n",
    "                                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                                    steps=steps,\n",
    "                                    offsets=offsets,\n",
    "                                    clip_boxes=clip_boxes,\n",
    "                                    variances=variances,\n",
    "                                    matching_type='multi',\n",
    "                                    pos_iou_threshold=0.5,\n",
    "                                    neg_iou_limit=0.5,\n",
    "                                    normalize_coords=normalize_coords)\n",
    "\n",
    "# 6: Create the generator handles that will be passed to Keras' `fit_generator()` function.\n",
    "# The input image and label are first processed by transformations. Then, the label will be further encoded by\n",
    "# ssd_input_encoder. The encoded labels are classId and offset to each anchor box.\n",
    "train_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[ssd_data_augmentation],\n",
    "                                         label_encoder=ssd_input_encoder,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'encoded_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n",
    "val_generator = val_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[convert_to_3_channels,\n",
    "                                                      resize],\n",
    "                                     label_encoder=ssd_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'encoded_labels'},\n",
    "                                     keep_images_without_gt=False)\n",
    "\n",
    "# Get the number of samples in the training and validations datasets.\n",
    "train_dataset_size = train_dataset.get_dataset_size()\n",
    "val_dataset_size = val_dataset.get_dataset_size()\n",
    "\n",
    "print(\"Number of images in the training dataset:\\t{:>6}\".format(train_dataset_size))\n",
    "print(\"Number of images in the validation dataset:\\t{:>6}\".format(val_dataset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    if epoch < 2:\n",
    "        return 0.0005\n",
    "    elif epoch < 55:\n",
    "        return 0.001\n",
    "    elif epoch < 100:\n",
    "        return 0.0001\n",
    "    else:\n",
    "        return 0.00001\n",
    "\n",
    "# def lr_schedule(epoch):\n",
    "#     if epoch < 30:\n",
    "#         return 0.001\n",
    "#     elif epoch < 60:\n",
    "#         return 0.0001\n",
    "#     else:\n",
    "#         return 0.00001\n",
    "\n",
    "# Define model callbacks.\n",
    "checkpoint_path = '../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD_0_00001'\n",
    "# checkpoint_path = '../trained_weights/current/ssd_augm_beta_0_01_source_only'\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "\n",
    "# TODO: Set the filepath under which you want to save the model.\n",
    "model_checkpoint = ModelCheckpoint(filepath=os.path.join(checkpoint_path, 'epoch-{epoch:02d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.h5'),\n",
    "                                   monitor='val_loss',\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=False,\n",
    "                                   save_weights_only=True,\n",
    "                                   mode='auto',\n",
    "                                   period=1)\n",
    "\n",
    "# model_checkpoint.best to the best validation loss from the previous training\n",
    "# model_checkpoint.best = 4.83704\n",
    "\n",
    "csv_logger = CSVLogger(filename=os.path.join(checkpoint_path, 'ssd_augm_beta_0_01_pool123_global_pool_SGD_0_00001_training_log.csv'),\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "\n",
    "learning_rate_scheduler = LearningRateScheduler(schedule=lr_schedule,\n",
    "                                                verbose=1)\n",
    "\n",
    "terminate_on_nan = TerminateOnNaN()\n",
    "\n",
    "TensorBoard_monitor = TensorBoard(log_dir=checkpoint_path)\n",
    "\n",
    "callbacks = [model_checkpoint,\n",
    "             csv_logger,\n",
    "             learning_rate_scheduler,\n",
    "             terminate_on_nan,\n",
    "             TensorBoard_monitor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.0005.\n",
      "1000/1000 [==============================] - 638s 638ms/step - loss: 10.1636 - pool1_GAP_substract_loss: 49706.2905 - pool2_GAP_substract_loss: 60638.2723 - pool3_GAP_substract_loss: 38591.1557 - conv4_3_norm_GAP_substract_loss: 4.7893 - fc7_GAP_substract_loss: 3.4692 - conv6_2_GAP_substract_loss: 2.2916 - conv7_2_GAP_substract_loss: 1.2475 - conv8_2_GAP_substract_loss: 3.0086 - conv9_2_GAP_substract_loss: 7.2614 - predictions_loss: 6.9408 - val_loss: 9.9794 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.9208\n",
      "\n",
      "Epoch 00001: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-01_loss-10.1636_val_loss-9.9794.h5\n",
      "Epoch 2/120\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.0005.\n",
      "1000/1000 [==============================] - 629s 629ms/step - loss: 8.9264 - pool1_GAP_substract_loss: 30955.2494 - pool2_GAP_substract_loss: 19625.3558 - pool3_GAP_substract_loss: 10220.1937 - conv4_3_norm_GAP_substract_loss: 4.7346 - fc7_GAP_substract_loss: 4.4940 - conv6_2_GAP_substract_loss: 3.4636 - conv7_2_GAP_substract_loss: 1.7866 - conv8_2_GAP_substract_loss: 4.1945 - conv9_2_GAP_substract_loss: 11.7275 - predictions_loss: 5.8217 - val_loss: 9.4300 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.4006\n",
      "\n",
      "Epoch 00002: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-02_loss-8.9264_val_loss-9.4300.h5\n",
      "Epoch 3/120\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 629s 629ms/step - loss: 8.6702 - pool1_GAP_substract_loss: 25139.7703 - pool2_GAP_substract_loss: 14849.8137 - pool3_GAP_substract_loss: 6779.1312 - conv4_3_norm_GAP_substract_loss: 5.0238 - fc7_GAP_substract_loss: 4.2971 - conv6_2_GAP_substract_loss: 3.3547 - conv7_2_GAP_substract_loss: 2.0362 - conv8_2_GAP_substract_loss: 4.8717 - conv9_2_GAP_substract_loss: 13.6695 - predictions_loss: 5.6217 - val_loss: 9.2942 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.3202\n",
      "\n",
      "Epoch 00003: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-03_loss-8.6702_val_loss-9.2942.h5\n",
      "Epoch 4/120\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 628s 628ms/step - loss: 8.3322 - pool1_GAP_substract_loss: 20714.7813 - pool2_GAP_substract_loss: 11241.2322 - pool3_GAP_substract_loss: 4730.1794 - conv4_3_norm_GAP_substract_loss: 5.4044 - fc7_GAP_substract_loss: 4.3451 - conv6_2_GAP_substract_loss: 3.4350 - conv7_2_GAP_substract_loss: 2.2697 - conv8_2_GAP_substract_loss: 5.6583 - conv9_2_GAP_substract_loss: 15.3842 - predictions_loss: 5.3487 - val_loss: 8.9918 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.0719\n",
      "\n",
      "Epoch 00004: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-04_loss-8.3322_val_loss-8.9918.h5\n",
      "Epoch 5/120\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 630s 630ms/step - loss: 8.0401 - pool1_GAP_substract_loss: 17157.9061 - pool2_GAP_substract_loss: 9038.7487 - pool3_GAP_substract_loss: 3636.6596 - conv4_3_norm_GAP_substract_loss: 5.7365 - fc7_GAP_substract_loss: 4.3530 - conv6_2_GAP_substract_loss: 3.5460 - conv7_2_GAP_substract_loss: 2.5118 - conv8_2_GAP_substract_loss: 6.5608 - conv9_2_GAP_substract_loss: 16.8224 - predictions_loss: 5.1170 - val_loss: 8.7602 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8932\n",
      "\n",
      "Epoch 00005: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-05_loss-8.0401_val_loss-8.7602.h5\n",
      "Epoch 6/120\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 631s 631ms/step - loss: 7.8274 - pool1_GAP_substract_loss: 15183.2925 - pool2_GAP_substract_loss: 8011.2758 - pool3_GAP_substract_loss: 3041.9287 - conv4_3_norm_GAP_substract_loss: 5.8685 - fc7_GAP_substract_loss: 4.3678 - conv6_2_GAP_substract_loss: 3.5944 - conv7_2_GAP_substract_loss: 2.6526 - conv8_2_GAP_substract_loss: 7.1476 - conv9_2_GAP_substract_loss: 17.6339 - predictions_loss: 4.9600 - val_loss: 8.4067 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.5912\n",
      "\n",
      "Epoch 00006: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-06_loss-7.8274_val_loss-8.4067.h5\n",
      "Epoch 7/120\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 629s 629ms/step - loss: 7.7101 - pool1_GAP_substract_loss: 13475.5886 - pool2_GAP_substract_loss: 7280.9528 - pool3_GAP_substract_loss: 2651.4935 - conv4_3_norm_GAP_substract_loss: 6.0201 - fc7_GAP_substract_loss: 4.3906 - conv6_2_GAP_substract_loss: 3.8157 - conv7_2_GAP_substract_loss: 3.0066 - conv8_2_GAP_substract_loss: 8.1270 - conv9_2_GAP_substract_loss: 19.1985 - predictions_loss: 4.8965 - val_loss: 8.6016 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8365\n",
      "\n",
      "Epoch 00007: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-07_loss-7.7101_val_loss-8.6016.h5\n",
      "Epoch 8/120\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 628s 628ms/step - loss: 7.5626 - pool1_GAP_substract_loss: 12229.2655 - pool2_GAP_substract_loss: 6402.1392 - pool3_GAP_substract_loss: 2281.9241 - conv4_3_norm_GAP_substract_loss: 6.1142 - fc7_GAP_substract_loss: 4.3449 - conv6_2_GAP_substract_loss: 3.6906 - conv7_2_GAP_substract_loss: 3.0526 - conv8_2_GAP_substract_loss: 8.4837 - conv9_2_GAP_substract_loss: 20.2134 - predictions_loss: 4.8012 - val_loss: 8.2777 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.5616\n",
      "\n",
      "Epoch 00008: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-08_loss-7.5626_val_loss-8.2777.h5\n",
      "Epoch 9/120\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 628s 628ms/step - loss: 7.4517 - pool1_GAP_substract_loss: 11115.4534 - pool2_GAP_substract_loss: 5824.7302 - pool3_GAP_substract_loss: 1992.9293 - conv4_3_norm_GAP_substract_loss: 6.4629 - fc7_GAP_substract_loss: 4.4101 - conv6_2_GAP_substract_loss: 3.7922 - conv7_2_GAP_substract_loss: 3.2360 - conv8_2_GAP_substract_loss: 9.2098 - conv9_2_GAP_substract_loss: 21.7968 - predictions_loss: 4.7408 - val_loss: 8.3014 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6331\n",
      "\n",
      "Epoch 00009: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-09_loss-7.4517_val_loss-8.3014.h5\n",
      "Epoch 10/120\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 629s 629ms/step - loss: 7.3297 - pool1_GAP_substract_loss: 10066.8788 - pool2_GAP_substract_loss: 5347.1672 - pool3_GAP_substract_loss: 1784.7491 - conv4_3_norm_GAP_substract_loss: 6.5327 - fc7_GAP_substract_loss: 4.4565 - conv6_2_GAP_substract_loss: 3.7871 - conv7_2_GAP_substract_loss: 3.2851 - conv8_2_GAP_substract_loss: 9.2386 - conv9_2_GAP_substract_loss: 21.7438 - predictions_loss: 4.6677 - val_loss: 8.3072 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6858\n",
      "\n",
      "Epoch 00010: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-10_loss-7.3297_val_loss-8.3072.h5\n",
      "Epoch 11/120\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 628s 628ms/step - loss: 7.2145 - pool1_GAP_substract_loss: 9581.3498 - pool2_GAP_substract_loss: 5015.7616 - pool3_GAP_substract_loss: 1600.5878 - conv4_3_norm_GAP_substract_loss: 6.7870 - fc7_GAP_substract_loss: 4.3528 - conv6_2_GAP_substract_loss: 3.7299 - conv7_2_GAP_substract_loss: 3.3839 - conv8_2_GAP_substract_loss: 9.7267 - conv9_2_GAP_substract_loss: 23.0633 - predictions_loss: 4.5998 - val_loss: 7.9722 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.3964\n",
      "\n",
      "Epoch 00011: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-11_loss-7.2145_val_loss-7.9722.h5\n",
      "Epoch 12/120\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 629s 629ms/step - loss: 7.0932 - pool1_GAP_substract_loss: 8850.4472 - pool2_GAP_substract_loss: 4636.2074 - pool3_GAP_substract_loss: 1450.6089 - conv4_3_norm_GAP_substract_loss: 6.7132 - fc7_GAP_substract_loss: 4.4372 - conv6_2_GAP_substract_loss: 3.7804 - conv7_2_GAP_substract_loss: 3.4463 - conv8_2_GAP_substract_loss: 9.6833 - conv9_2_GAP_substract_loss: 22.5978 - predictions_loss: 4.5248 - val_loss: 8.1347 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6035\n",
      "\n",
      "Epoch 00012: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-12_loss-7.0932_val_loss-8.1347.h5\n",
      "Epoch 13/120\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 627s 627ms/step - loss: 7.0397 - pool1_GAP_substract_loss: 8504.4618 - pool2_GAP_substract_loss: 4458.6483 - pool3_GAP_substract_loss: 1295.1406 - conv4_3_norm_GAP_substract_loss: 7.0663 - fc7_GAP_substract_loss: 4.4532 - conv6_2_GAP_substract_loss: 3.8404 - conv7_2_GAP_substract_loss: 3.6985 - conv8_2_GAP_substract_loss: 10.4000 - conv9_2_GAP_substract_loss: 24.0631 - predictions_loss: 4.5161 - val_loss: 8.0073 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.5196\n",
      "\n",
      "Epoch 00013: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-13_loss-7.0397_val_loss-8.0073.h5\n",
      "Epoch 14/120\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 629s 629ms/step - loss: 6.9454 - pool1_GAP_substract_loss: 7751.2916 - pool2_GAP_substract_loss: 4134.4776 - pool3_GAP_substract_loss: 1213.9325 - conv4_3_norm_GAP_substract_loss: 7.1011 - fc7_GAP_substract_loss: 4.4473 - conv6_2_GAP_substract_loss: 3.8445 - conv7_2_GAP_substract_loss: 3.7286 - conv8_2_GAP_substract_loss: 10.1902 - conv9_2_GAP_substract_loss: 23.3473 - predictions_loss: 4.4659 - val_loss: 7.9434 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.4981\n",
      "\n",
      "Epoch 00014: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-14_loss-6.9454_val_loss-7.9434.h5\n",
      "Epoch 15/120\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 629s 629ms/step - loss: 6.8836 - pool1_GAP_substract_loss: 7480.8877 - pool2_GAP_substract_loss: 3924.9746 - pool3_GAP_substract_loss: 1085.5750 - conv4_3_norm_GAP_substract_loss: 7.2778 - fc7_GAP_substract_loss: 4.3340 - conv6_2_GAP_substract_loss: 3.8345 - conv7_2_GAP_substract_loss: 3.8493 - conv8_2_GAP_substract_loss: 10.6220 - conv9_2_GAP_substract_loss: 23.4736 - predictions_loss: 4.4466 - val_loss: 7.8824 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.4787\n",
      "\n",
      "Epoch 00015: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-15_loss-6.8836_val_loss-7.8824.h5\n",
      "Epoch 16/120\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 630s 630ms/step - loss: 6.7902 - pool1_GAP_substract_loss: 7211.1432 - pool2_GAP_substract_loss: 3762.9615 - pool3_GAP_substract_loss: 1011.6197 - conv4_3_norm_GAP_substract_loss: 7.4014 - fc7_GAP_substract_loss: 4.4336 - conv6_2_GAP_substract_loss: 3.8681 - conv7_2_GAP_substract_loss: 3.7677 - conv8_2_GAP_substract_loss: 10.4824 - conv9_2_GAP_substract_loss: 23.5521 - predictions_loss: 4.3948 - val_loss: 7.8476 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.4845\n",
      "\n",
      "Epoch 00016: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-16_loss-6.7902_val_loss-7.8476.h5\n",
      "Epoch 17/120\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 628s 628ms/step - loss: 6.6849 - pool1_GAP_substract_loss: 6870.6941 - pool2_GAP_substract_loss: 3753.5810 - pool3_GAP_substract_loss: 937.9980 - conv4_3_norm_GAP_substract_loss: 7.4765 - fc7_GAP_substract_loss: 4.3879 - conv6_2_GAP_substract_loss: 3.8999 - conv7_2_GAP_substract_loss: 3.9726 - conv8_2_GAP_substract_loss: 11.1262 - conv9_2_GAP_substract_loss: 24.0060 - predictions_loss: 4.3300 - val_loss: 7.8217 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.4980\n",
      "\n",
      "Epoch 00017: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-17_loss-6.6849_val_loss-7.8217.h5\n",
      "Epoch 18/120\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 630s 630ms/step - loss: 6.6498 - pool1_GAP_substract_loss: 6733.8934 - pool2_GAP_substract_loss: 3638.0798 - pool3_GAP_substract_loss: 891.8692 - conv4_3_norm_GAP_substract_loss: 7.6222 - fc7_GAP_substract_loss: 4.3173 - conv6_2_GAP_substract_loss: 3.9311 - conv7_2_GAP_substract_loss: 4.1243 - conv8_2_GAP_substract_loss: 11.4693 - conv9_2_GAP_substract_loss: 24.4330 - predictions_loss: 4.3341 - val_loss: 7.8000 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.5149\n",
      "\n",
      "Epoch 00018: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-18_loss-6.6498_val_loss-7.8000.h5\n",
      "Epoch 19/120\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 627s 627ms/step - loss: 6.5471 - pool1_GAP_substract_loss: 6335.9450 - pool2_GAP_substract_loss: 3428.4007 - pool3_GAP_substract_loss: 823.3490 - conv4_3_norm_GAP_substract_loss: 7.6922 - fc7_GAP_substract_loss: 4.3742 - conv6_2_GAP_substract_loss: 3.9247 - conv7_2_GAP_substract_loss: 4.0854 - conv8_2_GAP_substract_loss: 11.4863 - conv9_2_GAP_substract_loss: 24.6744 - predictions_loss: 4.2703 - val_loss: 7.6573 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.4100\n",
      "\n",
      "Epoch 00019: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-19_loss-6.5471_val_loss-7.6573.h5\n",
      "Epoch 20/120\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 629s 629ms/step - loss: 6.5323 - pool1_GAP_substract_loss: 6165.4039 - pool2_GAP_substract_loss: 3307.9652 - pool3_GAP_substract_loss: 788.8989 - conv4_3_norm_GAP_substract_loss: 8.0635 - fc7_GAP_substract_loss: 4.3351 - conv6_2_GAP_substract_loss: 3.8678 - conv7_2_GAP_substract_loss: 4.0460 - conv8_2_GAP_substract_loss: 11.2939 - conv9_2_GAP_substract_loss: 23.6612 - predictions_loss: 4.2931 - val_loss: 7.7121 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.5015\n",
      "\n",
      "Epoch 00020: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-20_loss-6.5323_val_loss-7.7121.h5\n",
      "Epoch 21/120\n",
      "\n",
      "Epoch 00021: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 649s 649ms/step - loss: 6.4342 - pool1_GAP_substract_loss: 5999.3420 - pool2_GAP_substract_loss: 3201.9189 - pool3_GAP_substract_loss: 745.8258 - conv4_3_norm_GAP_substract_loss: 8.0584 - fc7_GAP_substract_loss: 4.2402 - conv6_2_GAP_substract_loss: 3.7899 - conv7_2_GAP_substract_loss: 3.9905 - conv8_2_GAP_substract_loss: 11.1398 - conv9_2_GAP_substract_loss: 22.8710 - predictions_loss: 4.2316 - val_loss: 7.7319 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.5572\n",
      "\n",
      "Epoch 00021: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-21_loss-6.4342_val_loss-7.7319.h5\n",
      "Epoch 22/120\n",
      "\n",
      "Epoch 00022: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 627s 627ms/step - loss: 6.3671 - pool1_GAP_substract_loss: 5917.9906 - pool2_GAP_substract_loss: 3253.5500 - pool3_GAP_substract_loss: 713.1929 - conv4_3_norm_GAP_substract_loss: 8.1985 - fc7_GAP_substract_loss: 4.2970 - conv6_2_GAP_substract_loss: 3.8605 - conv7_2_GAP_substract_loss: 4.0367 - conv8_2_GAP_substract_loss: 11.2424 - conv9_2_GAP_substract_loss: 22.9674 - predictions_loss: 4.2004 - val_loss: 7.7387 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.5994\n",
      "\n",
      "Epoch 00022: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-22_loss-6.3671_val_loss-7.7387.h5\n",
      "Epoch 23/120\n",
      "\n",
      "Epoch 00023: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 637s 637ms/step - loss: 6.3320 - pool1_GAP_substract_loss: 5557.8856 - pool2_GAP_substract_loss: 3053.3962 - pool3_GAP_substract_loss: 656.2332 - conv4_3_norm_GAP_substract_loss: 8.3460 - fc7_GAP_substract_loss: 4.3205 - conv6_2_GAP_substract_loss: 3.9820 - conv7_2_GAP_substract_loss: 4.2593 - conv8_2_GAP_substract_loss: 11.8045 - conv9_2_GAP_substract_loss: 23.4893 - predictions_loss: 4.2005 - val_loss: 7.6021 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.4970\n",
      "\n",
      "Epoch 00023: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-23_loss-6.3320_val_loss-7.6021.h5\n",
      "Epoch 24/120\n",
      "\n",
      "Epoch 00024: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 656s 656ms/step - loss: 6.2568 - pool1_GAP_substract_loss: 5490.8091 - pool2_GAP_substract_loss: 3036.6978 - pool3_GAP_substract_loss: 628.9671 - conv4_3_norm_GAP_substract_loss: 8.5345 - fc7_GAP_substract_loss: 4.4010 - conv6_2_GAP_substract_loss: 4.0774 - conv7_2_GAP_substract_loss: 4.4732 - conv8_2_GAP_substract_loss: 12.1677 - conv9_2_GAP_substract_loss: 23.7098 - predictions_loss: 4.1595 - val_loss: 7.6650 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.5935\n",
      "\n",
      "Epoch 00024: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-24_loss-6.2568_val_loss-7.6650.h5\n",
      "Epoch 25/120\n",
      "\n",
      "Epoch 00025: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 646s 646ms/step - loss: 6.2118 - pool1_GAP_substract_loss: 5314.6529 - pool2_GAP_substract_loss: 2934.4532 - pool3_GAP_substract_loss: 589.8015 - conv4_3_norm_GAP_substract_loss: 8.4523 - fc7_GAP_substract_loss: 4.3129 - conv6_2_GAP_substract_loss: 3.9554 - conv7_2_GAP_substract_loss: 4.3581 - conv8_2_GAP_substract_loss: 11.8803 - conv9_2_GAP_substract_loss: 22.8760 - predictions_loss: 4.1478 - val_loss: 7.5680 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.5291\n",
      "\n",
      "Epoch 00025: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-25_loss-6.2118_val_loss-7.5680.h5\n",
      "Epoch 26/120\n",
      "\n",
      "Epoch 00026: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 634s 634ms/step - loss: 6.1341 - pool1_GAP_substract_loss: 5278.7001 - pool2_GAP_substract_loss: 2809.9488 - pool3_GAP_substract_loss: 543.9588 - conv4_3_norm_GAP_substract_loss: 8.5516 - fc7_GAP_substract_loss: 4.2123 - conv6_2_GAP_substract_loss: 3.8453 - conv7_2_GAP_substract_loss: 4.2786 - conv8_2_GAP_substract_loss: 11.6647 - conv9_2_GAP_substract_loss: 22.5446 - predictions_loss: 4.1026 - val_loss: 7.5147 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.5077\n",
      "\n",
      "Epoch 00026: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-26_loss-6.1341_val_loss-7.5147.h5\n",
      "Epoch 27/120\n",
      "\n",
      "Epoch 00027: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 627s 627ms/step - loss: 6.0670 - pool1_GAP_substract_loss: 5181.6129 - pool2_GAP_substract_loss: 2757.9044 - pool3_GAP_substract_loss: 519.4225 - conv4_3_norm_GAP_substract_loss: 8.7198 - fc7_GAP_substract_loss: 4.3097 - conv6_2_GAP_substract_loss: 4.0088 - conv7_2_GAP_substract_loss: 4.5671 - conv8_2_GAP_substract_loss: 12.3883 - conv9_2_GAP_substract_loss: 23.1595 - predictions_loss: 4.0672 - val_loss: 7.4176 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.4418\n",
      "\n",
      "Epoch 00027: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-27_loss-6.0670_val_loss-7.4176.h5\n",
      "Epoch 28/120\n",
      "\n",
      "Epoch 00028: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 639s 639ms/step - loss: 6.0321 - pool1_GAP_substract_loss: 5064.2059 - pool2_GAP_substract_loss: 2778.8838 - pool3_GAP_substract_loss: 521.8800 - conv4_3_norm_GAP_substract_loss: 8.8485 - fc7_GAP_substract_loss: 4.2770 - conv6_2_GAP_substract_loss: 3.9288 - conv7_2_GAP_substract_loss: 4.5136 - conv8_2_GAP_substract_loss: 12.3330 - conv9_2_GAP_substract_loss: 22.8500 - predictions_loss: 4.0633 - val_loss: 7.6282 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6829\n",
      "\n",
      "Epoch 00028: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-28_loss-6.0321_val_loss-7.6282.h5\n",
      "Epoch 29/120\n",
      "\n",
      "Epoch 00029: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 669s 669ms/step - loss: 5.9763 - pool1_GAP_substract_loss: 5095.7054 - pool2_GAP_substract_loss: 2767.7318 - pool3_GAP_substract_loss: 500.1387 - conv4_3_norm_GAP_substract_loss: 9.0072 - fc7_GAP_substract_loss: 4.3162 - conv6_2_GAP_substract_loss: 3.9867 - conv7_2_GAP_substract_loss: 4.6886 - conv8_2_GAP_substract_loss: 13.0419 - conv9_2_GAP_substract_loss: 24.1397 - predictions_loss: 4.0375 - val_loss: 7.5219 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6063\n",
      "\n",
      "Epoch 00029: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-29_loss-5.9763_val_loss-7.5219.h5\n",
      "Epoch 30/120\n",
      "\n",
      "Epoch 00030: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 628s 628ms/step - loss: 5.9413 - pool1_GAP_substract_loss: 5020.6354 - pool2_GAP_substract_loss: 2712.7256 - pool3_GAP_substract_loss: 483.1194 - conv4_3_norm_GAP_substract_loss: 9.2555 - fc7_GAP_substract_loss: 4.2929 - conv6_2_GAP_substract_loss: 3.9654 - conv7_2_GAP_substract_loss: 4.6947 - conv8_2_GAP_substract_loss: 13.0606 - conv9_2_GAP_substract_loss: 24.2332 - predictions_loss: 4.0321 - val_loss: 7.4425 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.5560\n",
      "\n",
      "Epoch 00030: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-30_loss-5.9413_val_loss-7.4425.h5\n",
      "Epoch 31/120\n",
      "\n",
      "Epoch 00031: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 627s 627ms/step - loss: 5.9000 - pool1_GAP_substract_loss: 4889.3912 - pool2_GAP_substract_loss: 2634.1931 - pool3_GAP_substract_loss: 458.9194 - conv4_3_norm_GAP_substract_loss: 9.0777 - fc7_GAP_substract_loss: 4.2506 - conv6_2_GAP_substract_loss: 3.9860 - conv7_2_GAP_substract_loss: 4.6823 - conv8_2_GAP_substract_loss: 12.6539 - conv9_2_GAP_substract_loss: 22.7136 - predictions_loss: 4.0197 - val_loss: 7.5380 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6799\n",
      "\n",
      "Epoch 00031: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-31_loss-5.9000_val_loss-7.5380.h5\n",
      "Epoch 32/120\n",
      "\n",
      "Epoch 00032: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 626s 626ms/step - loss: 5.8574 - pool1_GAP_substract_loss: 4731.1539 - pool2_GAP_substract_loss: 2559.7207 - pool3_GAP_substract_loss: 427.4225 - conv4_3_norm_GAP_substract_loss: 9.0815 - fc7_GAP_substract_loss: 4.1130 - conv6_2_GAP_substract_loss: 3.8662 - conv7_2_GAP_substract_loss: 4.4870 - conv8_2_GAP_substract_loss: 11.8452 - conv9_2_GAP_substract_loss: 20.8812 - predictions_loss: 4.0055 - val_loss: 7.5026 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6722\n",
      "\n",
      "Epoch 00032: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-32_loss-5.8574_val_loss-7.5026.h5\n",
      "Epoch 33/120\n",
      "\n",
      "Epoch 00033: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 628s 628ms/step - loss: 5.7873 - pool1_GAP_substract_loss: 4684.4011 - pool2_GAP_substract_loss: 2535.2368 - pool3_GAP_substract_loss: 428.9487 - conv4_3_norm_GAP_substract_loss: 9.2180 - fc7_GAP_substract_loss: 4.2733 - conv6_2_GAP_substract_loss: 4.0772 - conv7_2_GAP_substract_loss: 4.8431 - conv8_2_GAP_substract_loss: 12.7752 - conv9_2_GAP_substract_loss: 22.3932 - predictions_loss: 3.9629 - val_loss: 7.4284 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6252\n",
      "\n",
      "Epoch 00033: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-33_loss-5.7873_val_loss-7.4284.h5\n",
      "Epoch 34/120\n",
      "\n",
      "Epoch 00034: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 627s 627ms/step - loss: 5.7529 - pool1_GAP_substract_loss: 4685.9918 - pool2_GAP_substract_loss: 2418.2712 - pool3_GAP_substract_loss: 409.3636 - conv4_3_norm_GAP_substract_loss: 9.2596 - fc7_GAP_substract_loss: 4.1643 - conv6_2_GAP_substract_loss: 4.0280 - conv7_2_GAP_substract_loss: 4.8654 - conv8_2_GAP_substract_loss: 12.9346 - conv9_2_GAP_substract_loss: 22.6281 - predictions_loss: 3.9554 - val_loss: 7.4145 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6377\n",
      "\n",
      "Epoch 00034: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-34_loss-5.7529_val_loss-7.4145.h5\n",
      "Epoch 35/120\n",
      "\n",
      "Epoch 00035: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 626s 626ms/step - loss: 5.7209 - pool1_GAP_substract_loss: 4749.3615 - pool2_GAP_substract_loss: 2483.9837 - pool3_GAP_substract_loss: 402.4457 - conv4_3_norm_GAP_substract_loss: 9.1994 - fc7_GAP_substract_loss: 4.2483 - conv6_2_GAP_substract_loss: 4.0521 - conv7_2_GAP_substract_loss: 4.9172 - conv8_2_GAP_substract_loss: 12.8951 - conv9_2_GAP_substract_loss: 21.8933 - predictions_loss: 3.9493 - val_loss: 7.2991 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.5479\n",
      "\n",
      "Epoch 00035: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-35_loss-5.7209_val_loss-7.2991.h5\n",
      "Epoch 36/120\n",
      "\n",
      "Epoch 00036: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 630s 630ms/step - loss: 5.6184 - pool1_GAP_substract_loss: 4573.0006 - pool2_GAP_substract_loss: 2464.3006 - pool3_GAP_substract_loss: 374.8342 - conv4_3_norm_GAP_substract_loss: 9.4423 - fc7_GAP_substract_loss: 4.2378 - conv6_2_GAP_substract_loss: 4.0932 - conv7_2_GAP_substract_loss: 4.9719 - conv8_2_GAP_substract_loss: 12.9375 - conv9_2_GAP_substract_loss: 21.7800 - predictions_loss: 3.8725 - val_loss: 7.4811 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7552\n",
      "\n",
      "Epoch 00036: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-36_loss-5.6184_val_loss-7.4811.h5\n",
      "Epoch 37/120\n",
      "\n",
      "Epoch 00037: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 626s 626ms/step - loss: 5.6301 - pool1_GAP_substract_loss: 4553.3382 - pool2_GAP_substract_loss: 2373.6933 - pool3_GAP_substract_loss: 363.7650 - conv4_3_norm_GAP_substract_loss: 9.5089 - fc7_GAP_substract_loss: 4.1712 - conv6_2_GAP_substract_loss: 4.0400 - conv7_2_GAP_substract_loss: 4.9009 - conv8_2_GAP_substract_loss: 12.8629 - conv9_2_GAP_substract_loss: 21.6404 - predictions_loss: 3.9092 - val_loss: 7.4360 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7346\n",
      "\n",
      "Epoch 00037: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-37_loss-5.6301_val_loss-7.4360.h5\n",
      "Epoch 38/120\n",
      "\n",
      "Epoch 00038: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 5.5750 - pool1_GAP_substract_loss: 4457.4367 - pool2_GAP_substract_loss: 2365.6433 - pool3_GAP_substract_loss: 356.2635 - conv4_3_norm_GAP_substract_loss: 9.2151 - fc7_GAP_substract_loss: 4.1264 - conv6_2_GAP_substract_loss: 4.0423 - conv7_2_GAP_substract_loss: 5.0117 - conv8_2_GAP_substract_loss: 13.0808 - conv9_2_GAP_substract_loss: 21.4801 - predictions_loss: 3.8786 - val_loss: 7.3670 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6899\n",
      "\n",
      "Epoch 00038: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-38_loss-5.5750_val_loss-7.3670.h5\n",
      "Epoch 39/120\n",
      "\n",
      "Epoch 00039: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 643s 643ms/step - loss: 5.5690 - pool1_GAP_substract_loss: 4457.1260 - pool2_GAP_substract_loss: 2322.1239 - pool3_GAP_substract_loss: 348.7503 - conv4_3_norm_GAP_substract_loss: 9.3529 - fc7_GAP_substract_loss: 4.1368 - conv6_2_GAP_substract_loss: 4.0380 - conv7_2_GAP_substract_loss: 4.9772 - conv8_2_GAP_substract_loss: 12.9972 - conv9_2_GAP_substract_loss: 21.5383 - predictions_loss: 3.8966 - val_loss: 7.4560 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8023\n",
      "\n",
      "Epoch 00039: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-39_loss-5.5690_val_loss-7.4560.h5\n",
      "Epoch 40/120\n",
      "\n",
      "Epoch 00040: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 5.5209 - pool1_GAP_substract_loss: 4308.2820 - pool2_GAP_substract_loss: 2323.6735 - pool3_GAP_substract_loss: 335.0060 - conv4_3_norm_GAP_substract_loss: 9.2456 - fc7_GAP_substract_loss: 4.0292 - conv6_2_GAP_substract_loss: 3.9988 - conv7_2_GAP_substract_loss: 4.9041 - conv8_2_GAP_substract_loss: 12.8416 - conv9_2_GAP_substract_loss: 20.7608 - predictions_loss: 3.8717 - val_loss: 7.3118 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6812\n",
      "\n",
      "Epoch 00040: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-40_loss-5.5209_val_loss-7.3118.h5\n",
      "Epoch 41/120\n",
      "\n",
      "Epoch 00041: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 626s 626ms/step - loss: 5.4688 - pool1_GAP_substract_loss: 4301.3058 - pool2_GAP_substract_loss: 2314.6643 - pool3_GAP_substract_loss: 327.6966 - conv4_3_norm_GAP_substract_loss: 9.5408 - fc7_GAP_substract_loss: 4.2730 - conv6_2_GAP_substract_loss: 4.2827 - conv7_2_GAP_substract_loss: 5.3254 - conv8_2_GAP_substract_loss: 13.9268 - conv9_2_GAP_substract_loss: 22.4618 - predictions_loss: 3.8424 - val_loss: 7.2865 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6782\n",
      "\n",
      "Epoch 00041: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-41_loss-5.4688_val_loss-7.2865.h5\n",
      "Epoch 42/120\n",
      "\n",
      "Epoch 00042: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 5.4390 - pool1_GAP_substract_loss: 4194.8006 - pool2_GAP_substract_loss: 2268.3039 - pool3_GAP_substract_loss: 320.1678 - conv4_3_norm_GAP_substract_loss: 9.6304 - fc7_GAP_substract_loss: 4.2111 - conv6_2_GAP_substract_loss: 4.1251 - conv7_2_GAP_substract_loss: 5.1128 - conv8_2_GAP_substract_loss: 13.2970 - conv9_2_GAP_substract_loss: 21.3382 - predictions_loss: 3.8348 - val_loss: 7.3925 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8060\n",
      "\n",
      "Epoch 00042: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-42_loss-5.4390_val_loss-7.3925.h5\n",
      "Epoch 43/120\n",
      "\n",
      "Epoch 00043: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 625s 625ms/step - loss: 5.3867 - pool1_GAP_substract_loss: 4183.3845 - pool2_GAP_substract_loss: 2190.1360 - pool3_GAP_substract_loss: 308.9186 - conv4_3_norm_GAP_substract_loss: 9.7310 - fc7_GAP_substract_loss: 4.0746 - conv6_2_GAP_substract_loss: 3.9487 - conv7_2_GAP_substract_loss: 4.9609 - conv8_2_GAP_substract_loss: 13.0257 - conv9_2_GAP_substract_loss: 20.8572 - predictions_loss: 3.8043 - val_loss: 7.2591 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6941\n",
      "\n",
      "Epoch 00043: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-43_loss-5.3867_val_loss-7.2591.h5\n",
      "Epoch 44/120\n",
      "\n",
      "Epoch 00044: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 637s 637ms/step - loss: 5.3587 - pool1_GAP_substract_loss: 4133.9264 - pool2_GAP_substract_loss: 2201.1416 - pool3_GAP_substract_loss: 303.9940 - conv4_3_norm_GAP_substract_loss: 9.6739 - fc7_GAP_substract_loss: 4.0831 - conv6_2_GAP_substract_loss: 4.0429 - conv7_2_GAP_substract_loss: 5.0907 - conv8_2_GAP_substract_loss: 13.0439 - conv9_2_GAP_substract_loss: 20.0953 - predictions_loss: 3.7975 - val_loss: 7.3562 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8121\n",
      "\n",
      "Epoch 00044: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-44_loss-5.3587_val_loss-7.3562.h5\n",
      "Epoch 45/120\n",
      "\n",
      "Epoch 00045: LearningRateScheduler setting learning rate to 0.001.\n",
      "1000/1000 [==============================] - 645s 645ms/step - loss: 5.2789 - pool1_GAP_substract_loss: 4143.6580 - pool2_GAP_substract_loss: 2152.4489 - pool3_GAP_substract_loss: 292.5267 - conv4_3_norm_GAP_substract_loss: 9.6561 - fc7_GAP_substract_loss: 4.0880 - conv6_2_GAP_substract_loss: 4.0022 - conv7_2_GAP_substract_loss: 5.1104 - conv8_2_GAP_substract_loss: 13.2999 - conv9_2_GAP_substract_loss: 20.4226 - predictions_loss: 3.7384 - val_loss: 7.2111 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6872\n",
      "\n",
      "Epoch 00045: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-45_loss-5.2789_val_loss-7.2111.h5\n",
      "Epoch 46/120\n",
      "\n",
      "Epoch 00046: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 627s 627ms/step - loss: 5.1391 - pool1_GAP_substract_loss: 4213.7074 - pool2_GAP_substract_loss: 2204.3076 - pool3_GAP_substract_loss: 300.1831 - conv4_3_norm_GAP_substract_loss: 9.8958 - fc7_GAP_substract_loss: 4.2909 - conv6_2_GAP_substract_loss: 4.3998 - conv7_2_GAP_substract_loss: 5.7388 - conv8_2_GAP_substract_loss: 14.8574 - conv9_2_GAP_substract_loss: 22.5339 - predictions_loss: 3.6099 - val_loss: 7.1207 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.5995\n",
      "\n",
      "Epoch 00046: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-46_loss-5.1391_val_loss-7.1207.h5\n",
      "Epoch 47/120\n",
      "\n",
      "Epoch 00047: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 5.1164 - pool1_GAP_substract_loss: 4116.3033 - pool2_GAP_substract_loss: 2209.1554 - pool3_GAP_substract_loss: 309.1632 - conv4_3_norm_GAP_substract_loss: 9.6690 - fc7_GAP_substract_loss: 4.4023 - conv6_2_GAP_substract_loss: 4.6166 - conv7_2_GAP_substract_loss: 6.0171 - conv8_2_GAP_substract_loss: 15.4053 - conv9_2_GAP_substract_loss: 23.0605 - predictions_loss: 3.5898 - val_loss: 7.1210 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6023\n",
      "\n",
      "Epoch 00047: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-47_loss-5.1164_val_loss-7.1210.h5\n",
      "Epoch 48/120\n",
      "\n",
      "Epoch 00048: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 5.0900 - pool1_GAP_substract_loss: 4072.9469 - pool2_GAP_substract_loss: 2251.0849 - pool3_GAP_substract_loss: 319.6506 - conv4_3_norm_GAP_substract_loss: 9.5372 - fc7_GAP_substract_loss: 4.5196 - conv6_2_GAP_substract_loss: 4.7519 - conv7_2_GAP_substract_loss: 6.2003 - conv8_2_GAP_substract_loss: 15.6161 - conv9_2_GAP_substract_loss: 23.1850 - predictions_loss: 3.5659 - val_loss: 7.1433 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6272\n",
      "\n",
      "Epoch 00048: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-48_loss-5.0900_val_loss-7.1433.h5\n",
      "Epoch 49/120\n",
      "\n",
      "Epoch 00049: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 5.0537 - pool1_GAP_substract_loss: 4046.0753 - pool2_GAP_substract_loss: 2269.3476 - pool3_GAP_substract_loss: 327.0208 - conv4_3_norm_GAP_substract_loss: 9.5869 - fc7_GAP_substract_loss: 4.5401 - conv6_2_GAP_substract_loss: 4.8457 - conv7_2_GAP_substract_loss: 6.3605 - conv8_2_GAP_substract_loss: 16.2229 - conv9_2_GAP_substract_loss: 24.2561 - predictions_loss: 3.5322 - val_loss: 7.0539 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.5403\n",
      "\n",
      "Epoch 00049: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-49_loss-5.0537_val_loss-7.0539.h5\n",
      "Epoch 50/120\n",
      "\n",
      "Epoch 00050: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 630s 630ms/step - loss: 5.0450 - pool1_GAP_substract_loss: 4040.9213 - pool2_GAP_substract_loss: 2325.7543 - pool3_GAP_substract_loss: 336.3387 - conv4_3_norm_GAP_substract_loss: 9.6778 - fc7_GAP_substract_loss: 4.6815 - conv6_2_GAP_substract_loss: 5.0593 - conv7_2_GAP_substract_loss: 6.6902 - conv8_2_GAP_substract_loss: 17.0539 - conv9_2_GAP_substract_loss: 25.6526 - predictions_loss: 3.5260 - val_loss: 7.1076 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.5965\n",
      "\n",
      "Epoch 00050: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-50_loss-5.0450_val_loss-7.1076.h5\n",
      "Epoch 51/120\n",
      "\n",
      "Epoch 00051: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 642s 642ms/step - loss: 5.0761 - pool1_GAP_substract_loss: 4053.6744 - pool2_GAP_substract_loss: 2339.2327 - pool3_GAP_substract_loss: 337.6653 - conv4_3_norm_GAP_substract_loss: 9.6116 - fc7_GAP_substract_loss: 4.6299 - conv6_2_GAP_substract_loss: 4.9745 - conv7_2_GAP_substract_loss: 6.5259 - conv8_2_GAP_substract_loss: 16.5473 - conv9_2_GAP_substract_loss: 24.6657 - predictions_loss: 3.5595 - val_loss: 7.1203 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6117\n",
      "\n",
      "Epoch 00051: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-51_loss-5.0761_val_loss-7.1203.h5\n",
      "Epoch 52/120\n",
      "\n",
      "Epoch 00052: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 5.0364 - pool1_GAP_substract_loss: 4026.8649 - pool2_GAP_substract_loss: 2306.6241 - pool3_GAP_substract_loss: 338.4710 - conv4_3_norm_GAP_substract_loss: 9.5735 - fc7_GAP_substract_loss: 4.6843 - conv6_2_GAP_substract_loss: 5.0674 - conv7_2_GAP_substract_loss: 6.6759 - conv8_2_GAP_substract_loss: 16.6998 - conv9_2_GAP_substract_loss: 24.7890 - predictions_loss: 3.5224 - val_loss: 7.1584 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6523\n",
      "\n",
      "Epoch 00052: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-52_loss-5.0364_val_loss-7.1584.h5\n",
      "Epoch 53/120\n",
      "\n",
      "Epoch 00053: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 5.0169 - pool1_GAP_substract_loss: 4055.1995 - pool2_GAP_substract_loss: 2338.2010 - pool3_GAP_substract_loss: 348.9094 - conv4_3_norm_GAP_substract_loss: 9.6191 - fc7_GAP_substract_loss: 4.7608 - conv6_2_GAP_substract_loss: 5.1569 - conv7_2_GAP_substract_loss: 6.7764 - conv8_2_GAP_substract_loss: 17.0617 - conv9_2_GAP_substract_loss: 25.2764 - predictions_loss: 3.5053 - val_loss: 7.1958 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6922\n",
      "\n",
      "Epoch 00053: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-53_loss-5.0169_val_loss-7.1958.h5\n",
      "Epoch 54/120\n",
      "\n",
      "Epoch 00054: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 5.0076 - pool1_GAP_substract_loss: 4041.5466 - pool2_GAP_substract_loss: 2361.9450 - pool3_GAP_substract_loss: 352.1626 - conv4_3_norm_GAP_substract_loss: 9.6704 - fc7_GAP_substract_loss: 4.7714 - conv6_2_GAP_substract_loss: 5.1392 - conv7_2_GAP_substract_loss: 6.6851 - conv8_2_GAP_substract_loss: 16.6176 - conv9_2_GAP_substract_loss: 24.3087 - predictions_loss: 3.4985 - val_loss: 7.1962 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6950\n",
      "\n",
      "Epoch 00054: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-54_loss-5.0076_val_loss-7.1962.h5\n",
      "Epoch 55/120\n",
      "\n",
      "Epoch 00055: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 4.9967 - pool1_GAP_substract_loss: 4057.2004 - pool2_GAP_substract_loss: 2381.9527 - pool3_GAP_substract_loss: 350.7774 - conv4_3_norm_GAP_substract_loss: 9.6507 - fc7_GAP_substract_loss: 4.8404 - conv6_2_GAP_substract_loss: 5.2536 - conv7_2_GAP_substract_loss: 6.8728 - conv8_2_GAP_substract_loss: 17.2504 - conv9_2_GAP_substract_loss: 25.4279 - predictions_loss: 3.4900 - val_loss: 7.1399 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6412\n",
      "\n",
      "Epoch 00055: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-55_loss-4.9967_val_loss-7.1399.h5\n",
      "Epoch 56/120\n",
      "\n",
      "Epoch 00056: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 5.0024 - pool1_GAP_substract_loss: 4067.3817 - pool2_GAP_substract_loss: 2387.7968 - pool3_GAP_substract_loss: 354.6374 - conv4_3_norm_GAP_substract_loss: 9.6989 - fc7_GAP_substract_loss: 4.8820 - conv6_2_GAP_substract_loss: 5.3615 - conv7_2_GAP_substract_loss: 7.0093 - conv8_2_GAP_substract_loss: 17.3920 - conv9_2_GAP_substract_loss: 25.5213 - predictions_loss: 3.4981 - val_loss: 7.1331 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6368\n",
      "\n",
      "Epoch 00056: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-56_loss-5.0024_val_loss-7.1331.h5\n",
      "Epoch 57/120\n",
      "\n",
      "Epoch 00057: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 626s 626ms/step - loss: 4.9913 - pool1_GAP_substract_loss: 4012.1647 - pool2_GAP_substract_loss: 2430.5844 - pool3_GAP_substract_loss: 362.6863 - conv4_3_norm_GAP_substract_loss: 9.5906 - fc7_GAP_substract_loss: 4.9031 - conv6_2_GAP_substract_loss: 5.4109 - conv7_2_GAP_substract_loss: 7.0702 - conv8_2_GAP_substract_loss: 17.5257 - conv9_2_GAP_substract_loss: 25.3448 - predictions_loss: 3.4895 - val_loss: 7.1682 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6745\n",
      "\n",
      "Epoch 00057: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-57_loss-4.9913_val_loss-7.1682.h5\n",
      "Epoch 58/120\n",
      "\n",
      "Epoch 00058: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 5.0158 - pool1_GAP_substract_loss: 4001.7326 - pool2_GAP_substract_loss: 2378.7081 - pool3_GAP_substract_loss: 362.9177 - conv4_3_norm_GAP_substract_loss: 9.6497 - fc7_GAP_substract_loss: 4.9136 - conv6_2_GAP_substract_loss: 5.4499 - conv7_2_GAP_substract_loss: 7.2176 - conv8_2_GAP_substract_loss: 18.1129 - conv9_2_GAP_substract_loss: 26.3107 - predictions_loss: 3.5165 - val_loss: 7.1503 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6589\n",
      "\n",
      "Epoch 00058: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-58_loss-5.0158_val_loss-7.1503.h5\n",
      "Epoch 59/120\n",
      "\n",
      "Epoch 00059: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 4.9651 - pool1_GAP_substract_loss: 3987.6463 - pool2_GAP_substract_loss: 2390.1993 - pool3_GAP_substract_loss: 363.5172 - conv4_3_norm_GAP_substract_loss: 9.6164 - fc7_GAP_substract_loss: 4.8366 - conv6_2_GAP_substract_loss: 5.3097 - conv7_2_GAP_substract_loss: 6.9677 - conv8_2_GAP_substract_loss: 17.4031 - conv9_2_GAP_substract_loss: 25.3136 - predictions_loss: 3.4683 - val_loss: 7.2600 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7711\n",
      "\n",
      "Epoch 00059: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-59_loss-4.9651_val_loss-7.2600.h5\n",
      "Epoch 60/120\n",
      "\n",
      "Epoch 00060: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 4.9569 - pool1_GAP_substract_loss: 3999.1926 - pool2_GAP_substract_loss: 2434.5500 - pool3_GAP_substract_loss: 369.8920 - conv4_3_norm_GAP_substract_loss: 9.7626 - fc7_GAP_substract_loss: 5.0071 - conv6_2_GAP_substract_loss: 5.5455 - conv7_2_GAP_substract_loss: 7.2588 - conv8_2_GAP_substract_loss: 18.0204 - conv9_2_GAP_substract_loss: 26.2258 - predictions_loss: 3.4624 - val_loss: 7.2048 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7183\n",
      "\n",
      "Epoch 00060: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-60_loss-4.9569_val_loss-7.2048.h5\n",
      "Epoch 61/120\n",
      "\n",
      "Epoch 00061: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 4.9534 - pool1_GAP_substract_loss: 3989.3608 - pool2_GAP_substract_loss: 2413.6763 - pool3_GAP_substract_loss: 369.9171 - conv4_3_norm_GAP_substract_loss: 9.6857 - fc7_GAP_substract_loss: 4.9013 - conv6_2_GAP_substract_loss: 5.4147 - conv7_2_GAP_substract_loss: 7.0841 - conv8_2_GAP_substract_loss: 17.6803 - conv9_2_GAP_substract_loss: 25.6059 - predictions_loss: 3.4613 - val_loss: 7.1600 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.6759\n",
      "\n",
      "Epoch 00061: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-61_loss-4.9534_val_loss-7.1600.h5\n",
      "Epoch 62/120\n",
      "\n",
      "Epoch 00062: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 4.9357 - pool1_GAP_substract_loss: 4006.5830 - pool2_GAP_substract_loss: 2472.0424 - pool3_GAP_substract_loss: 375.5575 - conv4_3_norm_GAP_substract_loss: 9.8342 - fc7_GAP_substract_loss: 5.0133 - conv6_2_GAP_substract_loss: 5.5582 - conv7_2_GAP_substract_loss: 7.2025 - conv8_2_GAP_substract_loss: 17.8112 - conv9_2_GAP_substract_loss: 25.8541 - predictions_loss: 3.4460 - val_loss: 7.2126 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7309\n",
      "\n",
      "Epoch 00062: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-62_loss-4.9357_val_loss-7.2126.h5\n",
      "Epoch 63/120\n",
      "\n",
      "Epoch 00063: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 4.9455 - pool1_GAP_substract_loss: 3996.6246 - pool2_GAP_substract_loss: 2488.5896 - pool3_GAP_substract_loss: 380.1378 - conv4_3_norm_GAP_substract_loss: 9.7563 - fc7_GAP_substract_loss: 4.9868 - conv6_2_GAP_substract_loss: 5.4688 - conv7_2_GAP_substract_loss: 7.0857 - conv8_2_GAP_substract_loss: 17.4812 - conv9_2_GAP_substract_loss: 25.0352 - predictions_loss: 3.4582 - val_loss: 7.2152 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7359\n",
      "\n",
      "Epoch 00063: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-63_loss-4.9455_val_loss-7.2152.h5\n",
      "Epoch 64/120\n",
      "\n",
      "Epoch 00064: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 625s 625ms/step - loss: 4.9557 - pool1_GAP_substract_loss: 3959.0467 - pool2_GAP_substract_loss: 2416.5621 - pool3_GAP_substract_loss: 375.2764 - conv4_3_norm_GAP_substract_loss: 9.5840 - fc7_GAP_substract_loss: 4.8783 - conv6_2_GAP_substract_loss: 5.3658 - conv7_2_GAP_substract_loss: 7.0517 - conv8_2_GAP_substract_loss: 17.6299 - conv9_2_GAP_substract_loss: 25.5778 - predictions_loss: 3.4709 - val_loss: 7.2037 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7268\n",
      "\n",
      "Epoch 00064: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-64_loss-4.9557_val_loss-7.2037.h5\n",
      "Epoch 65/120\n",
      "\n",
      "Epoch 00065: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 4.9589 - pool1_GAP_substract_loss: 3890.6691 - pool2_GAP_substract_loss: 2388.4611 - pool3_GAP_substract_loss: 370.5971 - conv4_3_norm_GAP_substract_loss: 9.4910 - fc7_GAP_substract_loss: 4.8737 - conv6_2_GAP_substract_loss: 5.3890 - conv7_2_GAP_substract_loss: 7.0748 - conv8_2_GAP_substract_loss: 17.3602 - conv9_2_GAP_substract_loss: 24.8502 - predictions_loss: 3.4765 - val_loss: 7.2033 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7288\n",
      "\n",
      "Epoch 00065: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-65_loss-4.9589_val_loss-7.2033.h5\n",
      "Epoch 66/120\n",
      "\n",
      "Epoch 00066: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 623s 623ms/step - loss: 4.9346 - pool1_GAP_substract_loss: 3945.1866 - pool2_GAP_substract_loss: 2470.8756 - pool3_GAP_substract_loss: 379.3494 - conv4_3_norm_GAP_substract_loss: 9.5711 - fc7_GAP_substract_loss: 5.0182 - conv6_2_GAP_substract_loss: 5.6283 - conv7_2_GAP_substract_loss: 7.4117 - conv8_2_GAP_substract_loss: 18.2392 - conv9_2_GAP_substract_loss: 25.9385 - predictions_loss: 3.4545 - val_loss: 7.2647 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7926\n",
      "\n",
      "Epoch 00066: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-66_loss-4.9346_val_loss-7.2647.h5\n",
      "Epoch 67/120\n",
      "\n",
      "Epoch 00067: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 4.9275 - pool1_GAP_substract_loss: 3976.7039 - pool2_GAP_substract_loss: 2506.5964 - pool3_GAP_substract_loss: 385.8035 - conv4_3_norm_GAP_substract_loss: 9.6498 - fc7_GAP_substract_loss: 5.0407 - conv6_2_GAP_substract_loss: 5.6573 - conv7_2_GAP_substract_loss: 7.3530 - conv8_2_GAP_substract_loss: 17.9342 - conv9_2_GAP_substract_loss: 25.5141 - predictions_loss: 3.4498 - val_loss: 7.2754 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8057\n",
      "\n",
      "Epoch 00067: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-67_loss-4.9275_val_loss-7.2754.h5\n",
      "Epoch 68/120\n",
      "\n",
      "Epoch 00068: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 4.9128 - pool1_GAP_substract_loss: 3931.8343 - pool2_GAP_substract_loss: 2418.4890 - pool3_GAP_substract_loss: 377.7732 - conv4_3_norm_GAP_substract_loss: 9.6375 - fc7_GAP_substract_loss: 5.0515 - conv6_2_GAP_substract_loss: 5.7153 - conv7_2_GAP_substract_loss: 7.4365 - conv8_2_GAP_substract_loss: 18.1665 - conv9_2_GAP_substract_loss: 26.0157 - predictions_loss: 3.4375 - val_loss: 7.1965 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7292\n",
      "\n",
      "Epoch 00068: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-68_loss-4.9128_val_loss-7.1965.h5\n",
      "Epoch 69/120\n",
      "\n",
      "Epoch 00069: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 626s 626ms/step - loss: 4.9113 - pool1_GAP_substract_loss: 3911.0942 - pool2_GAP_substract_loss: 2441.3157 - pool3_GAP_substract_loss: 382.3608 - conv4_3_norm_GAP_substract_loss: 9.7415 - fc7_GAP_substract_loss: 5.0911 - conv6_2_GAP_substract_loss: 5.6812 - conv7_2_GAP_substract_loss: 7.4202 - conv8_2_GAP_substract_loss: 18.2210 - conv9_2_GAP_substract_loss: 26.2052 - predictions_loss: 3.4384 - val_loss: 7.2430 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7780\n",
      "\n",
      "Epoch 00069: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-69_loss-4.9113_val_loss-7.2430.h5\n",
      "Epoch 70/120\n",
      "\n",
      "Epoch 00070: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 629s 629ms/step - loss: 4.9145 - pool1_GAP_substract_loss: 3923.5268 - pool2_GAP_substract_loss: 2490.4934 - pool3_GAP_substract_loss: 382.8293 - conv4_3_norm_GAP_substract_loss: 9.6303 - fc7_GAP_substract_loss: 5.0478 - conv6_2_GAP_substract_loss: 5.6492 - conv7_2_GAP_substract_loss: 7.3172 - conv8_2_GAP_substract_loss: 17.8893 - conv9_2_GAP_substract_loss: 25.3967 - predictions_loss: 3.4438 - val_loss: 7.2251 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7625\n",
      "\n",
      "Epoch 00070: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-70_loss-4.9145_val_loss-7.2251.h5\n",
      "Epoch 71/120\n",
      "\n",
      "Epoch 00071: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 648s 648ms/step - loss: 4.9197 - pool1_GAP_substract_loss: 3883.6533 - pool2_GAP_substract_loss: 2455.3788 - pool3_GAP_substract_loss: 377.5388 - conv4_3_norm_GAP_substract_loss: 9.4853 - fc7_GAP_substract_loss: 5.1131 - conv6_2_GAP_substract_loss: 5.8128 - conv7_2_GAP_substract_loss: 7.5788 - conv8_2_GAP_substract_loss: 18.4834 - conv9_2_GAP_substract_loss: 26.3184 - predictions_loss: 3.4515 - val_loss: 7.2225 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7621\n",
      "\n",
      "Epoch 00071: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-71_loss-4.9197_val_loss-7.2225.h5\n",
      "Epoch 72/120\n",
      "\n",
      "Epoch 00072: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 623s 623ms/step - loss: 4.9294 - pool1_GAP_substract_loss: 3877.2986 - pool2_GAP_substract_loss: 2467.8940 - pool3_GAP_substract_loss: 384.6973 - conv4_3_norm_GAP_substract_loss: 9.5597 - fc7_GAP_substract_loss: 5.1784 - conv6_2_GAP_substract_loss: 5.8796 - conv7_2_GAP_substract_loss: 7.6574 - conv8_2_GAP_substract_loss: 18.5185 - conv9_2_GAP_substract_loss: 26.0506 - predictions_loss: 3.4635 - val_loss: 7.3172 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8592\n",
      "\n",
      "Epoch 00072: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-72_loss-4.9294_val_loss-7.3172.h5\n",
      "Epoch 73/120\n",
      "\n",
      "Epoch 00073: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 4.8895 - pool1_GAP_substract_loss: 3987.7152 - pool2_GAP_substract_loss: 2525.5581 - pool3_GAP_substract_loss: 394.6832 - conv4_3_norm_GAP_substract_loss: 9.9015 - fc7_GAP_substract_loss: 5.1819 - conv6_2_GAP_substract_loss: 5.9069 - conv7_2_GAP_substract_loss: 7.7223 - conv8_2_GAP_substract_loss: 18.7851 - conv9_2_GAP_substract_loss: 26.4643 - predictions_loss: 3.4257 - val_loss: 7.3313 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8756\n",
      "\n",
      "Epoch 00073: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-73_loss-4.8895_val_loss-7.3313.h5\n",
      "Epoch 74/120\n",
      "\n",
      "Epoch 00074: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 4.8887 - pool1_GAP_substract_loss: 3947.4070 - pool2_GAP_substract_loss: 2511.7258 - pool3_GAP_substract_loss: 390.0250 - conv4_3_norm_GAP_substract_loss: 9.6914 - fc7_GAP_substract_loss: 5.0856 - conv6_2_GAP_substract_loss: 5.8551 - conv7_2_GAP_substract_loss: 7.6775 - conv8_2_GAP_substract_loss: 18.8171 - conv9_2_GAP_substract_loss: 26.7288 - predictions_loss: 3.4273 - val_loss: 7.2584 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8051\n",
      "\n",
      "Epoch 00074: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-74_loss-4.8887_val_loss-7.2584.h5\n",
      "Epoch 75/120\n",
      "\n",
      "Epoch 00075: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 4.8712 - pool1_GAP_substract_loss: 3914.0442 - pool2_GAP_substract_loss: 2506.2743 - pool3_GAP_substract_loss: 388.2214 - conv4_3_norm_GAP_substract_loss: 9.6547 - fc7_GAP_substract_loss: 5.1513 - conv6_2_GAP_substract_loss: 5.9139 - conv7_2_GAP_substract_loss: 7.7601 - conv8_2_GAP_substract_loss: 19.1122 - conv9_2_GAP_substract_loss: 27.3266 - predictions_loss: 3.4122 - val_loss: 7.2694 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8183\n",
      "\n",
      "Epoch 00075: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-75_loss-4.8712_val_loss-7.2694.h5\n",
      "Epoch 76/120\n",
      "\n",
      "Epoch 00076: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 4.8757 - pool1_GAP_substract_loss: 3883.6649 - pool2_GAP_substract_loss: 2523.9317 - pool3_GAP_substract_loss: 383.4376 - conv4_3_norm_GAP_substract_loss: 9.5062 - fc7_GAP_substract_loss: 5.1864 - conv6_2_GAP_substract_loss: 5.9017 - conv7_2_GAP_substract_loss: 7.6992 - conv8_2_GAP_substract_loss: 18.8763 - conv9_2_GAP_substract_loss: 26.8594 - predictions_loss: 3.4190 - val_loss: 7.2900 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8412\n",
      "\n",
      "Epoch 00076: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-76_loss-4.8757_val_loss-7.2900.h5\n",
      "Epoch 77/120\n",
      "\n",
      "Epoch 00077: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 4.8880 - pool1_GAP_substract_loss: 3872.3782 - pool2_GAP_substract_loss: 2515.0219 - pool3_GAP_substract_loss: 382.9632 - conv4_3_norm_GAP_substract_loss: 9.4490 - fc7_GAP_substract_loss: 5.1488 - conv6_2_GAP_substract_loss: 5.8535 - conv7_2_GAP_substract_loss: 7.6561 - conv8_2_GAP_substract_loss: 18.6725 - conv9_2_GAP_substract_loss: 26.3835 - predictions_loss: 3.4336 - val_loss: 7.2388 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7923\n",
      "\n",
      "Epoch 00077: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-77_loss-4.8880_val_loss-7.2388.h5\n",
      "Epoch 78/120\n",
      "\n",
      "Epoch 00078: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 625s 625ms/step - loss: 4.8631 - pool1_GAP_substract_loss: 3889.6779 - pool2_GAP_substract_loss: 2556.3772 - pool3_GAP_substract_loss: 394.9006 - conv4_3_norm_GAP_substract_loss: 9.6485 - fc7_GAP_substract_loss: 5.2568 - conv6_2_GAP_substract_loss: 6.0319 - conv7_2_GAP_substract_loss: 7.9247 - conv8_2_GAP_substract_loss: 19.4469 - conv9_2_GAP_substract_loss: 27.3675 - predictions_loss: 3.4110 - val_loss: 7.2437 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7995\n",
      "\n",
      "Epoch 00078: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-78_loss-4.8631_val_loss-7.2437.h5\n",
      "Epoch 79/120\n",
      "\n",
      "Epoch 00079: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 4.8571 - pool1_GAP_substract_loss: 3949.9127 - pool2_GAP_substract_loss: 2525.0415 - pool3_GAP_substract_loss: 392.1185 - conv4_3_norm_GAP_substract_loss: 9.7136 - fc7_GAP_substract_loss: 5.2433 - conv6_2_GAP_substract_loss: 6.0205 - conv7_2_GAP_substract_loss: 7.8964 - conv8_2_GAP_substract_loss: 19.1736 - conv9_2_GAP_substract_loss: 26.6318 - predictions_loss: 3.4072 - val_loss: 7.1774 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.7355\n",
      "\n",
      "Epoch 00079: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-79_loss-4.8571_val_loss-7.1774.h5\n",
      "Epoch 80/120\n",
      "\n",
      "Epoch 00080: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 4.8586 - pool1_GAP_substract_loss: 3942.0734 - pool2_GAP_substract_loss: 2523.3795 - pool3_GAP_substract_loss: 392.9576 - conv4_3_norm_GAP_substract_loss: 9.7125 - fc7_GAP_substract_loss: 5.2676 - conv6_2_GAP_substract_loss: 6.0506 - conv7_2_GAP_substract_loss: 7.9042 - conv8_2_GAP_substract_loss: 19.3385 - conv9_2_GAP_substract_loss: 27.2399 - predictions_loss: 3.4110 - val_loss: 7.2504 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8108\n",
      "\n",
      "Epoch 00080: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-80_loss-4.8586_val_loss-7.2504.h5\n",
      "Epoch 81/120\n",
      "\n",
      "Epoch 00081: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 639s 639ms/step - loss: 4.8522 - pool1_GAP_substract_loss: 3969.2594 - pool2_GAP_substract_loss: 2582.2045 - pool3_GAP_substract_loss: 396.2491 - conv4_3_norm_GAP_substract_loss: 9.6466 - fc7_GAP_substract_loss: 5.3019 - conv6_2_GAP_substract_loss: 6.0970 - conv7_2_GAP_substract_loss: 7.9593 - conv8_2_GAP_substract_loss: 19.3011 - conv9_2_GAP_substract_loss: 26.9182 - predictions_loss: 3.4067 - val_loss: 7.3154 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8781\n",
      "\n",
      "Epoch 00081: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-81_loss-4.8522_val_loss-7.3154.h5\n",
      "Epoch 82/120\n",
      "\n",
      "Epoch 00082: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 634s 634ms/step - loss: 4.8389 - pool1_GAP_substract_loss: 3934.4288 - pool2_GAP_substract_loss: 2583.2641 - pool3_GAP_substract_loss: 398.9786 - conv4_3_norm_GAP_substract_loss: 9.8870 - fc7_GAP_substract_loss: 5.3908 - conv6_2_GAP_substract_loss: 6.2816 - conv7_2_GAP_substract_loss: 8.3019 - conv8_2_GAP_substract_loss: 20.4820 - conv9_2_GAP_substract_loss: 28.8044 - predictions_loss: 3.3957 - val_loss: 7.3270 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8919\n",
      "\n",
      "Epoch 00082: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-82_loss-4.8389_val_loss-7.3270.h5\n",
      "Epoch 83/120\n",
      "\n",
      "Epoch 00083: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 4.8492 - pool1_GAP_substract_loss: 3853.4628 - pool2_GAP_substract_loss: 2574.6975 - pool3_GAP_substract_loss: 397.4737 - conv4_3_norm_GAP_substract_loss: 9.7101 - fc7_GAP_substract_loss: 5.2545 - conv6_2_GAP_substract_loss: 6.0994 - conv7_2_GAP_substract_loss: 7.9819 - conv8_2_GAP_substract_loss: 19.2695 - conv9_2_GAP_substract_loss: 26.7157 - predictions_loss: 3.4084 - val_loss: 7.3225 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8896\n",
      "\n",
      "Epoch 00083: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-83_loss-4.8492_val_loss-7.3225.h5\n",
      "Epoch 84/120\n",
      "\n",
      "Epoch 00084: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 638s 638ms/step - loss: 4.8375 - pool1_GAP_substract_loss: 3921.9600 - pool2_GAP_substract_loss: 2578.9565 - pool3_GAP_substract_loss: 394.8264 - conv4_3_norm_GAP_substract_loss: 9.6967 - fc7_GAP_substract_loss: 5.2706 - conv6_2_GAP_substract_loss: 6.1521 - conv7_2_GAP_substract_loss: 8.0619 - conv8_2_GAP_substract_loss: 19.4672 - conv9_2_GAP_substract_loss: 27.0462 - predictions_loss: 3.3989 - val_loss: 7.3630 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9324\n",
      "\n",
      "Epoch 00084: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-84_loss-4.8375_val_loss-7.3630.h5\n",
      "Epoch 85/120\n",
      "\n",
      "Epoch 00085: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 635s 635ms/step - loss: 4.8309 - pool1_GAP_substract_loss: 3929.3181 - pool2_GAP_substract_loss: 2585.8543 - pool3_GAP_substract_loss: 397.8351 - conv4_3_norm_GAP_substract_loss: 9.6954 - fc7_GAP_substract_loss: 5.3627 - conv6_2_GAP_substract_loss: 6.2803 - conv7_2_GAP_substract_loss: 8.2275 - conv8_2_GAP_substract_loss: 19.8262 - conv9_2_GAP_substract_loss: 27.5035 - predictions_loss: 3.3945 - val_loss: 7.2779 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8496\n",
      "\n",
      "Epoch 00085: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-85_loss-4.8309_val_loss-7.2779.h5\n",
      "Epoch 86/120\n",
      "\n",
      "Epoch 00086: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 638s 638ms/step - loss: 4.8225 - pool1_GAP_substract_loss: 3844.3600 - pool2_GAP_substract_loss: 2560.3276 - pool3_GAP_substract_loss: 396.8029 - conv4_3_norm_GAP_substract_loss: 9.6689 - fc7_GAP_substract_loss: 5.2917 - conv6_2_GAP_substract_loss: 6.1062 - conv7_2_GAP_substract_loss: 7.9717 - conv8_2_GAP_substract_loss: 19.1934 - conv9_2_GAP_substract_loss: 26.3253 - predictions_loss: 3.3884 - val_loss: 7.2962 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8700\n",
      "\n",
      "Epoch 00086: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-86_loss-4.8225_val_loss-7.2962.h5\n",
      "Epoch 87/120\n",
      "\n",
      "Epoch 00087: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 636s 636ms/step - loss: 4.8282 - pool1_GAP_substract_loss: 3850.6385 - pool2_GAP_substract_loss: 2556.8743 - pool3_GAP_substract_loss: 395.4329 - conv4_3_norm_GAP_substract_loss: 9.7185 - fc7_GAP_substract_loss: 5.4785 - conv6_2_GAP_substract_loss: 6.4275 - conv7_2_GAP_substract_loss: 8.4454 - conv8_2_GAP_substract_loss: 20.2968 - conv9_2_GAP_substract_loss: 28.0170 - predictions_loss: 3.3963 - val_loss: 7.3163 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8924\n",
      "\n",
      "Epoch 00087: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-87_loss-4.8282_val_loss-7.3163.h5\n",
      "Epoch 88/120\n",
      "\n",
      "Epoch 00088: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 4.8219 - pool1_GAP_substract_loss: 3880.6692 - pool2_GAP_substract_loss: 2576.2347 - pool3_GAP_substract_loss: 397.1738 - conv4_3_norm_GAP_substract_loss: 9.6887 - fc7_GAP_substract_loss: 5.4255 - conv6_2_GAP_substract_loss: 6.3769 - conv7_2_GAP_substract_loss: 8.4546 - conv8_2_GAP_substract_loss: 20.3474 - conv9_2_GAP_substract_loss: 28.3474 - predictions_loss: 3.3922 - val_loss: 7.2876 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8659\n",
      "\n",
      "Epoch 00088: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-88_loss-4.8219_val_loss-7.2876.h5\n",
      "Epoch 89/120\n",
      "\n",
      "Epoch 00089: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 4.8036 - pool1_GAP_substract_loss: 3875.4277 - pool2_GAP_substract_loss: 2612.3560 - pool3_GAP_substract_loss: 404.6191 - conv4_3_norm_GAP_substract_loss: 9.7513 - fc7_GAP_substract_loss: 5.5111 - conv6_2_GAP_substract_loss: 6.4710 - conv7_2_GAP_substract_loss: 8.5761 - conv8_2_GAP_substract_loss: 20.6951 - conv9_2_GAP_substract_loss: 28.6705 - predictions_loss: 3.3761 - val_loss: 7.3197 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9002\n",
      "\n",
      "Epoch 00089: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-89_loss-4.8036_val_loss-7.3197.h5\n",
      "Epoch 90/120\n",
      "\n",
      "Epoch 00090: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 4.8219 - pool1_GAP_substract_loss: 3815.1394 - pool2_GAP_substract_loss: 2562.1781 - pool3_GAP_substract_loss: 394.9362 - conv4_3_norm_GAP_substract_loss: 9.5688 - fc7_GAP_substract_loss: 5.3864 - conv6_2_GAP_substract_loss: 6.3271 - conv7_2_GAP_substract_loss: 8.3203 - conv8_2_GAP_substract_loss: 19.9539 - conv9_2_GAP_substract_loss: 27.3699 - predictions_loss: 3.3967 - val_loss: 7.3126 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8953\n",
      "\n",
      "Epoch 00090: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-90_loss-4.8219_val_loss-7.3126.h5\n",
      "Epoch 91/120\n",
      "\n",
      "Epoch 00091: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 624s 624ms/step - loss: 4.8236 - pool1_GAP_substract_loss: 3895.9290 - pool2_GAP_substract_loss: 2603.1853 - pool3_GAP_substract_loss: 397.7365 - conv4_3_norm_GAP_substract_loss: 9.6357 - fc7_GAP_substract_loss: 5.3946 - conv6_2_GAP_substract_loss: 6.2539 - conv7_2_GAP_substract_loss: 8.1199 - conv8_2_GAP_substract_loss: 19.1430 - conv9_2_GAP_substract_loss: 25.9579 - predictions_loss: 3.4005 - val_loss: 7.3042 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8891\n",
      "\n",
      "Epoch 00091: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-91_loss-4.8236_val_loss-7.3042.h5\n",
      "Epoch 92/120\n",
      "\n",
      "Epoch 00092: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 627s 627ms/step - loss: 4.7904 - pool1_GAP_substract_loss: 3886.0878 - pool2_GAP_substract_loss: 2582.7546 - pool3_GAP_substract_loss: 396.6399 - conv4_3_norm_GAP_substract_loss: 9.6991 - fc7_GAP_substract_loss: 5.4245 - conv6_2_GAP_substract_loss: 6.3603 - conv7_2_GAP_substract_loss: 8.3561 - conv8_2_GAP_substract_loss: 19.8707 - conv9_2_GAP_substract_loss: 27.1977 - predictions_loss: 3.3695 - val_loss: 7.3759 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9629\n",
      "\n",
      "Epoch 00092: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-92_loss-4.7904_val_loss-7.3759.h5\n",
      "Epoch 93/120\n",
      "\n",
      "Epoch 00093: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 626s 626ms/step - loss: 4.7772 - pool1_GAP_substract_loss: 3883.9533 - pool2_GAP_substract_loss: 2597.8243 - pool3_GAP_substract_loss: 399.2196 - conv4_3_norm_GAP_substract_loss: 9.6266 - fc7_GAP_substract_loss: 5.4544 - conv6_2_GAP_substract_loss: 6.4847 - conv7_2_GAP_substract_loss: 8.5334 - conv8_2_GAP_substract_loss: 20.1690 - conv9_2_GAP_substract_loss: 27.4470 - predictions_loss: 3.3585 - val_loss: 7.3491 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9383\n",
      "\n",
      "Epoch 00093: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-93_loss-4.7772_val_loss-7.3491.h5\n",
      "Epoch 94/120\n",
      "\n",
      "Epoch 00094: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 653s 653ms/step - loss: 4.7703 - pool1_GAP_substract_loss: 3840.7626 - pool2_GAP_substract_loss: 2612.0570 - pool3_GAP_substract_loss: 402.4538 - conv4_3_norm_GAP_substract_loss: 9.8667 - fc7_GAP_substract_loss: 5.5071 - conv6_2_GAP_substract_loss: 6.4940 - conv7_2_GAP_substract_loss: 8.5890 - conv8_2_GAP_substract_loss: 20.5293 - conv9_2_GAP_substract_loss: 27.7909 - predictions_loss: 3.3538 - val_loss: 7.3281 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9195\n",
      "\n",
      "Epoch 00094: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-94_loss-4.7703_val_loss-7.3281.h5\n",
      "Epoch 95/120\n",
      "\n",
      "Epoch 00095: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 653s 653ms/step - loss: 4.7612 - pool1_GAP_substract_loss: 3929.4314 - pool2_GAP_substract_loss: 2639.1364 - pool3_GAP_substract_loss: 401.3164 - conv4_3_norm_GAP_substract_loss: 9.8642 - fc7_GAP_substract_loss: 5.5479 - conv6_2_GAP_substract_loss: 6.5664 - conv7_2_GAP_substract_loss: 8.5960 - conv8_2_GAP_substract_loss: 20.2193 - conv9_2_GAP_substract_loss: 27.0772 - predictions_loss: 3.3467 - val_loss: 7.3463 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9399\n",
      "\n",
      "Epoch 00095: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-95_loss-4.7612_val_loss-7.3463.h5\n",
      "Epoch 96/120\n",
      "\n",
      "Epoch 00096: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 4.7830 - pool1_GAP_substract_loss: 3786.2825 - pool2_GAP_substract_loss: 2623.3592 - pool3_GAP_substract_loss: 398.9806 - conv4_3_norm_GAP_substract_loss: 9.7317 - fc7_GAP_substract_loss: 5.5495 - conv6_2_GAP_substract_loss: 6.6322 - conv7_2_GAP_substract_loss: 8.7885 - conv8_2_GAP_substract_loss: 21.2544 - conv9_2_GAP_substract_loss: 29.3415 - predictions_loss: 3.3709 - val_loss: 7.3851 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9808\n",
      "\n",
      "Epoch 00096: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-96_loss-4.7830_val_loss-7.3851.h5\n",
      "Epoch 97/120\n",
      "\n",
      "Epoch 00097: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 631s 631ms/step - loss: 4.7516 - pool1_GAP_substract_loss: 3868.1453 - pool2_GAP_substract_loss: 2637.1881 - pool3_GAP_substract_loss: 405.0470 - conv4_3_norm_GAP_substract_loss: 9.7650 - fc7_GAP_substract_loss: 5.5156 - conv6_2_GAP_substract_loss: 6.4981 - conv7_2_GAP_substract_loss: 8.5737 - conv8_2_GAP_substract_loss: 20.6727 - conv9_2_GAP_substract_loss: 28.2534 - predictions_loss: 3.3416 - val_loss: 7.4516 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 6.0495\n",
      "\n",
      "Epoch 00097: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-97_loss-4.7516_val_loss-7.4516.h5\n",
      "Epoch 98/120\n",
      "\n",
      "Epoch 00098: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 4.7358 - pool1_GAP_substract_loss: 3853.4163 - pool2_GAP_substract_loss: 2598.0806 - pool3_GAP_substract_loss: 400.0516 - conv4_3_norm_GAP_substract_loss: 9.6751 - fc7_GAP_substract_loss: 5.5819 - conv6_2_GAP_substract_loss: 6.6575 - conv7_2_GAP_substract_loss: 8.7490 - conv8_2_GAP_substract_loss: 20.5906 - conv9_2_GAP_substract_loss: 27.6902 - predictions_loss: 3.3279 - val_loss: 7.2633 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.8633\n",
      "\n",
      "Epoch 00098: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-98_loss-4.7358_val_loss-7.2633.h5\n",
      "Epoch 99/120\n",
      "\n",
      "Epoch 00099: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 625s 625ms/step - loss: 4.7549 - pool1_GAP_substract_loss: 3791.3573 - pool2_GAP_substract_loss: 2584.5422 - pool3_GAP_substract_loss: 400.8066 - conv4_3_norm_GAP_substract_loss: 9.6577 - fc7_GAP_substract_loss: 5.4291 - conv6_2_GAP_substract_loss: 6.4772 - conv7_2_GAP_substract_loss: 8.5331 - conv8_2_GAP_substract_loss: 20.3947 - conv9_2_GAP_substract_loss: 27.6382 - predictions_loss: 3.3492 - val_loss: 7.3710 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9731\n",
      "\n",
      "Epoch 00099: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-99_loss-4.7549_val_loss-7.3710.h5\n",
      "Epoch 100/120\n",
      "\n",
      "Epoch 00100: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 4.7522 - pool1_GAP_substract_loss: 3887.3083 - pool2_GAP_substract_loss: 2634.2694 - pool3_GAP_substract_loss: 402.3403 - conv4_3_norm_GAP_substract_loss: 9.6965 - fc7_GAP_substract_loss: 5.6083 - conv6_2_GAP_substract_loss: 6.7552 - conv7_2_GAP_substract_loss: 8.9447 - conv8_2_GAP_substract_loss: 21.2725 - conv9_2_GAP_substract_loss: 28.5814 - predictions_loss: 3.3485 - val_loss: 7.3359 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9402\n",
      "\n",
      "Epoch 00100: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-100_loss-4.7522_val_loss-7.3359.h5\n",
      "Epoch 101/120\n",
      "\n",
      "Epoch 00101: LearningRateScheduler setting learning rate to 1e-05.\n",
      "1000/1000 [==============================] - 625s 625ms/step - loss: 4.7453 - pool1_GAP_substract_loss: 3884.2556 - pool2_GAP_substract_loss: 2643.7356 - pool3_GAP_substract_loss: 401.5795 - conv4_3_norm_GAP_substract_loss: 9.7677 - fc7_GAP_substract_loss: 5.5400 - conv6_2_GAP_substract_loss: 6.6177 - conv7_2_GAP_substract_loss: 8.6123 - conv8_2_GAP_substract_loss: 20.2054 - conv9_2_GAP_substract_loss: 27.2758 - predictions_loss: 3.3427 - val_loss: 7.3474 - val_pool1_GAP_substract_loss: 0.0000e+00 - val_pool2_GAP_substract_loss: 0.0000e+00 - val_pool3_GAP_substract_loss: 0.0000e+00 - val_conv4_3_norm_GAP_substract_loss: 0.0000e+00 - val_fc7_GAP_substract_loss: 0.0000e+00 - val_conv6_2_GAP_substract_loss: 0.0000e+00 - val_conv7_2_GAP_substract_loss: 0.0000e+00 - val_conv8_2_GAP_substract_loss: 0.0000e+00 - val_conv9_2_GAP_substract_loss: 0.0000e+00 - val_predictions_loss: 5.9520\n",
      "\n",
      "Epoch 00101: saving model to ../trained_weights/current/ssd_augm_beta_0_01_pool123_global_pool_SGD/epoch-101_loss-4.7453_val_loss-7.3474.h5\n",
      "Epoch 102/120\n",
      "\n",
      "Epoch 00102: LearningRateScheduler setting learning rate to 1e-05.\n",
      " 374/1000 [==========>...................] - ETA: 6:27 - loss: 4.7218 - pool1_GAP_substract_loss: 3898.0218 - pool2_GAP_substract_loss: 2660.4406 - pool3_GAP_substract_loss: 404.6782 - conv4_3_norm_GAP_substract_loss: 9.6633 - fc7_GAP_substract_loss: 5.4332 - conv6_2_GAP_substract_loss: 6.4580 - conv7_2_GAP_substract_loss: 8.4519 - conv8_2_GAP_substract_loss: 20.3418 - conv9_2_GAP_substract_loss: 27.7693 - predictions_loss: 3.3194"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ea67b764e796>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                               \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset_size\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2719\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2720\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2721\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2693\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2694\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initial_epoch = 0\n",
    "final_epoch = 120\n",
    "steps_per_epoch = 1000\n",
    "\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              epochs=final_epoch,\n",
    "                              callbacks=callbacks,\n",
    "                              validation_data=val_generator,\n",
    "                              validation_steps=ceil(val_dataset_size/batch_size),\n",
    "                              initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Set the generator for the val_dataset or train_dataset predictions.\n",
    "\n",
    "predict_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[ssd_data_augmentation],\n",
    "                                         label_encoder=None,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'filenames',\n",
    "                                                  'inverse_transform',\n",
    "                                                  'original_images',\n",
    "                                                  'original_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n",
    "# 2: Generate samples.\n",
    "\n",
    "batch_images, batch_filenames, batch_inverse_transforms, batch_original_images, batch_original_labels = next(predict_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_images, batch_filenames, batch_inverse_transforms, batch_original_images, batch_original_labels = next(predict_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 5\n",
    "print(\"Image:\", batch_filenames[i])\n",
    "colors = plt.cm.hsv(np.linspace(0, 1, n_classes+1)).tolist()\n",
    "classes = ['background',\n",
    "           'person', 'rider', 'car', 'truck',\n",
    "           'bus', 'train', 'motorcycle', 'bicycle']\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.imshow(batch_images[0][i])\n",
    "plt.show()\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.imshow(batch_images[1][i])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3  # Which batch item to look at\n",
    "\n",
    "print(\"Image:\", batch_filenames[i])\n",
    "print()\n",
    "print(\"Ground truth boxes:\\n\")\n",
    "print(np.array(batch_original_labels[i]))\n",
    "\n",
    "# 3: Make predictions.\n",
    "\n",
    "y_pred = model.predict(batch_images)[-1]\n",
    "\n",
    "# Now let's decode the raw predictions in `y_pred`.\n",
    "\n",
    "# Had we created the model in 'inference' or 'inference_fast' mode,\n",
    "# then the model's final layer would be a `DecodeDetections` layer and\n",
    "# `y_pred` would already contain the decoded predictions,\n",
    "# but since we created the model in 'training' mode,\n",
    "# the model outputs raw predictions that still need to be decoded and filtered.\n",
    "# This is what the `decode_detections()` function is for.\n",
    "# It does exactly what the `DecodeDetections` layer would do,\n",
    "# but using Numpy instead of TensorFlow (i.e. on the CPU instead of the GPU).\n",
    "\n",
    "# `decode_detections()` with default argument values follows the procedure of the original SSD implementation:\n",
    "# First, a very low confidence threshold of 0.01 is applied to filter out the majority of the predicted boxes,\n",
    "# then greedy non-maximum suppression is performed per class with an intersection-over-union threshold of 0.45,\n",
    "# and out of what is left after that, the top 200 highest confidence boxes are returned.\n",
    "# Those settings are for precision-recall scoring purposes though.\n",
    "# In order to get some usable final predictions, we'll set the confidence threshold much higher, e.g. to 0.5,\n",
    "# since we're only interested in the very confident predictions.\n",
    "\n",
    "# 4: Decode the raw predictions in `y_pred`.\n",
    "\n",
    "y_pred_decoded = decode_detections(y_pred,\n",
    "                                   confidence_thresh=0.35,\n",
    "                                   iou_threshold=0.4,\n",
    "                                   top_k=200,\n",
    "                                   normalize_coords=normalize_coords,\n",
    "                                   img_height=img_height,\n",
    "                                   img_width=img_width)\n",
    "\n",
    "# We made the predictions on the resized images,\n",
    "# but we'd like to visualize the outcome on the original input images,\n",
    "# so we'll convert the coordinates accordingly.\n",
    "# Don't worry about that opaque `apply_inverse_transforms()` function below,\n",
    "# in this simple case it just applies `(* original_image_size / resized_image_size)` to the box coordinates.\n",
    "\n",
    "# 5: Convert the predictions for the original image.\n",
    "\n",
    "y_pred_decoded_inv = apply_inverse_transforms(y_pred_decoded, batch_inverse_transforms)\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=90)\n",
    "print(\"Predicted boxes:\\n\")\n",
    "print('   class   conf xmin   ymin   xmax   ymax')\n",
    "print(y_pred_decoded_inv[i])\n",
    "\n",
    "# Finally, let's draw the predicted boxes onto the image.\n",
    "# Each predicted box says its confidence next to the category name.\n",
    "# The ground truth boxes are also drawn onto the image in green for comparison.\n",
    "\n",
    "# 5: Draw the predicted boxes onto the image\n",
    "\n",
    "# Set the colors for the bounding boxes\n",
    "colors = plt.cm.hsv(np.linspace(0, 1, n_classes+1)).tolist()\n",
    "classes = ['background',\n",
    "           'person', 'rider', 'car', 'truck',\n",
    "           'bus', 'train', 'motorcycle', 'bicycle']\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.imshow(batch_original_images[i])\n",
    "\n",
    "current_axis = plt.gca()\n",
    "\n",
    "for box in batch_original_labels[i]:\n",
    "    xmin = box[1]\n",
    "    ymin = box[2]\n",
    "    xmax = box[3]\n",
    "    ymax = box[4]\n",
    "    label = '{}'.format(classes[int(box[0])])\n",
    "    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color='green', fill=False, linewidth=2))\n",
    "    current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor': 'green', 'alpha': 1.0})\n",
    "\n",
    "for box in y_pred_decoded_inv[i]:\n",
    "    xmin = box[2]\n",
    "    ymin = box[3]\n",
    "    xmax = box[4]\n",
    "    ymax = box[5]\n",
    "    color = colors[int(box[0])]\n",
    "    label = '{}: {:.2f}'.format(classes[int(box[0])], box[1])\n",
    "    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color=color, fill=False, linewidth=2))\n",
    "    current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor': color, 'alpha': 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
