{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from scipy.misc import imread\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from models.keras_ssd512_Siamese_analyze_contrastive_loss import ssd_512\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
    "from keras_layers.keras_layer_L2Normalization import L2Normalization\n",
    "from data_generator.object_detection_2d_data_generator_paired_input import DataGenerator\n",
    "from eval_utils.average_precision_evaluator import Evaluator\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "# Set a few configuration parameters.\n",
    "img_height = 512\n",
    "img_width = 512\n",
    "# model_mode indicates the way the pretrained model was created.\n",
    "# In training model, Model_Build should == 'Load_Model'. decode_detections will be called in the Evaluator.\n",
    "# However, decode_detections is run on CPU and is very slow.\n",
    "# In inference model, Model_Build should == 'New_Model_Load_Weights'.\n",
    "# DecodeDetections will be called when build the model. DecodeDetections is writen in tensorflow and is run GPU.\n",
    "# It seems that the result under inference model is slightly better than that under training model.\n",
    "# Maybe DecodeDetections and decode_detections are not exactly the same.\n",
    "model_mode = 'inference'  # 'training'#\n",
    "assert model_mode == 'inference'\n",
    "\n",
    "# model_path = '../trained_weights/SSD512_City_to_foggy0_01_resize_400_800/current/pool12_loss_weights_0_000005/epoch-55_loss-3.9684_val_loss-5.2015.h5'\n",
    "model_path = '../trained_weights/SSD512_City_to_foggy0_01_resize_400_800/current/source_only_run2/epoch-67_loss-3.8379_val_loss-6.0883.h5'\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "DatasetName = 'City_to_foggy0_01_paired_input'\n",
    "processed_dataset_path = './processed_dataset_h5/' + DatasetName\n",
    "if not os.path.exists(processed_dataset_path):\n",
    "    os.makedirs(processed_dataset_path)\n",
    "\n",
    "if len(glob.glob(os.path.join(processed_dataset_path, '*.h5'))):\n",
    "    Dataset_Build = 'Load_Dataset'\n",
    "else:\n",
    "    Dataset_Build = 'New_Dataset'\n",
    "\n",
    "\n",
    "# The anchor box scaling factors used in the original SSD512\n",
    "scales_coco = [0.07, 0.15, 0.3, 0.45, 0.6, 0.75, 0.9, 1.05]\n",
    "scales = scales_coco\n",
    "top_k = 200\n",
    "confidence_thresh = 0.01\n",
    "nms_iou_threshold = 0.5\n",
    "\n",
    "\n",
    "if DatasetName == 'City_to_foggy0_01_paired_input':\n",
    "    resize_image_to = (400, 800)\n",
    "\n",
    "    # Our model will produce predictions for these classes.\n",
    "    classes = ['background',\n",
    "               'person', 'rider', 'car', 'truck',\n",
    "               'bus', 'train', 'motorcycle', 'bicycle']\n",
    "    train_classes = classes\n",
    "    train_include_classes = 'all'\n",
    "    # Number of positive classes, 8 for domain Cityscapes, 20 for Pascal VOC, 80 for MS COCO, 1 for SIM10K\n",
    "    n_classes = len(classes) - 1\n",
    "\n",
    "else:\n",
    "    raise ValueError('Undefined dataset name.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Build the Keras model\n",
    "\n",
    "K.clear_session()  # Clear previous models from memory.\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from keras.backend.tensorflow_backend import set_session\n",
    "#\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "# config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "# # (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "# sess = tf.Session(config=config)\n",
    "# set_session(sess)  # set this TensorFlow session as the default session for Keras\n",
    "\n",
    "# model.output = `predictions`: (batch, n_boxes_total, n_classes + 4 + 8)\n",
    "# In inference mode, the predicted locations have been converted to absolute coordinates.\n",
    "# In addition, we have performed confidence thresholding, per-class non-maximum suppression, and top-k filtering.\n",
    "model = ssd_512(image_size=(img_height, img_width, 3),\n",
    "                n_classes=n_classes,\n",
    "                mode=model_mode,\n",
    "                l2_regularization=0.0005,\n",
    "                scales=scales,\n",
    "                aspect_ratios_per_layer= [[1.0, 2.0, 0.5],\n",
    "                                          [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                                          [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                                          [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                                          [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                                          [1.0, 2.0, 0.5],\n",
    "                                          [1.0, 2.0, 0.5]],\n",
    "                two_boxes_for_ar1=True,\n",
    "                steps=[8, 16, 32, 64, 128, 256, 512],\n",
    "                offsets=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n",
    "                clip_boxes=False,\n",
    "                variances=[0.1, 0.1, 0.2, 0.2],\n",
    "                normalize_coords=True,\n",
    "                subtract_mean=[123, 117, 104],\n",
    "                swap_channels=[2, 1, 0],\n",
    "                confidence_thresh=confidence_thresh,\n",
    "                iou_threshold=nms_iou_threshold,\n",
    "                top_k=top_k,\n",
    "                nms_max_output_size=400)\n",
    "\n",
    "# 2: Load the trained weights into the model\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labels: 100%|██████████| 5932/5932 [00:01<00:00, 3147.11it/s]\n",
      "Loading image IDs: 100%|██████████| 5932/5932 [00:01<00:00, 5842.98it/s]\n",
      "Loading evaluation-neutrality annotations: 100%|██████████| 5932/5932 [00:00<00:00, 8288.62it/s]\n"
     ]
    }
   ],
   "source": [
    "if Dataset_Build == 'New_Dataset':\n",
    "    # 1: Instantiate two `DataGenerator` objects: One for training, one for validation.\n",
    "\n",
    "    # Optional: If you have enough memory, consider loading the images into memory for the reasons explained above.\n",
    "\n",
    "    train_dataset = DataGenerator(dataset='train', load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "\n",
    "    Cityscapes_images_dir = '../../datasets/Cityscapes/JPEGImages'\n",
    "    Cityscapes_target_images_dir = '../../datasets/CITYSCAPES_beta_0_01/JPEGImages'\n",
    "\n",
    "    # The directories that contain the annotations.\n",
    "    Cityscapes_annotation_dir = '../../datasets/Cityscapes/Annotations'\n",
    "\n",
    "    # The paths to the image sets.\n",
    "    Cityscapes_train_source_image_set_filename = '../../datasets/Cityscapes/ImageSets/Main/train_source.txt'\n",
    "    Cityscapes_train_target_image_set_filename = '../../datasets/Cityscapes/ImageSets/Main/train_target.txt'\n",
    "\n",
    "    # images_dirs, image_set_filenames, and annotations_dirs should have the same length\n",
    "    train_dataset.parse_xml(images_dirs=[Cityscapes_images_dir,\n",
    "                                         Cityscapes_target_images_dir],\n",
    "                            image_set_filenames=[Cityscapes_train_source_image_set_filename,\n",
    "                                                 Cityscapes_train_target_image_set_filename],\n",
    "                            annotations_dirs=[Cityscapes_annotation_dir,\n",
    "                                              Cityscapes_annotation_dir],\n",
    "                            classes=classes,\n",
    "                            include_classes='all',\n",
    "                            exclude_truncated=False,\n",
    "                            exclude_difficult=False,\n",
    "                            ret=False)\n",
    "\n",
    "    # Optional: Convert the dataset into an HDF5 dataset. This will require more disk space, but will\n",
    "    # speed up the training. Doing this is not relevant in case you activated the `load_images_into_memory`\n",
    "    # option in the constructor, because in that cas the images are in memory already anyway. If you don't\n",
    "    # want to create HDF5 datasets, comment out the subsequent two function calls.\n",
    "\n",
    "    # After create these h5 files, if you have resized the input image, you need to reload these files. Otherwise,\n",
    "    # the images and the labels will not change.\n",
    "\n",
    "    train_dataset.create_hdf5_dataset(file_path=os.path.join(processed_dataset_path, 'dataset_train.h5'),\n",
    "                                      resize=resize_image_to,\n",
    "                                      variable_image_size=True,\n",
    "                                      verbose=True)\n",
    "\n",
    "    train_filenames = []\n",
    "    with open(Cityscapes_train_source_image_set_filename, 'r') as f:\n",
    "        train_filenames.extend([os.path.join(Cityscapes_images_dir, line.strip()) for line in f])\n",
    "    with open(Cityscapes_train_target_image_set_filename, 'r') as f:\n",
    "        train_filenames.extend([os.path.join(Cityscapes_target_images_dir, line.strip()) for line in f])\n",
    "\n",
    "    train_dataset = DataGenerator(dataset='train',\n",
    "                                  load_images_into_memory=False,\n",
    "                                  hdf5_dataset_path=os.path.join(processed_dataset_path, 'dataset_train.h5'),\n",
    "                                  filenames=train_filenames,\n",
    "                                  filenames_type='text',\n",
    "                                  images_dir=Cityscapes_images_dir)\n",
    "\n",
    "elif Dataset_Build == 'Load_Dataset':\n",
    "    # 1: Instantiate two `DataGenerator` objects: One for training, one for validation.\n",
    "\n",
    "    # The directories that contain the images.\n",
    "    Cityscapes_images_dir = '../../datasets/Cityscapes/JPEGImages'\n",
    "    Cityscapes_target_images_dir = '../../datasets/CITYSCAPES_beta_0_01/JPEGImages'\n",
    "\n",
    "    # The paths to the image sets.\n",
    "    Cityscapes_train_source_image_set_filename = '../../datasets/Cityscapes/ImageSets/Main/train_source.txt'\n",
    "    Cityscapes_train_target_image_set_filename = '../../datasets/Cityscapes/ImageSets/Main/train_target.txt'\n",
    "\n",
    "    train_filenames = []\n",
    "    with open(Cityscapes_train_source_image_set_filename, 'r') as f:\n",
    "        train_filenames.extend([os.path.join(Cityscapes_images_dir, line.strip()) for line in f])\n",
    "    with open(Cityscapes_train_target_image_set_filename, 'r') as f:\n",
    "        train_filenames.extend([os.path.join(Cityscapes_target_images_dir, line.strip()) for line in f])\n",
    "\n",
    "    train_dataset = DataGenerator(dataset='train',\n",
    "                                  load_images_into_memory=False,\n",
    "                                  hdf5_dataset_path=os.path.join(processed_dataset_path, 'dataset_train.h5'),\n",
    "                                  filenames=train_filenames,\n",
    "                                  filenames_type='text',\n",
    "                                  images_dir=Cityscapes_images_dir)\n",
    "\n",
    "else:\n",
    "    raise ValueError('Undefined Dataset_Build. Dataset_Build should be New_Dataset or Load_Dataset.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the dataset:\t  2966\n"
     ]
    }
   ],
   "source": [
    "# Make predictions:\n",
    "# 1: Set the generator for the predictions.\n",
    "\n",
    "# For the test generator:\n",
    "from data_generator.object_detection_2d_geometric_ops import Resize\n",
    "from data_generator.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
    "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
    "\n",
    "# First convert the input image to 3 channels and size img_height X img_width\n",
    "# Also, convert the groundtruth bounding box\n",
    "# Remember, if you want to visualize the predicted box on the original image,\n",
    "# you need to apply the corresponding reverse transformation.\n",
    "convert_to_3_channels = ConvertTo3Channels()\n",
    "resize = Resize(height=img_height, width=img_width)\n",
    "\n",
    "test_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                        shuffle=False,\n",
    "                                        transformations=[convert_to_3_channels,\n",
    "                                                         resize],\n",
    "                                        label_encoder=None,\n",
    "                                        returns={'processed_images',\n",
    "                                                 'filenames',\n",
    "                                                 'inverse_transform',\n",
    "                                                 'original_images',\n",
    "                                                 'original_labels'},\n",
    "                                        keep_images_without_gt=False)\n",
    "\n",
    "n_images = train_dataset.get_dataset_size() // 2\n",
    "n_batches = int(math.ceil(n_images / batch_size))\n",
    "print(\"Number of images in the dataset:\\t{:>6}\".format(n_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_loss = [0] * 9\n",
    "\n",
    "for i in range(n_batches):\n",
    "    batch_images, batch_filenames, batch_inverse_transforms, batch_original_images, batch_original_labels = next(test_generator)\n",
    "    y_pred = model.predict(batch_images)\n",
    "    for k, batch_item in enumerate(y_pred):\n",
    "        contrastive_loss[k] += np.mean(np.sum(batch_item, axis=1))\n",
    "\n",
    "contrastive_loss = np.array(contrastive_loss) / n_batches   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 41.22701824,  60.53421791,  56.74900597, 140.91022586,\n",
       "        51.06750465,  19.27183703,  14.15346731,  11.41272651,\n",
       "         7.76265386])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrastive_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVOID divide by zero\n",
    "contrastive_loss_proposed = [ 29.62207655,  49.57231681,  53.43487059, 115.70664217,\n",
    "        42.02834373,  16.65046193,  12.67079811,   8.9950151 ,\n",
    "         6.46483743]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVOID divide by zero\n",
    "contrastive_loss_source_only = [ 41.22701824,  60.53421791,  56.74900597, 140.91022586,\n",
    "        51.06750465,  19.27183703,  14.15346731,  11.41272651,\n",
    "         7.76265386]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for IJCAI response, adding 0.001\n",
    "contrastive_loss_proposed = [5.04625082e+08, 8.36688627e+08, 1.48802580e+07, 6.77545806e+05,\n",
    "       2.09336393e+05, 1.12413616e+05, 4.96980790e+04, 8.02024806e+03,\n",
    "       3.85601652e+02]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for IJCAI response, adding 0.001\n",
    "contrastive_loss_source_only = [6.69781816e+09, 1.46186793e+10, 2.33484823e+07, 1.61546344e+06,\n",
    "       4.04015456e+05, 2.01963388e+05, 7.64018686e+04, 1.47049118e+04,\n",
    "       6.43575637e+02]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for IJCAI response, normalized with global max pooling value, adding 0.01\n",
    "contrastive_loss_proposed = [ 0.19218799,  0.26703063,  2.66458113,  8.92538604,  7.46044039,\n",
    "       18.25614844, 51.1472713 ,  6.64656115,  5.85894508]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for IJCAI response, normalized with global max pooling value, adding 0.01\n",
    "contrastive_loss_source_only = [ 0.53074224,  0.49228338,  3.05895663, 14.70283298, 11.57102893,\n",
    "       20.50821445, 43.04239763, 13.49187959,  8.73257661]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for IJCAI response, normalized with global max pooling value, tf.equal to avoid divide by 0\n",
    "contrastive_loss_proposed = [1.92221819e-01, 2.67064144e-01, 1.51665934e+03, 1.85048188e+01,\n",
    "       5.32509794e+03, 1.95402640e+04, 9.41904442e+03, 1.14863743e+02,\n",
    "       6.18487097e+03]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for IJCAI response, normalized with global max pooling value, tf.equal to avoid divide by 0\n",
    "contrastive_loss_source_only = [5.30787090e-01, 4.92302118e-01, 1.61370720e+01, 1.25505152e+02,\n",
    "       7.12024566e+02, 7.94401277e+02, 3.36281843e+04, 8.80752039e+02,\n",
    "       2.94315979e+03]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for IJCAI response, normalized with global max pooling value, adding 1\n",
    "contrastive_loss_proposed = [0.18890265, 0.26374821, 1.37825854, 1.30700487, 0.67191207,\n",
    "       0.36299754, 0.27047483, 0.17931732, 0.14087468]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for IJCAI response, normalized with global max pooling value, adding 1\n",
    "contrastive_loss_source_only = [0.52634729, 0.49043587, 1.71674401, 2.17975823, 1.00082819,\n",
    "       0.52302337, 0.38313519, 0.28283545, 0.21496117]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for IJCAI response, normalized with global max pooling value, adding 0.001\n",
    "contrastive_loss_proposed = [1.92218434e-01, 2.67060777e-01, 6.38386107e+01, 9.93041125e+00,\n",
    "       3.87639775e+02, 1.48070468e+03, 4.66142665e+03, 4.56978680e+02,\n",
    "       3.85601652e+02]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for IJCAI response, normalized with global max pooling value, adding 0.001\n",
    "contrastive_loss_source_only = [5.30782587e-01, 4.92300261e-01, 5.68850232e+01, 8.36258052e+01,\n",
    "       6.64256759e+02, 1.66917436e+03, 3.91029150e+03, 1.00404477e+03,\n",
    "       6.43569766e+02]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for IJCAI response, normalized with global average pooling value, adding 0.001\n",
    "contrastive_loss_proposed = [ 192.62369924,  295.19604805, 9712.65418632,  236.18907731,\n",
    "       1789.8825612 , 2647.91241862, 5191.25377614,  503.6586093 ,\n",
    "        385.60165188]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for IJCAI response, normalized with global average pooling value, adding 0.001\n",
    "contrastive_loss_source_only = [   98.39816577,   237.8309297 , 13709.02575914,  1746.75376691,\n",
    "        2495.12519618,  2765.07829778,  4314.53441207,  1083.84743108,\n",
    "         643.57563687]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_loss_source_only = [9.08601779e+04, 2.04481271e+05, 9.59327304e+01, 8.43805271e+00,\n",
    "       5.91060393e+00, 5.22462989e+00, 6.19375172e+00, 8.13321501e+00,\n",
    "       3.64138057e+00]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_loss_proposed = [7.47265665e+03, 1.03122126e+04, 7.17594097e+01, 4.71510388e+00,\n",
    "       3.74769450e+00, 3.49003024e+00, 3.93897199e+00, 5.81021576e+00,\n",
    "       2.63103838e+00]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "decrease_perc = []\n",
    "for i in range(len(contrastive_loss_source_only)):\n",
    "    prec = (contrastive_loss_source_only[i] - contrastive_loss_proposed[i]) / contrastive_loss_source_only[i] * 100\n",
    "    decrease_perc.append(prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[92.46582887224875,\n",
       " 94.27657854837817,\n",
       " 36.268842621946355,\n",
       " 58.05873477396678,\n",
       " 48.18604340721064,\n",
       " 44.33960674100001,\n",
       " 34.9517493345706,\n",
       " 45.45871359799655,\n",
       " 40.08448582711033]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decrease_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_loss = [0] * 9\n",
    "\n",
    "for i in range(n_batches):\n",
    "    batch_images, batch_filenames, batch_inverse_transforms, batch_original_images, batch_original_labels = next(test_generator)\n",
    "    y_pred = model.predict(batch_images)\n",
    "    for k, batch_item in enumerate(y_pred):\n",
    "        contrastive_loss[k] += np.mean(np.sum(np.square(batch_item), axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 5 # Which batch item to look at\n",
    "print(\"Image:\", batch_filenames[i])\n",
    "print()\n",
    "plt.figure(figsize=(20,12))\n",
    "plt.imshow(batch_images[0][i])\n",
    "plt.show()\n",
    "plt.figure(figsize=(20,12))\n",
    "plt.imshow(batch_images[1][i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_loss_source_only = [9902167.52734375,\n",
    " 8212133.498046875,\n",
    " 1898.532931804657,\n",
    " 726.2631573081017,\n",
    " 370.5313905775547,\n",
    " 426.72280448675156,\n",
    " 814.8744538426399,\n",
    " 1798.734995484352,\n",
    " 1249.4038797020912]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_loss_proposed = [268103.11239624023,\n",
    " 167943.8009033203,\n",
    " 1157.6479418873787,\n",
    " 314.6913783252239,\n",
    " 227.71135184168816,\n",
    " 292.81590285897255,\n",
    " 630.1058374643326,\n",
    " 1495.1181874871254,\n",
    " 976.1152518987656]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_list = []\n",
    "for i in range(len(contrastive_loss_source_only)):\n",
    "    prec = contrastive_loss_proposed[i] / contrastive_loss_source_only[i] * 100\n",
    "    prec_list.append(prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decrease_perc = []\n",
    "for i in range(len(contrastive_loss_source_only)):\n",
    "    prec = (contrastive_loss_source_only[i] - contrastive_loss_proposed[i]) / contrastive_loss_source_only[i] * 100\n",
    "    decrease_perc.append(prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decrease_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_loss = [0] * 9\n",
    "\n",
    "for i in range(n_batches):\n",
    "    batch_images, batch_filenames, batch_inverse_transforms, batch_original_images, batch_original_labels = next(test_generator)\n",
    "    y_pred = model.predict(batch_images)\n",
    "    for k, batch_item in enumerate(y_pred):\n",
    "        contrastive_loss[k] += np.sum(batch_item)\n",
    "\n",
    "contrastive_loss = np.array(contrastive_loss) / n_batches  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([89.03311326, 50.91661614,  2.31443392,  2.26736963,  2.6562787 ,\n",
       "        5.77774082,  8.92251364, 12.95260058,  9.80180925])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrastive_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target image average activation\n",
    "average_activation_proposed = [89.03311326, 50.91661614,  2.31443392,  2.26736963,  2.6562787 ,\n",
    "        5.77774082,  8.92251364, 12.95260058,  9.80180925]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target image average activation\n",
    "average_activation_source_only = [264.42036191, 197.77866324,   2.24881325,   2.08996526,\n",
    "         2.67859298,   5.72385142,   9.42822579,  13.02158254,\n",
    "        10.31799655]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source image average activation\n",
    "average_activation_proposed = [101.61124099,  61.79482715,   2.27916633,   2.15520009,\n",
    "         2.70128587,   5.89298219,   9.18925723,  13.43297875,\n",
    "        10.14730055]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source image average activation\n",
    "average_activation_source_only = [320.51822706, 273.04391451,   2.20899288,   1.97514819,\n",
    "         2.75458072,   5.88947867,   9.78709607,  13.67050219,\n",
    "        10.79725393]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
